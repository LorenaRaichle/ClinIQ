{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4229e6daf53c4a00a2374af0cbd7163c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c2a4df6064ad40b4a8850d33e40ddf09",
              "IPY_MODEL_e9b795a7a2e448efbcb1d61f074d8030",
              "IPY_MODEL_a4213b32c4934a35adc133c95e02f3ee"
            ],
            "layout": "IPY_MODEL_1d486f44e33945508b01869eefdea363"
          }
        },
        "c2a4df6064ad40b4a8850d33e40ddf09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dd3d8b56a184bb8a3e334982a5c8384",
            "placeholder": "​",
            "style": "IPY_MODEL_f1a32abd70674f67bade3ac7bce82585",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e9b795a7a2e448efbcb1d61f074d8030": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d51da2a5af04b8fabd6412281f70454",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f134d58f384340f38f35afb28d517c58",
            "value": 2
          }
        },
        "a4213b32c4934a35adc133c95e02f3ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53111cfc3b5a4c28b14624a6a8e256f2",
            "placeholder": "​",
            "style": "IPY_MODEL_24823155ec974bb4ab2a24753335c9c8",
            "value": " 2/2 [01:16&lt;00:00, 34.79s/it]"
          }
        },
        "1d486f44e33945508b01869eefdea363": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dd3d8b56a184bb8a3e334982a5c8384": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1a32abd70674f67bade3ac7bce82585": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d51da2a5af04b8fabd6412281f70454": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f134d58f384340f38f35afb28d517c58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "53111cfc3b5a4c28b14624a6a8e256f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24823155ec974bb4ab2a24753335c9c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adria100/clin_IQ/blob/main/Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOME IMPORTS"
      ],
      "metadata": {
        "id": "uJntZ1SVyOF-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "RYOj4bW6RoRA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a58d7c8-ef9e-4ba0-99c4-b6c78415bc57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!pip install datasets\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset, concatenate_datasets\n",
        "import json\n",
        "from requests.exceptions import RequestException\n",
        "import time\n",
        "import re\n",
        "import pandas as pd\n",
        "import json\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", load_in_4bit=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "llama_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "_mOrUK6J1s0j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "4229e6daf53c4a00a2374af0cbd7163c",
            "c2a4df6064ad40b4a8850d33e40ddf09",
            "e9b795a7a2e448efbcb1d61f074d8030",
            "a4213b32c4934a35adc133c95e02f3ee",
            "1d486f44e33945508b01869eefdea363",
            "6dd3d8b56a184bb8a3e334982a5c8384",
            "f1a32abd70674f67bade3ac7bce82585",
            "7d51da2a5af04b8fabd6412281f70454",
            "f134d58f384340f38f35afb28d517c58",
            "53111cfc3b5a4c28b14624a6a8e256f2",
            "24823155ec974bb4ab2a24753335c9c8"
          ]
        },
        "outputId": "ec43fba6-7904-4b9b-9ed1-84f78804f1be"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4229e6daf53c4a00a2374af0cbd7163c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#check dataset structure"
      ],
      "metadata": {
        "id": "ChHKpN9QDFsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_dataset_structure(x):\n",
        "    try:\n",
        "        dataset = load_dataset(x)\n",
        "\n",
        "        # Print the names of the splits\n",
        "        print(\"Dataset splits:\", dataset.keys())\n",
        "\n",
        "        # Print number of samples in each split\n",
        "        for split in dataset.keys():\n",
        "            print(f\"{split} size: {len(dataset[split])}\")\n",
        "\n",
        "        # Print column names (structure)\n",
        "        print(\"Columns:\", dataset[\"train\"].column_names)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ElNQ7bmuNc-j"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA from GITHUB and mostly HUGGINGFACE"
      ],
      "metadata": {
        "id": "47ek1TepdbTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_MC1_dataset():\n",
        "    try:\n",
        "        # Load dataset from Hugging Face\n",
        "        dataset = load_dataset(\"bigbio/med_qa\")\n",
        "        transformed_data_MC1 = []\n",
        "        for item in concatenate_datasets([dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]]):\n",
        "            # Ensure only English questions are kept\n",
        "            #if item[\"language\"] == \"english\":\n",
        "                transformed_item = {\n",
        "                    \"correct_answer\": item[\"answer_idx\"],  # Convert index to A/B/C/D format\n",
        "                    \"options\": {  # Extract only the values from option dictionary\n",
        "                        \"A\": item[\"options\"][0][\"value\"],\n",
        "                        \"B\": item[\"options\"][1][\"value\"],\n",
        "                        \"C\": item[\"options\"][2][\"value\"],\n",
        "                        \"D\": item[\"options\"][3][\"value\"]\n",
        "                    },\n",
        "                    \"question\": item[\"question\"],\n",
        "                    \"source\": {\n",
        "                        \"isbn\": \"000-0000000000\",\n",
        "                        \"page\": 0,\n",
        "                        \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "                    },\n",
        "                    \"type\": \"multiple_choice\"\n",
        "                }\n",
        "                transformed_data_MC1.append(transformed_item)\n",
        "        return transformed_data_MC1\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "transformed_MC1_data = transform_MC1_dataset()\n",
        "print(json.dumps(transformed_MC1_data[:3], indent=4))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GntAi1MxNs7k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3aa3172-68a1-406e-ba3f-a139ea7f3146"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"correct_answer\": \"E\",\n",
            "        \"options\": {\n",
            "            \"A\": \"Ampicillin\",\n",
            "            \"B\": \"Ceftriaxone\",\n",
            "            \"C\": \"Ciprofloxacin\",\n",
            "            \"D\": \"Doxycycline\"\n",
            "        },\n",
            "        \"question\": \"A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7\\u00b0F (36.5\\u00b0C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
            "        },\n",
            "        \"type\": \"multiple_choice\"\n",
            "    },\n",
            "    {\n",
            "        \"correct_answer\": \"A\",\n",
            "        \"options\": {\n",
            "            \"A\": \"Placing the infant in a supine position on a firm mattress while sleeping\",\n",
            "            \"B\": \"Routine postnatal electrocardiogram (ECG)\",\n",
            "            \"C\": \"Keeping the infant covered and maintaining a high room temperature\",\n",
            "            \"D\": \"Application of a device to maintain the sleeping position\"\n",
            "        },\n",
            "        \"question\": \"A 3-month-old baby died suddenly at night while asleep. His mother noticed that he had died only after she awoke in the morning. No cause of death was determined based on the autopsy. Which of the following precautions could have prevented the death of the baby?\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
            "        },\n",
            "        \"type\": \"multiple_choice\"\n",
            "    },\n",
            "    {\n",
            "        \"correct_answer\": \"A\",\n",
            "        \"options\": {\n",
            "            \"A\": \"Abnormal migration of ventral pancreatic bud\",\n",
            "            \"B\": \"Complete failure of proximal duodenum to recanalize\",\n",
            "            \"C\": \"Error in neural crest cell migration\",\n",
            "            \"D\": \"Abnormal hypertrophy of the pylorus\"\n",
            "        },\n",
            "        \"question\": \"A mother brings her 3-week-old infant to the pediatrician's office because she is concerned about his feeding habits. He was born without complications and has not had any medical problems up until this time. However, for the past 4 days, he has been fussy, is regurgitating all of his feeds, and his vomit is yellow in color. On physical exam, the child's abdomen is minimally distended but no other abnormalities are appreciated. Which of the following embryologic errors could account for this presentation?\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
            "        },\n",
            "        \"type\": \"multiple_choice\"\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_short_answer1_dataset():\n",
        "    dataset = load_dataset(\"HPAI-BSC/OpenMedQA\")\n",
        "    transformed_data_short_answer1 = []\n",
        "    for item in dataset['train']:  # Assuming 'train' split contains the data\n",
        "        transformed_item = {\n",
        "            \"answer\": item[\"answer\"],\n",
        "            \"question\": item[\"question\"],\n",
        "            \"source\": {\n",
        "                \"isbn\": \"000-0000000000\",  # Placeholder value\n",
        "                \"page\": 0,  # Placeholder value\n",
        "                \"paragraph_id\": \"000-0000000000-p00-para26\"  # Placeholder value\n",
        "            },\n",
        "            \"type\": \"short_answer\"\n",
        "        }\n",
        "        transformed_data_short_answer1.append(transformed_item)\n",
        "    return transformed_data_short_answer1\n",
        "\n",
        "transformed_short_answer1_data = transform_short_answer1_dataset()\n",
        "print(json.dumps(transformed_short_answer1_data[:5], indent=4))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "g_M7w5B2bDDl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d19eec80-20dc-46ac-ec85-012980da7fe1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"answer\": \"Tell the attending that he cannot fail to disclose this mistake\",\n",
            "        \"question\": \"A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the procedure, the resident inadvertently cuts a flexor tendon, which is subsequently repaired without complication. The attending advises the resident not to report this complication in the operative report, stating that disclosure may unnecessarily worry the patient. What is the appropriate next action for the resident to take in this situation?\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para26\"\n",
            "        },\n",
            "        \"type\": \"short_answer\"\n",
            "    },\n",
            "    {\n",
            "        \"answer\": \"Cross-linking of DNA\",\n",
            "        \"question\": \"A 67-year-old man with transitional cell carcinoma of the bladder comes to the physician because of a 2-day history of ringing sensation in his ear. He received his first course of neoadjuvant chemotherapy 1 week ago. Pure tone audiometry shows a sensorineural hearing loss of 45 dB. What mechanism of action is responsible for the therapeutic benefit of the chemotherapeutic agent most likely causing this patient's symptoms?\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para26\"\n",
            "        },\n",
            "        \"type\": \"short_answer\"\n",
            "    },\n",
            "    {\n",
            "        \"answer\": \"Cholesterol embolization\",\n",
            "        \"question\": \"A 61-year-old man with type 2 diabetes mellitus and osteoarthritis presents with decreased urinary output and malaise two weeks after undergoing emergency cardiac catheterization with stenting for unstable angina. His medications include insulin, naproxen, aspirin, clopidogrel, and metoprolol. Vital signs show a temperature of 38\\u00b0C (100.4\\u00b0F), pulse 93/min, and blood pressure 125/85 mm Hg. Physical examination reveals mottled, reticulated purplish discoloration of the feet. Laboratory studies demonstrate leukocytosis with eosinophilia, elevated serum urea nitrogen and creatinine, and a renal biopsy showing intravascular spindle-shaped vacuoles. What is the most likely diagnosis for this patient's condition?\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para26\"\n",
            "        },\n",
            "        \"type\": \"short_answer\"\n",
            "    },\n",
            "    {\n",
            "        \"answer\": \"Lactose-fermenting, gram-negative rods forming pink colonies on MacConkey agar\",\n",
            "        \"question\": \"A 39-year-old woman is brought to the emergency department with fevers, chills, and left lower quadrant pain. Her vital signs show a temperature of 39.1\\u00b0C (102.3\\u00b0F), pulse 126/min, respirations 28/min, and blood pressure 80/50 mm Hg. Physical examination reveals blood oozing at an IV site, mucopurulent cervical discharge, and left adnexal tenderness. Laboratory results include a platelet count of 14,200/mm\\u00b3, fibrinogen 83 mg/mL, D-dimer 965 ng/mL, and a positive phenol test identifying a phosphorylated N-acetylglucosamine dimer with six fatty acids. What Gram stain characteristics and colony morphology on MacConkey agar would most likely be observed in the organism causing this infection?\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para26\"\n",
            "        },\n",
            "        \"type\": \"short_answer\"\n",
            "    },\n",
            "    {\n",
            "        \"answer\": \"Ketotifen eye drops\",\n",
            "        \"question\": \"A 35-year-old man presents with a one-week history of itchy, watery eyes and frequent sneezing, similar to an episode he experienced last spring. His medical history includes iron deficiency anemia and ankylosing spondylitis. Current medications are ferrous sulfate, artificial tear drops, and indomethacin. Vital signs are normal, and ocular examination reveals bilateral conjunctival injection with watery discharge, normal pupils, and unremarkable anterior chamber. What is the most appropriate treatment for this patient?\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para26\"\n",
            "        },\n",
            "        \"type\": \"short_answer\"\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"qiaojin/PubMedQA\", \"pqa_artificial\")\n",
        "transformed_data_TF2 = [] # Initialize an empty list for True/False questions\n",
        "for entry in dataset[\"train\"]:\n",
        "    question = entry['question'].strip()\n",
        "    answer = entry['final_decision'].strip()\n",
        "    # Convert final_decision to True/False\n",
        "    transformed_answer = \"True\" if answer.lower() == \"yes\" else \"False\"\n",
        "    # Create the formatted True/False entry\n",
        "    formatted_entry = {\n",
        "        \"answer\": transformed_answer,\n",
        "        \"question\": question,\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para31\"\n",
        "        },\n",
        "        \"type\": \"true_false\"\n",
        "    }\n",
        "\n",
        "    transformed_data_TF2.append(formatted_entry)\n",
        "transformed_TF2_data = transformed_data_TF2\n",
        "# Print the first 3 formatted entries\n",
        "print(json.dumps(transformed_TF2_data[:3], indent=4))"
      ],
      "metadata": {
        "id": "YsJ3APzfhprG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ad2e0c5-8b4a-4b0e-8f46-4447f003db24"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"answer\": \"True\",\n",
            "        \"question\": \"Are group 2 innate lymphoid cells ( ILC2s ) increased in chronic rhinosinusitis with nasal polyps or eosinophilia?\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para31\"\n",
            "        },\n",
            "        \"type\": \"true_false\"\n",
            "    },\n",
            "    {\n",
            "        \"answer\": \"True\",\n",
            "        \"question\": \"Does vagus nerve contribute to the development of steatohepatitis and obesity in phosphatidylethanolamine N-methyltransferase deficient mice?\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para31\"\n",
            "        },\n",
            "        \"type\": \"true_false\"\n",
            "    },\n",
            "    {\n",
            "        \"answer\": \"True\",\n",
            "        \"question\": \"Does psammaplin A induce Sirtuin 1-dependent autophagic cell death in doxorubicin-resistant MCF-7/adr human breast cancer cells and xenografts?\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para31\"\n",
            "        },\n",
            "        \"type\": \"true_false\"\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama"
      ],
      "metadata": {
        "id": "OSGk_HvEVq_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import Memory\n",
        "from tqdm.auto import tqdm\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data if not already downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Set pad_token_id to eos_token_id for open-end generation\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Initialize caching\n",
        "memory = Memory(location=\".cache\", verbose=0)\n",
        "\n",
        "# Function to generate prompt\n",
        "def generate_prompt(example):\n",
        "    return f\"\"\"\n",
        "    Question: {example['question']}\n",
        "    Answer: {example['answer']}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "\n",
        "# Function to extract reasoning and answer\n",
        "def extract_reasoning(response):\n",
        "    generated_text = response[0][\"generated_text\"]\n",
        "    sentences = nltk.sent_tokenize(generated_text)\n",
        "    answer = sentences[-1]\n",
        "    reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(sentences[:-1]) if step]\n",
        "    return reasoning, answer\n",
        "\n",
        "# Function to generate reasoning steps (with caching)\n",
        "@memory.cache\n",
        "def generate_reasoning_steps(examples):\n",
        "    prompts = [generate_prompt(example) for example in examples]\n",
        "    reasoning_steps = []\n",
        "    answers = []\n",
        "    for prompt in prompts:\n",
        "        try:\n",
        "            pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "            response = pipe(prompt, max_new_tokens=256, do_sample=True, batch_size=4)\n",
        "            reasoning, answer = extract_reasoning(response)\n",
        "            reasoning_steps.append(reasoning)\n",
        "            answers.append(answer)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during llama_pipeline call: {e}\")\n",
        "            reasoning_steps.append([\"Error: Could not generate reasoning.\"])\n",
        "            answers.append(\"Error: Could not generate answer.\")\n",
        "    return {\"reasoning\": reasoning_steps, \"answer\": answers}\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"lavita/MedQuAD\", split=\"train\")\n",
        "\n",
        "# Apply the function to the dataset\n",
        "dataset = dataset.map(generate_reasoning_steps, batched=True, batch_size=4)\n",
        "\n",
        "# Transform the dataset to the desired format\n",
        "transformed_data_R1 = []\n",
        "for item in tqdm(dataset, desc=\"Transforming data\"):\n",
        "    formatted_item = {\n",
        "        \"answer\": item[\"answer\"],\n",
        "        \"question\": item[\"question\"],\n",
        "        \"reasoning\": item[\"reasoning\"],\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "    transformed_data_R1.append(formatted_item)\n",
        "\n",
        "# Print the first 3 formatted entries\n",
        "print(json.dumps(transformed_data_R1[:3], indent=4))\n",
        "transformed_R1_data = transformed_data_R1"
      ],
      "metadata": {
        "id": "n9ivzROnOVrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_MC2_dataset():\n",
        "    try:\n",
        "        # Load both test and validation splits\n",
        "        dataset_test = load_dataset(\"stellalisy/mediQ\",split=\"test\")\n",
        "        dataset_validation = load_dataset(\"stellalisy/mediQ\", split=\"validation\")\n",
        "\n",
        "        transformed_data_MC2 = []\n",
        "\n",
        "        # Process both splits\n",
        "        for dataset in [dataset_test, dataset_validation]:\n",
        "            for item in dataset:\n",
        "                context = item.get(\"context\", \"\")\n",
        "                context = re.sub(r\"[\\[\\]\\{\\}\\(\\)\\'\\\"]\", \"\", str(context)) # Remove other brackets and quotes\n",
        "                transformed_item = {\n",
        "                    \"correct_answer\": item[\"answer_idx\"],\n",
        "                    \"options\": item[\"options\"],\n",
        "                    \"question\": item[\"question\"] + \" \" + context,\n",
        "                    \"source\": {\n",
        "                        \"isbn\": \"000-0000000000\",\n",
        "                        \"page\": 0,\n",
        "                        \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "                    },\n",
        "                    \"type\": \"multiple_choice\"\n",
        "                }\n",
        "                transformed_data_MC2.append(transformed_item)\n",
        "\n",
        "        # Return the combined transformed data\n",
        "        print(json.dumps(transformed_data_MC2[:3], indent=4))\n",
        "        return transformed_data_MC2\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "\n",
        "# Call the function and get the length\n",
        "transformed_MC2_data = transform_MC2_dataset()"
      ],
      "metadata": {
        "id": "opWFWV8jpbhi",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4909edf-7471-4665-fb5d-1fe4ae2638e1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"correct_answer\": \"B\",\n",
            "        \"options\": {\n",
            "            \"A\": \"Disclose the error to the patient and put it in the operative report\",\n",
            "            \"B\": \"Tell the attending that he cannot fail to disclose this mistake\",\n",
            "            \"C\": \"Report the physician to the ethics committee\",\n",
            "            \"D\": \"Refuse to dictate the operative report\"\n",
            "        },\n",
            "        \"question\": \"Which of the following is the correct next action for the resident to take? A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician., During the case, the resident inadvertently cuts a flexor tendon., The tendon is repaired without complication., The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily., He tells the resident to leave this complication out of the operative report.\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
            "        },\n",
            "        \"type\": \"multiple_choice\"\n",
            "    },\n",
            "    {\n",
            "        \"correct_answer\": \"D\",\n",
            "        \"options\": {\n",
            "            \"A\": \"Inhibition of proteasome\",\n",
            "            \"B\": \"Hyperstabilization of microtubules\",\n",
            "            \"C\": \"Generation of free radicals\",\n",
            "            \"D\": \"Cross-linking of DNA\"\n",
            "        },\n",
            "        \"question\": \"The expected beneficial effect of the drug that caused this patient's symptoms is most likely due to which of the following actions? A 67-year-old man with transitional cell carcinoma of the bladder comes to the physician because of a 2-day history of ringing sensation in his ear., He received this first course of neoadjuvant chemotherapy 1 week ago., Pure tone audiometry shows a sensorineural hearing loss of 45 dB.\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
            "        },\n",
            "        \"type\": \"multiple_choice\"\n",
            "    },\n",
            "    {\n",
            "        \"correct_answer\": \"B\",\n",
            "        \"options\": {\n",
            "            \"A\": \"Renal papillary necrosis\",\n",
            "            \"B\": \"Cholesterol embolization\",\n",
            "            \"C\": \"Eosinophilic granulomatosis with polyangiitis\",\n",
            "            \"D\": \"Polyarteritis nodosa\"\n",
            "        },\n",
            "        \"question\": \"Which of the following is the most likely cause of this patient's symptoms?\\\" Two weeks after undergoing an emergency cardiac catherization with stenting for unstable angina pectoris, a 61-year-old man has decreased urinary output and malaise., He has type 2 diabetes mellitus and osteoarthritis of the hips., Prior to admission, his medications were insulin and naproxen., He was also started on aspirin, clopidogrel, and metoprolol after the coronary intervention., His temperature is 38\\u00b0C 100.4\\u00b0F, pulse is 93/min, and blood pressure is 125/85 mm Hg., Examination shows mottled, reticulated purplish discoloration of the feet., Laboratory studies show: | Hemoglobin count 14 g/dL | Leukocyte count 16,400/mm3 | Segmented neutrophils 56% | Eosinophils 11% | Lymphocytes 31% | Monocytes 2% | Platelet count 260,000/mm3 | Erythrocyte sedimentation rate 68 mm/h | Serum | Urea nitrogen 25 mg/dL | Creatinine 4.2 mg/dL | Renal biopsy shows intravascular spindle-shaped vacuoles.\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
            "        },\n",
            "        \"type\": \"multiple_choice\"\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_MC3_dataset():\n",
        "    try:\n",
        "        # Load dataset from Hugging Face\n",
        "        dataset = load_dataset(\"openlifescienceai/medmcqa\")  # Loads the train split directly\n",
        "\n",
        "        transformed_data_MC3 = []\n",
        "\n",
        "        for item in concatenate_datasets([dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]]):  # Iterate directly over dataset\n",
        "            # Map numerical index to letter option\n",
        "            answer_mapping = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}\n",
        "            correct_answer = answer_mapping.get(item[\"cop\"], None)  # Get letter option or None if not found\n",
        "\n",
        "            transformed_item = {\n",
        "                \"correct_answer\": correct_answer, # Use mapped answer\n",
        "                \"options\": {\n",
        "                    \"A\": item[\"opa\"],\n",
        "                    \"B\": item[\"opb\"],\n",
        "                    \"C\": item[\"opc\"],\n",
        "                    \"D\": item[\"opd\"]\n",
        "                },\n",
        "                \"question\": item[\"question\"],\n",
        "                \"source\": {\n",
        "                    \"isbn\": \"000-0000000000\",\n",
        "                    \"page\": 0,\n",
        "                    \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "                },\n",
        "                \"type\": \"multiple_choice\"\n",
        "            }\n",
        "            transformed_data_MC3.append(transformed_item)\n",
        "        return transformed_data_MC3\n",
        "\n",
        "    except RequestException as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "\n",
        "# Now you can use transformed_MC3_data as before\n",
        "transformed_MC3_data = transform_MC3_dataset()\n",
        "print(json.dumps(transformed_MC3_data[:10], indent=4)) # Example: print first 10 entries"
      ],
      "metadata": {
        "id": "X_7Od8Sm90KO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5ed3c47-03dc-461f-eb5f-acb46e10aa94"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"correct_answer\": \"C\",\n",
            "        \"options\": {\n",
            "            \"A\": \"Hyperplasia\",\n",
            "            \"B\": \"Hyperophy\",\n",
            "            \"C\": \"Atrophy\",\n",
            "            \"D\": \"Dyplasia\"\n",
            "        },\n",
            "        \"question\": \"Chronic urethral obstruction due to benign prismatic hyperplasia can lead to the following change in kidney parenchyma\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
            "        },\n",
            "        \"type\": \"multiple_choice\"\n",
            "    },\n",
            "    {\n",
            "        \"correct_answer\": \"C\",\n",
            "        \"options\": {\n",
            "            \"A\": \"Vitamin C\",\n",
            "            \"B\": \"Vitamin B7\",\n",
            "            \"C\": \"Vitamin B12\",\n",
            "            \"D\": \"Vitamin D\"\n",
            "        },\n",
            "        \"question\": \"Which vitamin is supplied from only animal source:\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
            "        },\n",
            "        \"type\": \"multiple_choice\"\n",
            "    },\n",
            "    {\n",
            "        \"correct_answer\": \"D\",\n",
            "        \"options\": {\n",
            "            \"A\": \"Adjustable gastric banding\",\n",
            "            \"B\": \"Biliopancreatic diversion\",\n",
            "            \"C\": \"Duodenal Switch\",\n",
            "            \"D\": \"Roux en Y Duodenal By pass\"\n",
            "        },\n",
            "        \"question\": \"All of the following are surgical options for morbid obesity except -\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
            "        },\n",
            "        \"type\": \"multiple_choice\"\n",
            "    },\n",
            "    {\n",
            "        \"correct_answer\": \"A\",\n",
            "        \"options\": {\n",
            "            \"A\": \"Central aery of the retina\",\n",
            "            \"B\": \"Infraorbital aery\",\n",
            "            \"C\": \"Lacrimal aery\",\n",
            "            \"D\": \"Nasociliary aretry\"\n",
            "        },\n",
            "        \"question\": \"Following endaerectomy on the right common carotid, a patient is found to be blind in the right eye. It is appears that a small thrombus embolized during surgery and lodged in the aery supplying the optic nerve. Which aery would be blocked?\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
            "        },\n",
            "        \"type\": \"multiple_choice\"\n",
            "    },\n",
            "    {\n",
            "        \"correct_answer\": \"B\",\n",
            "        \"options\": {\n",
            "            \"A\": \"Directly\",\n",
            "            \"B\": \"IG1-1\",\n",
            "            \"C\": \"Thyroxine\",\n",
            "            \"D\": \"Intranuclear receptors\"\n",
            "        },\n",
            "        \"question\": \"Growth hormone has its effect on growth through?\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
            "        },\n",
            "        \"type\": \"multiple_choice\"\n",
            "    },\n",
            "    {\n",
            "        \"correct_answer\": \"C\",\n",
            "        \"options\": {\n",
            "            \"A\": \"Louse\",\n",
            "            \"B\": \"Tick\",\n",
            "            \"C\": \"Mite\",\n",
            "            \"D\": \"Milk\"\n",
            "        },\n",
            "        \"question\": \"Scrub typhus is transmitted by: September 2004\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
            "        },\n",
            "        \"type\": \"multiple_choice\"\n",
            "    },\n",
            "    {\n",
            "        \"correct_answer\": \"C\",\n",
            "        \"options\": {\n",
            "            \"A\": \"Punctation\",\n",
            "            \"B\": \"Mosaicism\",\n",
            "            \"C\": \"Satellite lesions\",\n",
            "            \"D\": \"Atypical vessels\"\n",
            "        },\n",
            "        \"question\": \"Abnormal vascular patterns seen with colposcopy in case of cervical intraepithelial neoplasia  are all except\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
            "        },\n",
            "        \"type\": \"multiple_choice\"\n",
            "    },\n",
            "    {\n",
            "        \"correct_answer\": \"C\",\n",
            "        \"options\": {\n",
            "            \"A\": \"Anal fissure\",\n",
            "            \"B\": \"Hemorrhoid\",\n",
            "            \"C\": \"Pilonidal sinus\",\n",
            "            \"D\": \"Rectal ulcer\"\n",
            "        },\n",
            "        \"question\": \"Per rectum examination is not a useful test for diagnosis of\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
            "        },\n",
            "        \"type\": \"multiple_choice\"\n",
            "    },\n",
            "    {\n",
            "        \"correct_answer\": \"C\",\n",
            "        \"options\": {\n",
            "            \"A\": \"ab\",\n",
            "            \"B\": \"bc\",\n",
            "            \"C\": \"abc\",\n",
            "            \"D\": \"bcd\"\n",
            "        },\n",
            "        \"question\": \"Characteristics of Remifentanyl \\u2013 a) Metabolised by plasma esteraseb) Short half lifec) More potent than Alfentanyld) Dose reduced in hepatic and renal diseasee) Duration of action more than Alfentanyl\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
            "        },\n",
            "        \"type\": \"multiple_choice\"\n",
            "    },\n",
            "    {\n",
            "        \"correct_answer\": \"C\",\n",
            "        \"options\": {\n",
            "            \"A\": \"Decreased ability to copy\",\n",
            "            \"B\": \"Decreased execution\",\n",
            "            \"C\": \"Deficit of expression by gesture\",\n",
            "            \"D\": \"Deficit of fluent speech\"\n",
            "        },\n",
            "        \"question\": \"Hypomimia is ?\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
            "        },\n",
            "        \"type\": \"multiple_choice\"\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of retry attempts\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Attempt to load the dataset with retry logic\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"UCSC-VLAA/MedReason\")  # Replace with your dataset name\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "transformed_data_MC4 = []\n",
        "\n",
        "# Process each entry in the dataset\n",
        "for entry in dataset['train']:\n",
        "    question = entry['question'].strip()  # Assume the question is stored in the 'question' column\n",
        "    #answer = entry['answer'].strip()  # Assume the answer is in the 'answer' column - not needed here\n",
        "    options_raw = entry['options'].strip()  # Assume the options are in the 'options' column\n",
        "\n",
        "    # Extract and format options\n",
        "    options = {}\n",
        "    for line in options_raw.split(\"\\n\"):\n",
        "        if line.strip() and \". \" in line:  # Check if the line is not empty and contains \". \"\n",
        "            choice, text = line.split(\". \", 1)  # Split into choice and text\n",
        "            options[choice.strip()] = text.strip()\n",
        "\n",
        "    # Extract answer text (using string manipulation or regex)\n",
        "    answer_text = entry['answer'].strip().split(\".\")[0]  # Split at the first \".\" and take the first part\n",
        "\n",
        "    # Find the correct answer letter (using word-based matching)\n",
        "    correct_answer_letter = None\n",
        "    for letter, option_text in options.items():\n",
        "        for word in answer_text.split():  # Iterate through words in the answer\n",
        "            if word in option_text:  # Check if the word is present in the option text\n",
        "                correct_answer_letter = letter\n",
        "                break  # Stop searching if a match is found\n",
        "        if correct_answer_letter:  # Stop searching options if a match is found\n",
        "            break\n",
        "\n",
        "    correct_answer = correct_answer_letter\n",
        "\n",
        "    # Define source information (using placeholders)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": \"000-0000000000-p00-para06\"\n",
        "    }\n",
        "\n",
        "    # Construct the formatted entry\n",
        "    formatted_entry = {\n",
        "        \"correct_answer\": correct_answer,  # Use the found letter\n",
        "        \"options\": options,  # Use the formatted options dictionary\n",
        "        \"question\": question,\n",
        "        \"source\": source,\n",
        "        \"type\": \"multiple_choice\"\n",
        "    }\n",
        "    transformed_data_MC4.append(formatted_entry)\n",
        "\n",
        "# Print first 3 formatted entries for verification (optional)\n",
        "print(json.dumps(transformed_data_MC4[:3], indent=4))\n",
        "\n",
        "transformed_MC4_data = transformed_data_MC4"
      ],
      "metadata": {
        "id": "G6Tog88KCudq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3263703-fa8f-455e-d318-41f0466452a8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"correct_answer\": \"C\",\n",
            "        \"options\": {\n",
            "            \"A\": \"Deep transverse Perineus\",\n",
            "            \"B\": \"Perinial membrane\",\n",
            "            \"C\": \"Colle's fascia\",\n",
            "            \"D\": \"Sphincter Urethrae\"\n",
            "        },\n",
            "        \"question\": \"Urogenital Diaphragm is made up of the following, except:\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para06\"\n",
            "        },\n",
            "        \"type\": \"multiple_choice\"\n",
            "    },\n",
            "    {\n",
            "        \"correct_answer\": \"A\",\n",
            "        \"options\": {\n",
            "            \"A\": \"After 5 years\",\n",
            "            \"B\": \"After 2 years\",\n",
            "            \"C\": \"After 10 years\",\n",
            "            \"D\": \"At the time of diagnosis\"\n",
            "        },\n",
            "        \"question\": \"Child with Type I Diabetes. What is the advised time for fundus examinations from the time of diagnosis?\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para06\"\n",
            "        },\n",
            "        \"type\": \"multiple_choice\"\n",
            "    },\n",
            "    {\n",
            "        \"correct_answer\": \"A\",\n",
            "        \"options\": {\n",
            "            \"A\": \"Fecal antigen test\",\n",
            "            \"B\": \"Biopsy urease test\",\n",
            "            \"C\": \"Serological test\",\n",
            "            \"D\": \"Urea breath test\"\n",
            "        },\n",
            "        \"question\": \"Most sensitive test for H pylori is-\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para06\"\n",
            "        },\n",
            "        \"type\": \"multiple_choice\"\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "no llama used, reasoning in the dataset"
      ],
      "metadata": {
        "id": "S675ghjRWI5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "import re\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"UCSC-VLAA/MedReason\", split=\"train\")\n",
        "\n",
        "# Function to extract reasoning and answer from the \"reasoning\" column\n",
        "def extract_reasoning_and_answer(example):\n",
        "    reasoning_text = example[\"reasoning\"]\n",
        "\n",
        "    # Split into sections using regex\n",
        "    sections = re.split(r\"(Finding reasoning paths:|Reasoning Process:|Conclusion:)\", reasoning_text)\n",
        "\n",
        "    # Extract relevant parts\n",
        "    reasoning_process = sections[4].strip() if len(sections) > 4 else \"\"\n",
        "    conclusion = sections[6].strip() if len(sections) > 6 else \"\"\n",
        "\n",
        "    # Combine reasoning paths and process into steps, starting from 1\n",
        "    reasoning_steps = []\n",
        "    step_counter = 1  # Initialize step counter\n",
        "\n",
        "    if reasoning_process:\n",
        "        for line in reasoning_process.split('\\n'):\n",
        "              if line.strip():  # Check if line is not empty\n",
        "                    reasoning_steps.append(f\"Step {step_counter}: {line.strip()}\")\n",
        "                    step_counter += 1  # Increment step counter\n",
        "\n",
        "        # Extract the answer from the conclusion\n",
        "        answer = conclusion.split('.')[-2].strip() if conclusion else \"\"  # Last sentence before trailing period\n",
        "\n",
        "    return {\"reasoning\": reasoning_steps, \"answer\": answer}\n",
        "\n",
        "# Apply the function to the dataset\n",
        "dataset = dataset.map(extract_reasoning_and_answer)\n",
        "\n",
        "# Transform the dataset to the desired format\n",
        "transformed_data_R2 = []\n",
        "for item in tqdm(dataset, desc=\"Transforming data\"):\n",
        "    formatted_item = {\n",
        "        \"answer\": item[\"answer\"],\n",
        "        \"question\": item[\"question\"],\n",
        "        \"reasoning\": item[\"reasoning\"],\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000-p00-para01\"  # You can adjust the paragraph_id as needed\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "    transformed_data_R2.append(formatted_item)\n",
        "\n",
        "# Print the first 3 formatted entries\n",
        "print(json.dumps(transformed_data_R2[:3], indent=4))\n",
        "transformed_R2_data = transformed_data_R2"
      ],
      "metadata": {
        "id": "1apaXR1pQPtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama"
      ],
      "metadata": {
        "id": "hCbEPdayTf6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data if not already downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Set pad_token_id to eos_token_id for open-end generation\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Function to generate prompt\n",
        "def generate_prompt(example):\n",
        "    return f\"\"\"\n",
        "    Question: {example['question']}\n",
        "    Answer: {example['answer']}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "\n",
        "# Function to extract reasoning and answer\n",
        "def extract_reasoning(response):\n",
        "    generated_text = response[0][\"generated_text\"]\n",
        "    sentences = nltk.sent_tokenize(generated_text)\n",
        "    answer = sentences[-1]  # Last sentence is the answer\n",
        "    reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(sentences[:-1]) if step]\n",
        "    return reasoning, answer\n",
        "\n",
        "# Function to generate reasoning steps\n",
        "def generate_reasoning_steps(examples):\n",
        "    prompts = [generate_prompt(example) for example in examples]\n",
        "    reasoning_steps = []\n",
        "    answers = []\n",
        "    for prompt in prompts:\n",
        "        try:\n",
        "            pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "            response = pipe(prompt, max_new_tokens=256, do_sample=True, batch_size=4)\n",
        "            reasoning, answer = extract_reasoning(response)\n",
        "            reasoning_steps.append(reasoning)\n",
        "            answers.append(answer)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during llama_pipeline call: {e}\")\n",
        "            reasoning_steps.append([\"Error: Could not generate reasoning.\"])\n",
        "            answers.append(\"Error: Could not generate answer.\")\n",
        "    return {\"reasoning\": reasoning_steps, \"answer\": answers}\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"YvanAlvin/medicalQALlama2\", split=\"train\")\n",
        "\n",
        "# Extract question and answer from the \"text\" column\n",
        "def extract_question_answer(example):\n",
        "    text = example[\"text\"]\n",
        "    match = re.search(r'\\[INST\\](.*?)\\[/INST\\]', text, re.DOTALL)\n",
        "    question = match.group(1).strip() if match else \"Unknown Question\"\n",
        "    answer = text.split(\"[/INST]\")[-1].strip()\n",
        "    return {\"question\": question, \"answer\": answer}\n",
        "\n",
        "dataset = dataset.map(extract_question_answer)\n",
        "\n",
        "# Apply the reasoning generation function\n",
        "dataset = dataset.map(generate_reasoning_steps, batched=True, batch_size=4)\n",
        "\n",
        "# Transform the dataset to the desired format\n",
        "transformed_data_R3 = []\n",
        "for item in tqdm(dataset, desc=\"Transforming data\"):\n",
        "    formatted_item = {\n",
        "        \"answer\": item[\"answer\"],\n",
        "        \"question\": item[\"question\"],\n",
        "        \"reasoning\": item[\"reasoning\"],\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "    transformed_data_R3.append(formatted_item)\n",
        "\n",
        "# Print the first 3 formatted entries\n",
        "print(json.dumps(transformed_data_R3[:3], indent=4))\n",
        "transformed_R3_data = transformed_data_R3"
      ],
      "metadata": {
        "id": "f1WXUIacSdJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama"
      ],
      "metadata": {
        "id": "boXW3nEHTleq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy model for sentence segmentation\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Number of retry attempts\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Attempt to load the dataset with retry logic\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"YvanAlvin/medicalchat200llama2\", split=\"train\")\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "transformed_data_R4 = []\n",
        "\n",
        "# Process each item in the dataset\n",
        "for item in dataset:\n",
        "    text = item[\"text\"]\n",
        "\n",
        "    # Extract the question from inside [INST]...[/INST]\n",
        "    match = re.search(r'\\[INST\\](.*?)\\[/INST\\]', text, re.DOTALL)\n",
        "    question = match.group(1).strip() if match else \"Unknown Question\"\n",
        "\n",
        "    # Extract the answer as everything after [/INST]\n",
        "    answer = text.split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "    # Generate reasoning using CodeLlama\n",
        "    prompt = f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "        response = pipe(prompt, max_new_tokens=256, do_sample=True, batch_size=4)\n",
        "\n",
        "        generated_text = response[0][\"generated_text\"]\n",
        "        sentences = nltk.sent_tokenize(generated_text)\n",
        "        final_answer = sentences[-1]\n",
        "        reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(sentences[:-1]) if step]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during llama_pipeline call: {e}\")\n",
        "        reasoning = [\"Error: Could not generate reasoning.\"]\n",
        "        final_answer = \"Error: Could not generate answer.\"\n",
        "\n",
        "    formatted_item = {\n",
        "        \"answer\": final_answer,\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "\n",
        "    transformed_data_R4.append(formatted_item)\n",
        "\n",
        "transformed_R4_data = transformed_data_R4"
      ],
      "metadata": {
        "id": "xjvAefDuTH-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama"
      ],
      "metadata": {
        "id": "-UDM0ZIyVZMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Anzahl der Wiederholungsversuche für das Laden des Datensatzes\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Dataset laden mit Wiederholungslogik\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"eashuu/medical_qa\", split=\"train\")\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "# Function to generate prompt\n",
        "def generate_prompt(question, answer):\n",
        "    return f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "\n",
        "# Function to extract reasoning and answer\n",
        "def extract_reasoning(response):\n",
        "    generated_text = response[0][\"generated_text\"]\n",
        "    sentences = nltk.sent_tokenize(generated_text)\n",
        "    answer = sentences[-1]  # Last sentence is the answer\n",
        "    reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(sentences[:-1]) if step]\n",
        "    return reasoning, answer\n",
        "\n",
        "# Function to generate reasoning steps\n",
        "def generate_reasoning_steps(question, answer):\n",
        "    prompt = generate_prompt(question, answer)\n",
        "    try:\n",
        "        pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "        response = pipe(prompt, max_new_tokens=256, do_sample=True, batch_size=4)\n",
        "        reasoning, final_answer = extract_reasoning(response)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during llama_pipeline call: {e}\")\n",
        "        reasoning = [\"Error: Could not generate reasoning.\"]\n",
        "        final_answer = \"Error: Could not generate answer.\"\n",
        "    return reasoning, final_answer\n",
        "\n",
        "# Modify processing logic\n",
        "transformed_data_R5 = []\n",
        "\n",
        "for item in dataset:\n",
        "    question = item[\"instruction\"].replace(\"Q. \", \"\", 1).strip()\n",
        "    answer = item[\"output\"].strip()\n",
        "\n",
        "    # CodeLlama for argumentation verwenden\n",
        "    reasoning, final_answer = generate_reasoning_steps(question, answer)\n",
        "\n",
        "    formatted_item = {\n",
        "        \"answer\": final_answer,\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "\n",
        "    transformed_data_R5.append(formatted_item)\n",
        "transformed_R5_data = transformed_data_R5"
      ],
      "metadata": {
        "id": "Io8hX-sHU5OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama"
      ],
      "metadata": {
        "id": "s3jWWSDmWFRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of retry attempts\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\")\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "transformed_data_R6 = []\n",
        "\n",
        "# Process each item in the dataset\n",
        "for item in dataset[\"train\"]:\n",
        "    question = item[\"Question\"].strip()\n",
        "    answer = item[\"Response\"].strip()\n",
        "\n",
        "    # Generate reasoning using CodeLlama\n",
        "    prompt = f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "        response = pipe(prompt, max_new_tokens=256, do_sample=True, batch_size=4)\n",
        "\n",
        "        generated_text = response[0][\"generated_text\"]\n",
        "        sentences = nltk.sent_tokenize(generated_text)\n",
        "        final_answer = sentences[-1]\n",
        "        reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(sentences[:-1]) if step]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during llama_pipeline call: {e}\")\n",
        "        reasoning = [\"Error: Could not generate reasoning.\"]\n",
        "        final_answer = \"Error: Could not generate answer.\"\n",
        "\n",
        "    formatted_item = {\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"answer\": final_answer,\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "\n",
        "    transformed_data_R6.append(formatted_item)\n",
        "\n",
        "# Print the first 3 formatted entries\n",
        "#print(transformed_data_R6[:3])\n",
        "\n",
        "transformed_R6_data = transformed_data_R6"
      ],
      "metadata": {
        "id": "7-CqzouRWCJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama"
      ],
      "metadata": {
        "id": "PYnz8wurX3tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Anzahl der Wiederholungsversuche für das Laden des Datensatzes\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Dataset laden mit Wiederholungslogik\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"wentechno/medicalQA-50thPlus\", split=\"train\")\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "# Function to generate prompt\n",
        "def generate_prompt(question, answer):\n",
        "    return f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "\n",
        "# Function to extract reasoning and answer\n",
        "def extract_reasoning(response):\n",
        "    generated_text = response[0][\"generated_text\"]\n",
        "    sentences = nltk.sent_tokenize(generated_text)\n",
        "    answer = sentences[-1]  # Last sentence is the answer\n",
        "    reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(sentences[:-1]) if step]\n",
        "    return reasoning, answer\n",
        "\n",
        "# Function to generate reasoning steps\n",
        "def generate_reasoning_steps(question, answer):\n",
        "    prompt = generate_prompt(question, answer)\n",
        "    try:\n",
        "        pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "        response = pipe(prompt, max_new_tokens=256, do_sample=True, batch_size=4)\n",
        "        reasoning, final_answer = extract_reasoning(response)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during llama_pipeline call: {e}\")\n",
        "        reasoning = [\"Error: Could not generate reasoning.\"]\n",
        "        final_answer = \"Error: Could not generate answer.\"\n",
        "    return reasoning, final_answer\n",
        "\n",
        "# Modify processing logic\n",
        "transformed_data_R7 = []\n",
        "\n",
        "for item in dataset:\n",
        "    question = item[\"instruction\"].replace(\"Q. \", \"\", 1).strip()\n",
        "    answer = item[\"output\"].strip()\n",
        "\n",
        "    # CodeLlama für Argumentation verwenden\n",
        "    reasoning, final_answer = generate_reasoning_steps(question, answer)\n",
        "\n",
        "    formatted_item = {\n",
        "        \"answer\": final_answer,  # Use the extracted final answer\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,  # Use the extracted reasoning steps\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "\n",
        "    transformed_data_R7.append(formatted_item)\n",
        "transformed_R7_data = transformed_data_R7"
      ],
      "metadata": {
        "id": "Tp7i4dqtWoAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset(\"Ajayaadhi/Medical-QA\")\n",
        "\n",
        "# Initialize a list to hold the reformatted entries\n",
        "transformed_data_short_answer2 = []\n",
        "\n",
        "for entry in dataset[\"train\"]:  # Process all entries\n",
        "    text = entry[\"train\"]  # Adjust this if the key is different\n",
        "\n",
        "    # Extract question\n",
        "    question_match = re.search(r\"### Input:\\n(.+?)\\n\\[INST\\]\", text, re.DOTALL)\n",
        "    question = question_match.group(1).strip() if question_match else \"\"\n",
        "\n",
        "    # Extract answer\n",
        "    answer_match = re.search(r\"### Response:\\n(.+?)</s>\", text, re.DOTALL)\n",
        "    answer = answer_match.group(1).strip() if answer_match else \"\"\n",
        "\n",
        "    # Define source information (using placeholders)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": \"000-0000000-p00-para01\"\n",
        "    }\n",
        "\n",
        "    # Determine response type\n",
        "    response_type = \"short_answer\"\n",
        "\n",
        "    # Construct the reformatted entry\n",
        "    reformatted_entry = {\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"source\": source,\n",
        "        \"type\": response_type\n",
        "    }\n",
        "\n",
        "    # Append to the list\n",
        "    transformed_data_short_answer2.append(reformatted_entry)\n",
        "\n",
        "# Print the first 3 formatted entries\n",
        "transformed_short_answer2_data = transformed_data_short_answer2\n",
        "print(json.dumps(transformed_short_answer1_data[:4], indent = 4))"
      ],
      "metadata": {
        "id": "TWOpCxIu3QYq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "799d0141-f68f-42d4-c0a1-36f772843f40"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"answer\": \"Tell the attending that he cannot fail to disclose this mistake\",\n",
            "        \"question\": \"A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the procedure, the resident inadvertently cuts a flexor tendon, which is subsequently repaired without complication. The attending advises the resident not to report this complication in the operative report, stating that disclosure may unnecessarily worry the patient. What is the appropriate next action for the resident to take in this situation?\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para26\"\n",
            "        },\n",
            "        \"type\": \"short_answer\"\n",
            "    },\n",
            "    {\n",
            "        \"answer\": \"Cross-linking of DNA\",\n",
            "        \"question\": \"A 67-year-old man with transitional cell carcinoma of the bladder comes to the physician because of a 2-day history of ringing sensation in his ear. He received his first course of neoadjuvant chemotherapy 1 week ago. Pure tone audiometry shows a sensorineural hearing loss of 45 dB. What mechanism of action is responsible for the therapeutic benefit of the chemotherapeutic agent most likely causing this patient's symptoms?\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para26\"\n",
            "        },\n",
            "        \"type\": \"short_answer\"\n",
            "    },\n",
            "    {\n",
            "        \"answer\": \"Cholesterol embolization\",\n",
            "        \"question\": \"A 61-year-old man with type 2 diabetes mellitus and osteoarthritis presents with decreased urinary output and malaise two weeks after undergoing emergency cardiac catheterization with stenting for unstable angina. His medications include insulin, naproxen, aspirin, clopidogrel, and metoprolol. Vital signs show a temperature of 38\\u00b0C (100.4\\u00b0F), pulse 93/min, and blood pressure 125/85 mm Hg. Physical examination reveals mottled, reticulated purplish discoloration of the feet. Laboratory studies demonstrate leukocytosis with eosinophilia, elevated serum urea nitrogen and creatinine, and a renal biopsy showing intravascular spindle-shaped vacuoles. What is the most likely diagnosis for this patient's condition?\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para26\"\n",
            "        },\n",
            "        \"type\": \"short_answer\"\n",
            "    },\n",
            "    {\n",
            "        \"answer\": \"Lactose-fermenting, gram-negative rods forming pink colonies on MacConkey agar\",\n",
            "        \"question\": \"A 39-year-old woman is brought to the emergency department with fevers, chills, and left lower quadrant pain. Her vital signs show a temperature of 39.1\\u00b0C (102.3\\u00b0F), pulse 126/min, respirations 28/min, and blood pressure 80/50 mm Hg. Physical examination reveals blood oozing at an IV site, mucopurulent cervical discharge, and left adnexal tenderness. Laboratory results include a platelet count of 14,200/mm\\u00b3, fibrinogen 83 mg/mL, D-dimer 965 ng/mL, and a positive phenol test identifying a phosphorylated N-acetylglucosamine dimer with six fatty acids. What Gram stain characteristics and colony morphology on MacConkey agar would most likely be observed in the organism causing this infection?\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para26\"\n",
            "        },\n",
            "        \"type\": \"short_answer\"\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama"
      ],
      "metadata": {
        "id": "pKWclgB7YuTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Anzahl der Wiederholungsversuche für das Laden des Datensatzes\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Dataset laden mit Wiederholungslogik\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"KaungHtetCho/MedicalQA\", split=\"train\")\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "\n",
        "# Funktion zur Generierung von Argumentationsschritten\n",
        "def generate_reasoning_steps(question, answer):\n",
        "    prompt = f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "        response = pipe(prompt, max_new_tokens=256, do_sample=True, batch_size=4) # Using the pipeline\n",
        "        # Extract reasoning and final answer\n",
        "        generated_text = response[0][\"generated_text\"]\n",
        "        sentences = nltk.sent_tokenize(generated_text)\n",
        "        final_answer = sentences[-1]  # Last sentence is the final answer\n",
        "        reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(sentences[:-1]) if step]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during llama_pipeline call: {e}\")\n",
        "        reasoning = [\"Error: Could not generate reasoning.\"]\n",
        "        final_answer = \"Error: Could not generate answer.\"\n",
        "\n",
        "    return reasoning, final_answer\n",
        "\n",
        "transformed_data_R8 = []\n",
        "\n",
        "# Process each entry in the dataset\n",
        "for entry in dataset:\n",
        "    patient = entry['Patient'].strip()\n",
        "    doctor = entry['Doctor'].strip()\n",
        "\n",
        "    # Form the question from description + patient details\n",
        "    question = patient\n",
        "\n",
        "    # Generate answer using CodeLlama\n",
        "    # answer = llama_pipeline(question, max_length=256, do_sample=True)[0][\"generated_text\"].strip()\n",
        "    answer = doctor\n",
        "    # Generate reasoning using CodeLlama\n",
        "    reasoning, final_answer = generate_reasoning_steps(question, answer)\n",
        "\n",
        "\n",
        "    # Define source information (using placeholders)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": \"000-0000-p00-para01\"\n",
        "    }\n",
        "\n",
        "    # Define the type based on reasoning complexity\n",
        "    response_type = \"multi_hop\"\n",
        "\n",
        "    # Construct the formatted entry\n",
        "    formatted_entry = {\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"answer\": final_answer,  # Using generated answer\n",
        "        \"source\": source,\n",
        "        \"type\": response_type\n",
        "    }\n",
        "\n",
        "    transformed_data_R8.append(formatted_entry)\n",
        "transformed_R8_data = transformed_data_R8\n",
        "# Print first 3 formatted entries\n",
        "print(json.dumps(transformed_data_R8[:3], indent=4))"
      ],
      "metadata": {
        "id": "7hBl8wfoYDVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Anzahl der Wiederholungsversuche für das Laden des Datensatzes\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Dataset laden mit Wiederholungslogik\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"qiaojin/PubMedQA\", \"pqa_unlabeled\")\n",
        "        break\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "# Funktion zur Generierung von Argumentationsschritten\n",
        "def generate_reasoning_steps(question, answer):\n",
        "    prompt = f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "        response = pipe(prompt, max_new_tokens=256, do_sample=True, batch_size=4) # Using the pipeline\n",
        "        # Extract reasoning and final answer\n",
        "        generated_text = response[0][\"generated_text\"]\n",
        "        sentences = nltk.sent_tokenize(generated_text)\n",
        "        final_answer = sentences[-1]  # Last sentence is the final answer\n",
        "        reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(sentences[:-1]) if step]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during llama_pipeline call: {e}\")\n",
        "        reasoning = [\"Error: Could not generate reasoning.\"]\n",
        "        final_answer = \"Error: Could not generate answer.\"\n",
        "\n",
        "    return reasoning, final_answer\n",
        "\n",
        "transformed_data_R9 = []\n",
        "\n",
        "# Process each entry in the dataset\n",
        "for entry in dataset[\"train\"]:\n",
        "    question = entry['question'].strip()\n",
        "    answer = entry['long_answer'].strip()\n",
        "\n",
        "    # Use CodeLlama to generate reasoning\n",
        "    reasoning, final_answer = generate_reasoning_steps(question, answer)  # Extract final_answer\n",
        "\n",
        "    # Define source information (using placeholders)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": \"000-0000-p00-para01\"\n",
        "    }\n",
        "\n",
        "    # Define the type based on reasoning complexity\n",
        "    response_type = \"multi_hop\"\n",
        "\n",
        "    # Construct the formatted entry\n",
        "    formatted_entry = {\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"answer\": final_answer,  # Use extracted final_answer\n",
        "        \"source\": source,\n",
        "        \"type\": response_type\n",
        "    }\n",
        "\n",
        "    transformed_data_R9.append(formatted_entry)\n",
        "\n",
        "transformed_R9_data = transformed_data_R9\n",
        "# Print first 3 formatted entries\n",
        "#print(json.dumps(transformed_data_R9[:3], indent=4))"
      ],
      "metadata": {
        "id": "oXxUdiBbZKLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA FROM KAGGLE"
      ],
      "metadata": {
        "id": "Wpt4rFU37RaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle pandas\n"
      ],
      "metadata": {
        "id": "Vpzmmvo53jYO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "939b6da1-f4bc-4a98-8683-6783ee2025ae"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d thedevastator/comprehensive-medical-q-a-dataset --unzip\n"
      ],
      "metadata": {
        "id": "gQad59AC3wJ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5ed62ca-cca9-4f5e-d89f-ab1606bc1bbf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.config/kaggle/kaggle.json'\n",
            "Dataset URL: https://www.kaggle.com/datasets/thedevastator/comprehensive-medical-q-a-dataset\n",
            "License(s): CC0-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import auth\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "# Authenticate with Kaggle API\n",
        "os.environ['KAGGLE_USERNAME'] = \"apfresh\" # Replace with your username\n",
        "os.environ['KAGGLE_KEY'] = \"50af00b12093dc762e1d2d1c138dd817\"\n",
        "api = KaggleApi()\n",
        "api.authenticate()"
      ],
      "metadata": {
        "id": "7p2ULHnJ67jX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcd95f72-5e22-4834-e4a2-f0dac6be58dc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.config/kaggle/kaggle.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset (assuming it's already available in Colab)\n",
        "file_path = \"/content/train.csv\"  # Adjust based on actual filename\n",
        "dataset = pd.read_csv(file_path)\n",
        "\n",
        "# Initialize a list to hold the reformatted entries\n",
        "transformed_data_short_answer3 = []\n",
        "\n",
        "# Iterate over the dataset entries\n",
        "for index, entry in dataset.iterrows():\n",
        "    question = entry.get('Question', '').strip()\n",
        "    answer = entry.get('Answer', '').strip()\n",
        "\n",
        "    # Define source information (using placeholders here)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": f\"000-0000000-p00-para{index+1:02d}\"\n",
        "    }\n",
        "\n",
        "    # Determine the response type based on answer length\n",
        "    response_type = \"short_answer\"\n",
        "\n",
        "    # Construct the reformatted entry\n",
        "    reformatted_entry = {\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"source\": source,\n",
        "        \"type\": response_type\n",
        "    }\n",
        "\n",
        "    # Append to the list\n",
        "    transformed_data_short_answer3.append(reformatted_entry)\n",
        "# Convert the list to JSON format and print the first 3 entries\n",
        "#print(json.dumps(transformed_data_short_answer3[:3], indent=4))\n",
        "transformed_short_answer3_data = transformed_data_short_answer3\n",
        "print(json.dumps(transformed_short_answer3_data[:4], indent=4))"
      ],
      "metadata": {
        "id": "IJeGfraN5jJ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6f5cd18-8417-44a2-fa52-2aec7f9d2a12"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"question\": \"Who is at risk for Lymphocytic Choriomeningitis (LCM)? ?\",\n",
            "        \"answer\": \"LCMV infections can occur after exposure to fresh urine, droppings, saliva, or nesting materials from infected rodents.  Transmission may also occur when these materials are directly introduced into broken skin, the nose, the eyes, or the mouth, or presumably, via the bite of an infected rodent. Person-to-person transmission has not been reported, with the exception of vertical transmission from infected mother to fetus, and rarely, through organ transplantation.\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000-p00-para01\"\n",
            "        },\n",
            "        \"type\": \"short_answer\"\n",
            "    },\n",
            "    {\n",
            "        \"question\": \"What are the symptoms of Lymphocytic Choriomeningitis (LCM) ?\",\n",
            "        \"answer\": \"LCMV is most commonly recognized as causing neurological disease, as its name implies, though infection without symptoms or mild febrile illnesses are more common clinical manifestations. \\n                \\nFor infected persons who do become ill, onset of symptoms usually occurs 8-13 days after exposure to the virus as part of a biphasic febrile illness. This initial phase, which may last as long as a week, typically begins with any or all of the following symptoms: fever, malaise, lack of appetite, muscle aches, headache, nausea, and vomiting. Other symptoms appearing less frequently include sore throat, cough, joint pain, chest pain, testicular pain, and parotid (salivary gland) pain. \\n                \\nFollowing a few days of recovery, a second phase of illness may occur. Symptoms may consist of meningitis (fever, headache, stiff neck, etc.), encephalitis (drowsiness, confusion, sensory disturbances, and/or motor abnormalities, such as paralysis), or meningoencephalitis (inflammation of both the brain and meninges). LCMV has also been known to cause acute hydrocephalus (increased fluid on the brain), which often requires surgical shunting to relieve increased intracranial pressure. In rare instances, infection results in myelitis (inflammation of the spinal cord) and presents with symptoms such as muscle weakness, paralysis, or changes in body sensation. An association between LCMV infection and myocarditis (inflammation of the heart muscles) has been suggested. \\n                \\nPrevious observations show that most patients who develop aseptic meningitis or encephalitis due to LCMV survive. No chronic infection has been described in humans, and after the acute phase of illness, the virus is cleared from the body. However, as in all infections of the central nervous system, particularly encephalitis, temporary or permanent neurological damage is possible. Nerve deafness and arthritis have been reported. \\n                \\nWomen who become infected with LCMV during pregnancy may pass the infection on to the fetus. Infections occurring during the first trimester may result in fetal death and pregnancy termination, while in the second and third trimesters, birth defects can develop. Infants infected In utero can have many serious and permanent birth defects, including vision problems, mental retardation, and hydrocephaly (water on the brain). Pregnant women may recall a flu-like illness during pregnancy, or may not recall any illness. \\n                \\nLCM is usually not fatal. In general, mortality is less than 1%.\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000-p00-para02\"\n",
            "        },\n",
            "        \"type\": \"short_answer\"\n",
            "    },\n",
            "    {\n",
            "        \"question\": \"Who is at risk for Lymphocytic Choriomeningitis (LCM)? ?\",\n",
            "        \"answer\": \"Individuals of all ages who come into contact with urine, feces, saliva, or blood of wild mice are potentially at risk for infection. Owners of pet mice or hamsters may be at risk for infection if these animals originate from colonies that were contaminated with LCMV, or if their animals are infected from other wild mice. Human fetuses are at risk of acquiring infection vertically from an infected mother. \\n                \\nLaboratory workers who work with the virus or handle infected animals are also at risk. However, this risk can be minimized by utilizing animals from sources that regularly test for the virus, wearing proper protective laboratory gear, and following appropriate safety precautions.\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000-p00-para03\"\n",
            "        },\n",
            "        \"type\": \"short_answer\"\n",
            "    },\n",
            "    {\n",
            "        \"question\": \"How to diagnose Lymphocytic Choriomeningitis (LCM) ?\",\n",
            "        \"answer\": \"During the first phase of the disease, the most common laboratory abnormalities are a low white blood cell count (leukopenia) and a low platelet count (thrombocytopenia). Liver enzymes in the serum may also be mildly elevated. After the onset of neurological disease during the second phase, an increase in protein levels, an increase in the number of white blood cells or a decrease in the glucose levels in the cerebrospinal fluid (CSF) is usually found. \\n   \\nLaboratory diagnosis is usually made by detecting IgM and IgG antibodies in the CSF and serum. Virus can be detected by PCR or virus isolation in the CSF at during the acute stage of illness.\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000-p00-para04\"\n",
            "        },\n",
            "        \"type\": \"short_answer\"\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d pythonafroz/medquad-medical-question-answer-for-ai-research --unzip\n"
      ],
      "metadata": {
        "id": "RBfFVCdr7Ajw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b8641d-48d6-4f1f-d9fc-8a097c048921"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/pythonafroz/medquad-medical-question-answer-for-ai-research\n",
            "License(s): CC-BY-SA-4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from transformers import pipeline\n",
        "\n",
        "# Assuming you have already loaded the model and tokenizer as 'model' and 'tokenizer'\n",
        "# and that nltk.download('punkt') has been executed\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/medquad.csv\"  # Adjust based on the actual filename\n",
        "dataset = pd.read_csv(file_path)\n",
        "\n",
        "# Function to generate prompt\n",
        "def generate_prompt(question, answer):\n",
        "    return f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "\n",
        "# Function to extract reasoning and answer\n",
        "def extract_reasoning(response):\n",
        "    generated_text = response[0][\"generated_text\"]\n",
        "    sentences = nltk.sent_tokenize(generated_text)\n",
        "    answer = sentences[-1]  # Last sentence is the answer\n",
        "    reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(sentences[:-1]) if step]\n",
        "    return reasoning, answer\n",
        "\n",
        "# Function to generate reasoning steps\n",
        "def generate_reasoning_steps(question, answer):\n",
        "    prompt = generate_prompt(question, answer)\n",
        "    try:\n",
        "        pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "        response = pipe(prompt, max_new_tokens=256, do_sample=True, batch_size=4)\n",
        "        reasoning, final_answer = extract_reasoning(response)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during llama_pipeline call: {e}\")\n",
        "        reasoning = [\"Error: Could not generate reasoning.\"]\n",
        "        final_answer = \"Error: Could not generate answer.\"\n",
        "    return reasoning, final_answer\n",
        "\n",
        "# Initialize a list to hold the reformatted entries\n",
        "transformed_data_R10 = []\n",
        "\n",
        "# Iterate over the dataset entries\n",
        "for index, entry in dataset.iterrows():\n",
        "    question = entry.get('Question', '').strip()\n",
        "    answer = entry.get('Answer', '').strip()\n",
        "\n",
        "    # Generate reasoning using CodeLlama\n",
        "    reasoning, final_answer = generate_reasoning_steps(question, answer)\n",
        "\n",
        "    # Define source information (using placeholders here)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": f\"000-0000000-p00-para{index+1:02d}\"\n",
        "    }\n",
        "\n",
        "    # Construct the reformatted entry\n",
        "    reformatted_entry = {\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,  # Include the reasoning steps\n",
        "        \"answer\": final_answer,  # Use the final answer (single sentence)\n",
        "        \"source\": source,\n",
        "        \"type\": \"multi_hop\"  # You might need to adjust the type\n",
        "    }\n",
        "\n",
        "    # Append to the list\n",
        "    transformed_data_R10.append(reformatted_entry)\n",
        "\n",
        "transformed_R10_data = transformed_data_R10"
      ],
      "metadata": {
        "id": "lCfTdDtYZ1BG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "96a0c894-81a6-42ba-f732-3b56f71ae2e6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(\n",
            "Device set to use cuda:0\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-75015151b4bd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Generate reasoning using CodeLlama\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mreasoning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_reasoning_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# Define source information (using placeholders here)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-75015151b4bd>\u001b[0m in \u001b[0;36mgenerate_reasoning_steps\u001b[0;34m(question, answer)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mreasoning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_reasoning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     def preprocess(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1377\u001b[0m             )\n\u001b[1;32m   1378\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1384\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1386\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1387\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1284\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1286\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1287\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generation_config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2464\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2465\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2466\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3432\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3434\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3436\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    822\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m                 )\n\u001b[1;32m    570\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    572\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         hidden_states, self_attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mhidden_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mquery_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LOCAL DATA"
      ],
      "metadata": {
        "id": "yLVNoX6_Ehrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from io import StringIO\n",
        "\n",
        "#not working\n",
        "csv = \"/content/true_false_questions.csv\"\n",
        "\n",
        "# Fetch the CSV data\n",
        "try:\n",
        "    df = pd.read_csv(csv)  # Read CSV directly from Colab environment\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found: {csv}. Please make sure it is uploaded to Colab.\")\n",
        "    exit()\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse the CSV file: {csv}. Please check its format.\")\n",
        "    exit()\n",
        "\n",
        "df = pd.read_csv(csv)\n",
        "# Transform data\n",
        "transformed_data_TF1 = []\n",
        "for _, row in df.iterrows():\n",
        "    formatted_item = {\n",
        "        \"question\": row[\"text\"],  # Extract question\n",
        "        \"answer\": str(row[\"label\"]),  # Extract answer as string\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",  # Placeholder value\n",
        "            \"page\": 0,  # Placeholder value\n",
        "            \"paragraph_id\": \"000-0000000000-p00-paraXX\"  # Placeholder value\n",
        "        },\n",
        "        \"type\": \"true_false\"\n",
        "    }\n",
        "    transformed_data_TF1.append(formatted_item)\n",
        "\n",
        "# Save formatted data to JSON\n",
        "transformed_TF1_data = transformed_data_TF1"
      ],
      "metadata": {
        "id": "O64XzUf2ifuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ENTIRE DATASET"
      ],
      "metadata": {
        "id": "w_wLy9bMCy9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "lists = [transformed_TF2_data, transformed_short_answer1_data, transformed_short_answer2_data, transformed_short_answer3_data,\n",
        "        transformed_MC1_data, transformed_MC2_data, transformed_MC3_data, transformed_MC4_data\n",
        "         ]\n",
        "         #transformed_TF1_data,\n",
        "# transformed_R1_data, transformed_R2_data,transformed_R3_data,transformed_R4_data,transformed_R5_data,transformed_R6_data,transformed_R7_data,transformed_R8_data, transformed_R9_data, transformed_R10_data\n",
        "DATA = list(itertools.chain(*lists))\n"
      ],
      "metadata": {
        "id": "7TdXFkBOra0E"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removes duplicates, tokenization, stopwords, lemmatization, padding\n",
        "\n",
        "# Handle Class Imbalance:\n",
        "\n",
        "#    SMOTE will help generate synthetic samples for underrepresented classes in the dataset.\n",
        "\n",
        "#    Class Weights can be used in the model to give more importance to underrepresented classes during training.\n",
        "\n",
        "# Paraphrasing / Question Modification:\n",
        "\n",
        "#    We will use a GPT-based model to paraphrase or modify questions to generate additional training samples."
      ],
      "metadata": {
        "id": "OUPfGmrhCdpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SPLIT BY TYPE and SAVE .ZIP TO REPOSITORY"
      ],
      "metadata": {
        "id": "h3W5LepvH9ky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "\n",
        "    Ensure the data is preprocessed appropriately for each prompt style before creating the datasets.\n",
        "    Adjust the batch size, training steps, epochs, and other hyperparameters to find the best performance.\n",
        "    Regularly evaluate the performance on your test set for each prompt style.\n",
        "    This example assumes your prompt styles data is readily available.\n",
        "    Remember to make necessary imports and data modifications for smooth execution.\n"
      ],
      "metadata": {
        "id": "Ui9VFHI3-ZHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "Ad-Pq5_GkFvl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b8e1185-d850-48db-9873-b29a61dbf771"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "# Initialize lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Lemmatizes, removes stopwords, and lowercases the input text.\"\"\"\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.lower() not in stop_words and token.isalnum()]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def remove_duplicates_and_empty(data):\n",
        "    \"\"\"Removes duplicates and entries with empty questions or answers.\"\"\"\n",
        "    seen_pairs = set()\n",
        "    filtered_data = []\n",
        "    for entry in data:\n",
        "        # Check if both 'question' and 'answer' keys exist\n",
        "        if \"question\" in entry and \"answer\" in entry:\n",
        "            q, a = entry[\"question\"], entry[\"answer\"]\n",
        "\n",
        "            if not q or not a:  # Remove if empty\n",
        "                continue\n",
        "\n",
        "            pair = (q.strip().lower(), a.strip().lower())  # Normalize case for comparison\n",
        "            if pair not in seen_pairs:\n",
        "                seen_pairs.add(pair)\n",
        "                filtered_data.append(entry)\n",
        "    return filtered_data\"\"\""
      ],
      "metadata": {
        "id": "cP519IeiB1rS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f662c62-d256-436c-aca9-893ab4aebb04"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Lemmatizes, removes stopwords, and lowercases the input text.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [\n",
        "        lemmatizer.lemmatize(token.lower())\n",
        "        for token in tokens\n",
        "        if token.lower() not in stop_words and token.isalnum()\n",
        "    ]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def remove_duplicates_and_empty(data):\n",
        "    \"\"\"Removes duplicates and entries with missing or empty questions/answers.\"\"\"\n",
        "    seen_pairs = set()\n",
        "    filtered_data = []\n",
        "\n",
        "    for entry in data:\n",
        "        q = entry.get(\"question\", \"\")\n",
        "        a = entry.get(\"answer\", \"\")\n",
        "\n",
        "        if not q or not a:\n",
        "            continue\n",
        "\n",
        "        pair = (q.strip().lower(), a.strip().lower())\n",
        "        if pair not in seen_pairs:\n",
        "            seen_pairs.add(pair)\n",
        "            filtered_data.append(entry)\n",
        "\n",
        "    return filtered_data\n",
        "\n",
        "def preprocess_dataset(data):\n",
        "    \"\"\"Preprocesses question and answer text across various formats.\"\"\"\n",
        "    cleaned_data = remove_duplicates_and_empty(data)\n",
        "\n",
        "    for entry in cleaned_data:\n",
        "        entry[\"question\"] = preprocess_text(entry.get(\"question\", \"\"))\n",
        "        entry[\"answer\"] = preprocess_text(entry.get(\"answer\", \"\"))\n",
        "\n",
        "        # Special handling for multiple-choice options (if present)\n",
        "        if entry.get(\"type\") == \"multiple_choice\" and \"options\" in entry:\n",
        "            processed_options = {}\n",
        "            for key, val in entry[\"options\"].items():\n",
        "                processed_options[key] = preprocess_text(val)\n",
        "            entry[\"options\"] = processed_options\n",
        "\n",
        "        # Optional: preprocess reasoning if it exists\n",
        "        if \"reasoning\" in entry:\n",
        "            entry[\"reasoning\"] = preprocess_text(entry[\"reasoning\"])\n",
        "\n",
        "    return cleaned_data\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaUwJ_xOQu6v",
        "outputId": "2ebe46af-2049-4859-fa60-513101830dc8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#It's usually better to apply stopword removal and lemmatization before removing duplicates\n",
        "#preprocessed_data = [{k: preprocess_text(v) if isinstance(v, str) else v\n",
        "#                       for k, v in d.items()} for d in DATA]\n",
        "#preprocessed_data = remove_duplicates_and_empty(preprocessed_data)  # Apply to DATA"
      ],
      "metadata": {
        "id": "0DIk_DxVD8Rb"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Initialize tools\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Lemmatizes, removes stopwords, and lowercases the input text.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [\n",
        "        lemmatizer.lemmatize(token.lower())\n",
        "        for token in tokens\n",
        "        if token.lower() not in stop_words and token.isalnum()\n",
        "    ]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def remove_duplicates_and_empty(data):\n",
        "    \"\"\"Removes duplicates and entries with missing or empty questions and valid answer fields.\"\"\"\n",
        "    seen_pairs = set()\n",
        "    filtered_data = []\n",
        "\n",
        "    for entry in data:\n",
        "        q = entry.get(\"question\", \"\")\n",
        "        a = entry.get(\"answer\") or entry.get(\"correct_answer\", \"\")\n",
        "\n",
        "        if not q or not a:\n",
        "            continue\n",
        "\n",
        "        pair = (q.strip().lower(), a.strip().lower())\n",
        "        if pair not in seen_pairs:\n",
        "            seen_pairs.add(pair)\n",
        "            filtered_data.append(entry)\n",
        "\n",
        "    return filtered_data\n",
        "\n",
        "def preprocess_and_clean_dataset(data):\n",
        "    \"\"\"Applies preprocessing to all relevant fields and removes duplicates.\"\"\"\n",
        "    preprocessed_data = []\n",
        "\n",
        "    for d in data:\n",
        "        processed_entry = {}\n",
        "\n",
        "        for k, v in d.items():\n",
        "            # Preprocess main text fields\n",
        "            if k in [\"question\", \"answer\", \"reasoning\"] and isinstance(v, str):\n",
        "                processed_entry[k] = preprocess_text(v)\n",
        "            # Preprocess options dict (for multiple_choice)\n",
        "            elif k == \"options\" and isinstance(v, dict):\n",
        "                processed_entry[k] = {opt_k: preprocess_text(opt_v)\n",
        "                                      for opt_k, opt_v in v.items()}\n",
        "            else:\n",
        "                processed_entry[k] = v\n",
        "\n",
        "        preprocessed_data.append(processed_entry)\n",
        "\n",
        "    return remove_duplicates_and_empty(preprocessed_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLvW10oQRnU0",
        "outputId": "54b5b630-fa3b-46a4-ac4a-5a5b0f26c4e1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming DATA is your mixed-format dataset\n",
        "cleaned_data = preprocess_and_clean_dataset(DATA)\n",
        "\n",
        "# Preview a result\n",
        "from pprint import pprint\n",
        "pprint(cleaned_data[:4])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jaj9lpNSRtYr",
        "outputId": "2b80112e-e1b8-4e7a-c7e1-ed5c94a043d7"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'answer': 'true',\n",
            "  'question': 'group 2 innate lymphoid cell ilc2s increased chronic '\n",
            "              'rhinosinusitis nasal polyp eosinophilia',\n",
            "  'source': {'isbn': '000-0000000000',\n",
            "             'page': 0,\n",
            "             'paragraph_id': '000-0000000000-p00-para31'},\n",
            "  'type': 'true_false'},\n",
            " {'answer': 'true',\n",
            "  'question': 'vagus nerve contribute development steatohepatitis obesity '\n",
            "              'phosphatidylethanolamine deficient mouse',\n",
            "  'source': {'isbn': '000-0000000000',\n",
            "             'page': 0,\n",
            "             'paragraph_id': '000-0000000000-p00-para31'},\n",
            "  'type': 'true_false'},\n",
            " {'answer': 'true',\n",
            "  'question': 'psammaplin induce sirtuin autophagic cell death human breast '\n",
            "              'cancer cell xenograft',\n",
            "  'source': {'isbn': '000-0000000000',\n",
            "             'page': 0,\n",
            "             'paragraph_id': '000-0000000000-p00-para31'},\n",
            "  'type': 'true_false'},\n",
            " {'answer': 'true',\n",
            "  'question': 'methylation fgfr2 gene associated high birth weight centile '\n",
            "              'human',\n",
            "  'source': {'isbn': '000-0000000000',\n",
            "             'page': 0,\n",
            "             'paragraph_id': '000-0000000000-p00-para31'},\n",
            "  'type': 'true_false'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split in Test and Train"
      ],
      "metadata": {
        "id": "rGRfqej7jn_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Group by Question Type\n",
        "grouped_data = {}\n",
        "for item in cleaned_data:\n",
        "    question_type = item['type']\n",
        "    if question_type not in grouped_data:\n",
        "        grouped_data[question_type] = []\n",
        "    grouped_data[question_type].append(item)\n",
        "\n",
        "# 2. Stratified Split within Each Group\n",
        "train_data = []\n",
        "test_data = []\n",
        "for question_type, data in grouped_data.items():\n",
        "    # Create a temporary DataFrame for easier stratification (optional)\n",
        "    df = pd.DataFrame(data)\n",
        "    # Perform stratified split, using 'type' column for stratification\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['type'], random_state=42)  # Adjust test_size as needed\n",
        "    # Append the split data to the overall train and test sets\n",
        "    train_data.extend(train_df.to_dict('records'))\n",
        "    test_data.extend(test_df.to_dict('records'))\n",
        "\n",
        "# 3. Combine Splits\n",
        "# Now you have train_data and test_data with equal representation of question types\n",
        "print(f\"Train data size: {len(train_data)}\")\n",
        "print(f\"Test data size: {len(test_data)}\")"
      ],
      "metadata": {
        "id": "m1ZpZyEHjmxb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1562405e-b92e-4581-876f-494e4aca8a28"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data size: 365608\n",
            "Test data size: 91404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Group by Question Type\n",
        "grouped_data = {}\n",
        "for item in cleaned_data:  # Use the cleaned and preprocessed data\n",
        "    question_type = item.get('type', 'unknown')\n",
        "    grouped_data.setdefault(question_type, []).append(item)\n",
        "\n",
        "# 2. Stratified Split within Each Group\n",
        "train_data = []\n",
        "test_data = []\n",
        "\n",
        "for question_type, data in grouped_data.items():\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    if len(df) < 2:\n",
        "        # Not enough samples to split; assign all to train\n",
        "        train_data.extend(df.to_dict('records'))\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        train_df, test_df = train_test_split(\n",
        "            df, test_size=0.2, stratify=df['type'], random_state=42\n",
        "        )\n",
        "    except ValueError:\n",
        "        # Fallback if stratification fails (e.g. all types are the same)\n",
        "        train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "    train_data.extend(train_df.to_dict('records'))\n",
        "    test_data.extend(test_df.to_dict('records'))\n",
        "\n",
        "# 3. Result Summary\n",
        "print(f\"Train data size: {len(train_data)}\")\n",
        "print(f\"Test data size: {len(test_data)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHcRTz05TQe_",
        "outputId": "35861a9d-4356-4c44-bcae-bd24008bcb74"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data size: 365608\n",
            "Test data size: 91404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting before formatting avoids potential data leakage, where information from the test set might influence the model during training."
      ],
      "metadata": {
        "id": "M8ynijmQnGex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format training data\n",
        "train_short_answer_data = [\n",
        "    {\n",
        "        \"input\": f\"Answer the following question:\\n{d['question']}\",\n",
        "        \"output\": d[\"answer\"]\n",
        "    }\n",
        "    for d in train_data if d['type'] == 'short_answer'\n",
        "]\n",
        "\n",
        "train_multi_hop_data = [\n",
        "    {\n",
        "        \"input\": f\"Answer the following multi-hop question:\\n{d['question']}\",\n",
        "        \"output\": f\"{d['reasoning']}\\nAnswer: {d['answer']}\"\n",
        "    }\n",
        "    for d in train_data if d['type'] == 'multi_hop'\n",
        "]\n",
        "\n",
        "train_true_false_data = [\n",
        "    {\n",
        "        \"input\": f\"Is the following statement true or false?\\nStatement: {d['question']}\",\n",
        "        \"output\": d[\"answer\"]\n",
        "    }\n",
        "    for d in train_data if d['type'] == 'true_false'\n",
        "]\n",
        "\n",
        "train_multiple_choice_data = [\n",
        "    {\n",
        "        \"input\": (\n",
        "            f\"Choose the correct option:\\nQuestion: {d['question']}\\nOptions:\\n\" +\n",
        "            '\\n'.join([f\"{key}) {value}\" for key, value in d['options'].items()])\n",
        "        ),\n",
        "        \"output\": d[\"correct_answer\"]\n",
        "    }\n",
        "    for d in train_data\n",
        "    if d['type'] == 'multiple_choice' and 'question' in d and 'options' in d and 'correct_answer' in d\n",
        "]\n",
        "\n",
        "\n",
        "# Format testing data\n",
        "test_short_answer_data = [\n",
        "    {\n",
        "        \"input\": f\"Answer the following question:\\n{d['question']}\",\n",
        "        \"output\": d[\"answer\"]\n",
        "    }\n",
        "    for d in test_data if d['type'] == 'short_answer'\n",
        "]\n",
        "\n",
        "test_multi_hop_data = [\n",
        "    {\n",
        "        \"input\": f\"Answer the following multi-hop question:\\n{d['question']}\",\n",
        "        \"output\": f\"{d['reasoning']}\\nAnswer: {d['answer']}\"\n",
        "    }\n",
        "    for d in test_data if d['type'] == 'multi_hop'\n",
        "]\n",
        "\n",
        "\n",
        "test_true_false_data = [\n",
        "    {\n",
        "        \"input\": f\"Is the following statement true or false?\\nStatement: {d['question']}\",\n",
        "        \"output\": d[\"answer\"]\n",
        "    }\n",
        "    for d in test_data if d['type'] == 'true_false'\n",
        "]\n",
        "\n",
        "test_multiple_choice_data = [\n",
        "    {\n",
        "        \"input\": (\n",
        "            f\"Choose the correct option:\\nQuestion: {d['question']}\\nOptions:\\n\" +\n",
        "            '\\n'.join([f\"{key}) {value}\" for key, value in d['options'].items()])\n",
        "        ),\n",
        "        \"output\": d[\"correct_answer\"]\n",
        "    }\n",
        "    for d in test_data\n",
        "    if d['type'] == 'multiple_choice' and 'question' in d and 'options' in d and 'correct_answer' in d\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "kuuBQY0i0s1b"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save data to .zip"
      ],
      "metadata": {
        "id": "sa-eaSLc1Zyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = {\n",
        "    \"short_answer\": [],\n",
        "    \"true_false\": [],\n",
        "    \"multiple_choice\": [],\n",
        "    \"multi_hop\": []\n",
        "}\n",
        "\n",
        "test_data = {\n",
        "    \"short_answer\": [],\n",
        "    \"true_false\": [],\n",
        "    \"multiple_choice\": [],\n",
        "    \"multi_hop\": []\n",
        "}\n",
        "\n",
        "# Add your transformed data to the appropriate lists:\n",
        "train_data[\"short_answer\"].extend(train_short_answer_data)\n",
        "train_data[\"true_false\"].extend(train_true_false_data)\n",
        "train_data[\"multiple_choice\"].extend(train_multiple_choice_data)\n",
        "train_data[\"multi_hop\"].extend(train_multi_hop_data)\n",
        "\n",
        "test_data[\"short_answer\"].extend(test_short_answer_data)\n",
        "test_data[\"true_false\"].extend(test_true_false_data)\n",
        "test_data[\"multiple_choice\"].extend(test_multiple_choice_data)\n",
        "test_data[\"multi_hop\"].extend(test_multi_hop_data)"
      ],
      "metadata": {
        "id": "GJrKVTMU1dps"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import zipfile\n",
        "\n",
        "# Convert the test_data list to JSON format\n",
        "json_data_1 = json.dumps(test_data, indent=4)\n",
        "\n",
        "# Create a zip file and write the JSON data to it\n",
        "with zipfile.ZipFile(\"test_dataset.zip\", \"w\") as zipf:\n",
        "    zipf.writestr(\"test_dataset.json\", json_data_1)\n",
        "\n",
        "json_data_2 = json.dumps(train_data, indent=4)\n",
        "\n",
        "# Create a zip file and write the JSON data to it\n",
        "with zipfile.ZipFile(\"train_dataset.zip\", \"w\") as zipf:\n",
        "    zipf.writestr(\"train_dataset.json\", json_data_2)\n",
        "\n",
        "print(\"test/train_dataset.zip created successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjzPj0kwMMdf",
        "outputId": "cb7d6221-7d80-40dc-ba8f-237338ef388b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test/train_dataset.zip created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install github3.py"
      ],
      "metadata": {
        "id": "FAyUrYNW7HLy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79d9cffa-8475-4ae5-f58c-7bca5845745c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting github3.py\n",
            "  Downloading github3.py-4.0.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: pyjwt>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.3.0->github3.py) (2.10.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from github3.py) (2.9.0.post0)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.11/dist-packages (from github3.py) (2.32.3)\n",
            "Requirement already satisfied: uritemplate>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from github3.py) (4.1.1)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.3.0->github3.py) (43.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.6.0->github3.py) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.18->github3.py) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.18->github3.py) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.18->github3.py) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.18->github3.py) (2025.4.26)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.3.0->github3.py) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.4.0->pyjwt[crypto]>=2.3.0->github3.py) (2.22)\n",
            "Downloading github3.py-4.0.1-py3-none-any.whl (151 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.8/151.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: github3.py\n",
            "Successfully installed github3.py-4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import zipfile\n",
        "import github3\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# 1. Get GitHub token from Secrets\n",
        "github_token = userdata.get('git')\n",
        "\n",
        "# 2. Authenticate with GitHub\n",
        "gh = github3.login(token=github_token)\n",
        "\n",
        "# 3. Repository Information\n",
        "repo_owner = 'Adria100'  # Replace with your username\n",
        "repo_name = 'clin_IQ'  # Replace with your repository name\n",
        "repo = gh.repository(repo_owner, repo_name)\n",
        "\n",
        "# 4. Function to create zip and upload to GitHub\n",
        "def save_data_to_zip_and_upload(data_dict, zip_file_name):\n",
        "    with zipfile.ZipFile(zip_file_name, \"w\") as zipf:\n",
        "        for data_type, data_list in data_dict.items():\n",
        "            file_name = f\"{data_type}_data.json\"\n",
        "            with zipf.open(file_name, \"w\") as f:\n",
        "                f.write(json.dumps(data_list, indent=4).encode())\n",
        "\n",
        "    # Upload the zip file to GitHub\n",
        "    with open(zip_file_name, \"rb\") as f:\n",
        "        content = f.read()\n",
        "        repo.create_file(\n",
        "            path=f\"data/processed/{zip_file_name}\",  # Path in the repository\n",
        "            message=f\"Adding {zip_file_name}\",  # Commit message\n",
        "            content=content,\n",
        "            branch='main'  # Replace with your branch name if needed\n",
        "        )\n",
        "\n",
        "    print(f\"Uploaded {zip_file_name} to GitHub\")\n",
        "    os.remove(zip_file_name)  # Remove local zip file\n",
        "\n",
        "# 5. Assuming you have train_data and test_data dictionaries populated\n",
        "# ... (your code to populate train_data and test_data) ...\n",
        "\n",
        "# 6. Save and upload the zip files\n",
        "save_data_to_zip_and_upload(train_data, \"train_dataset.zip\")\n",
        "save_data_to_zip_and_upload(test_data, \"test_dataset.zip\")"
      ],
      "metadata": {
        "id": "TVwD0KMA7Bxk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "74cf3122-0e66-4aae-8aae-6eb991a1afd3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "404 Not Found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-e65dc854009e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mrepo_owner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Adria100'\u001b[0m  \u001b[0;31m# Replace with your username\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mrepo_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'clin_IQ'\u001b[0m  \u001b[0;31m# Replace with your repository name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mrepo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepository\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_owner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# 4. Function to create zip and upload to GitHub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/github3/github.py\u001b[0m in \u001b[0;36mrepository\u001b[0;34m(self, owner, repository)\u001b[0m\n\u001b[1;32m   2101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mowner\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrepository\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m             \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"repos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepository\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2103\u001b[0;31m             \u001b[0mjson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_instance_or_null\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRepository\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/github3/models.py\u001b[0m in \u001b[0;36m_json\u001b[0;34m(self, response, expected_status_code, include_cache_info)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mactual_status_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_status_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mactual_status_code\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mactual_status_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m304\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: 404 Not Found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import zipfile\n",
        "import github3\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# 1. Get GitHub token from Secrets\n",
        "github_token = userdata.get('git')\n",
        "\n",
        "# 2. Authenticate with GitHub\n",
        "gh = github3.login(token=github_token)\n",
        "\n",
        "# 3. Repository Information\n",
        "repo_owner = 'Adria100'  # Replace with your username\n",
        "repo_name = 'clin_IQ'  # Replace with your repository name\n",
        "\n",
        "# Check if the repository exists before trying to access it\n",
        "try:\n",
        "    repo = gh.repository(repo_owner, repo_name)\n",
        "    if repo is None:\n",
        "        raise github3.exceptions.NotFoundError(\"Repository not found\")\n",
        "except github3.exceptions.NotFoundError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    # Handle the error, e.g., exit or provide instructions to the user\n",
        "    exit() # You can replace this with your error handling\n",
        "\n",
        "# 4. Function to create zip and upload to GitHub\n",
        "def save_data_to_zip_and_upload(data_dict, zip_file_name):\n",
        "    with zipfile.ZipFile(zip_file_name, \"w\") as zipf:\n",
        "        for data_type, data_list in data_dict.items():\n",
        "            file_name = f\"{data_type}_data.json\"\n",
        "            with zipf.open(file_name, \"w\") as f:\n",
        "                f.write(json.dumps(data_list, indent=4).encode())\n",
        "\n",
        "    # Upload the zip file to GitHub\n",
        "    with open(zip_file_name, \"rb\") as f:\n",
        "        content = f.read()\n",
        "        repo.create_file(\n",
        "            path=f\"data/processed/{zip_file_name}\",  # Path in the repository\n",
        "            message=f\"Adding {zip_file_name}\",  # Commit message\n",
        "            content=content,\n",
        "            branch='main'  # Replace with your branch name if needed\n",
        "        )\n",
        "\n",
        "    print(f\"Uploaded {zip_file_name} to GitHub\")\n",
        "    os.remove(zip_file_name)  # Remove local zip file\n",
        "\n",
        "# 5. Assuming you have train_data and test_data dictionaries populated\n",
        "# ... (your code to populate train_data and test_data) ...\n",
        "\n",
        "# 6. Save and upload the zip files\n",
        "save_data_to_zip_and_upload(train_data, \"train_dataset.zip\")\n",
        "save_data_to_zip_and_upload(test_data, \"test_dataset.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "y7eXMcHPGwXz",
        "outputId": "055f4f19-bd49-48c3-ef6c-be2870bafa5c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 404 Not Found\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-141e67a4b920>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# 6. Save and upload the zip files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0msave_data_to_zip_and_upload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_dataset.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0msave_data_to_zip_and_upload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test_dataset.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "\n",
        "model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\"\"\"inputs = tokenizer(\n",
        "    your_data,\n",
        "    padding=\"max_length\",  # Pad to the maximum length\n",
        "    truncation=True,        # Truncate if exceeding the maximum length\n",
        "    max_length=512,        # Adjust the maximum length as needed\n",
        "    return_tensors=\"pt\"     # Return PyTorch tensors\n",
        ")\"\"\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             load_in_8bit=True,\n",
        "                                             device_map='auto')"
      ],
      "metadata": {
        "id": "eJEtDDNH-ECw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiPromptTrainer(Trainer):\n",
        "    def __init__(self, *args, prompt_styles_data, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.prompt_styles_data = prompt_styles_data  # Store data for each style\n",
        "\n",
        "    def training_step(self, model, inputs):\n",
        "        # Iterate over each prompt style\n",
        "        for style, data in self.prompt_styles_data.items():\n",
        "            # Create a dataloader for the current style\n",
        "            train_dataloader = self.get_train_dataloader(data)\n",
        "\n",
        "            # Perform a training step for the current style\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "              batch = batch.to(self.args.device)\n",
        "              outputs = model(**batch)\n",
        "              loss = outputs.loss\n",
        "              loss.backward()\n",
        "              self.optimizer.step()\n",
        "              self.optimizer.zero_grad()\n",
        "\n",
        "        return {'loss': loss.item()}  # Return the loss"
      ],
      "metadata": {
        "id": "cBeb4K8R-N1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",          # Output directory\n",
        "    per_device_train_batch_size=4,  # Batch size per device\n",
        "    gradient_accumulation_steps=4,  # Gradient accumulation steps\n",
        "    num_train_epochs=3,              # Number of training epochs\n",
        "    fp16=True,                       # Enable mixed precision training\n",
        "    logging_dir='./logs',            # Directory for storing logs\n",
        "    learning_rate=2e-5,             # Learning rate\n",
        "    weight_decay=0.01,              # Weight decay\n",
        "    optim=\"adamw_torch\",\n",
        "    save_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "trainer = MultiPromptTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=None,   # Not used in this example\n",
        "    prompt_styles_data={\n",
        "        \"short_answer\": train_short_answer_data,\n",
        "        \"multi_hop\": train_multi_hop_data,\n",
        "        \"true_false\": train_true_false_data,\n",
        "        \"multiple_choice\": train_multiple_choice_data\n",
        "    }\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "z3VOm5-E-PcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"./fine_tuned_llama\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_llama\")"
      ],
      "metadata": {
        "id": "kK8OKhTd-Sjb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
