{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5de7c984e82f46fba53d776bf23ebf2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_beec22736d624e78b9b8281594974ccd",
              "IPY_MODEL_e82bb5cf67104e198fd833ca711bff19",
              "IPY_MODEL_34db7cc336cf414faf365c878e9b1013"
            ],
            "layout": "IPY_MODEL_e31aaaad98b84df7a644b56e1a88f878"
          }
        },
        "beec22736d624e78b9b8281594974ccd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89e48d08bb16461b87e72637c82fc1d9",
            "placeholder": "​",
            "style": "IPY_MODEL_d419e305d52141bfa1467df5a2212f2a",
            "value": "train_data.csv: 100%"
          }
        },
        "e82bb5cf67104e198fd833ca711bff19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80c2a4299b184845bf99db5b9184788c",
            "max": 41624342,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9105b24244f2444eb091e6f1941e00a7",
            "value": 41624342
          }
        },
        "34db7cc336cf414faf365c878e9b1013": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10f511408624461db907b666a60e9aa3",
            "placeholder": "​",
            "style": "IPY_MODEL_b4c3811ed7af4a74a6ab78b008b25172",
            "value": " 41.6M/41.6M [00:00&lt;00:00, 64.2MB/s]"
          }
        },
        "e31aaaad98b84df7a644b56e1a88f878": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89e48d08bb16461b87e72637c82fc1d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d419e305d52141bfa1467df5a2212f2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80c2a4299b184845bf99db5b9184788c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9105b24244f2444eb091e6f1941e00a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "10f511408624461db907b666a60e9aa3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4c3811ed7af4a74a6ab78b008b25172": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "592e28249b2a494b92e0c4168659eeec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4bada59db17645db8fc3ad8b62b64ec7",
              "IPY_MODEL_78bb9afcf7c844cf8fda5de188e97de6",
              "IPY_MODEL_c35ed3c940314645bc304e2ecf8ec3fb"
            ],
            "layout": "IPY_MODEL_d16c633ce5ae479e8c11fd06af647e5d"
          }
        },
        "4bada59db17645db8fc3ad8b62b64ec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_958690204aef459f82c07a5fbb2e337c",
            "placeholder": "​",
            "style": "IPY_MODEL_f9ee20c06ec14ed5a94dfe7bb84bc952",
            "value": "Generating train split: 100%"
          }
        },
        "78bb9afcf7c844cf8fda5de188e97de6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fe107d97bcd42bd87b44270bb35ff75",
            "max": 49897,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9808ebe843a347458ce1a1a5ddfd6c08",
            "value": 49897
          }
        },
        "c35ed3c940314645bc304e2ecf8ec3fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfb6ff50b4574129ad64026c1c860559",
            "placeholder": "​",
            "style": "IPY_MODEL_ab9527d0bafc45b893f33653be99672c",
            "value": " 49897/49897 [00:00&lt;00:00, 53095.72 examples/s]"
          }
        },
        "d16c633ce5ae479e8c11fd06af647e5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "958690204aef459f82c07a5fbb2e337c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9ee20c06ec14ed5a94dfe7bb84bc952": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fe107d97bcd42bd87b44270bb35ff75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9808ebe843a347458ce1a1a5ddfd6c08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dfb6ff50b4574129ad64026c1c860559": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab9527d0bafc45b893f33653be99672c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adria100/clin_IQ/blob/main/MEDQAdatasetformatting(3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOME IMPORTS"
      ],
      "metadata": {
        "id": "uJntZ1SVyOF-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "RYOj4bW6RoRA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a57aadcd-723e-43c6-c12c-e7ef5acb0fed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.12.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.5\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!pip install datasets\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset, concatenate_datasets\n",
        "import json\n",
        "from requests.exceptions import RequestException\n",
        "import time\n",
        "import re\n",
        "import pandas as pd\n",
        "import json\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "\"\"\"model_id = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", load_in_4bit=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "llama_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\"\"\""
      ],
      "metadata": {
        "id": "_mOrUK6J1s0j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "6a277a5b-92fb-482d-fe94-85cb4bfa93f8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'model_id = \"codellama/CodeLlama-7b-Instruct-hf\"\\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", load_in_4bit=True)\\ntokenizer = AutoTokenizer.from_pretrained(model_id)\\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\\nllama_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#check dataset structure"
      ],
      "metadata": {
        "id": "ChHKpN9QDFsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_dataset_structure(x):\n",
        "    try:\n",
        "        dataset = load_dataset(x)\n",
        "\n",
        "        # Print the names of the splits\n",
        "        print(\"Dataset splits:\", dataset.keys())\n",
        "\n",
        "        # Print number of samples in each split\n",
        "        for split in dataset.keys():\n",
        "            print(f\"{split} size: {len(dataset[split])}\")\n",
        "\n",
        "        # Print column names (structure)\n",
        "        print(\"Columns:\", dataset[\"train\"].column_names)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ElNQ7bmuNc-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA from GITHUB and mostly HUGGINGFACE"
      ],
      "metadata": {
        "id": "47ek1TepdbTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_MC1_dataset():\n",
        "    try:\n",
        "        # Load dataset from Hugging Face\n",
        "        dataset = load_dataset(\"bigbio/med_qa\")\n",
        "        transformed_data_MC1 = []\n",
        "        for item in concatenate_datasets([dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]]):\n",
        "            # Ensure only English questions are kept\n",
        "            #if item[\"language\"] == \"english\":\n",
        "                transformed_item = {\n",
        "                    \"correct_answer\": item[\"answer_idx\"],  # Convert index to A/B/C/D format\n",
        "                    \"options\": {  # Extract only the values from option dictionary\n",
        "                        \"A\": item[\"options\"][0][\"value\"],\n",
        "                        \"B\": item[\"options\"][1][\"value\"],\n",
        "                        \"C\": item[\"options\"][2][\"value\"],\n",
        "                        \"D\": item[\"options\"][3][\"value\"]\n",
        "                    },\n",
        "                    \"question\": item[\"question\"],\n",
        "                    \"source\": {\n",
        "                        \"isbn\": \"000-0000000000\",\n",
        "                        \"page\": 0,\n",
        "                        \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "                    },\n",
        "                    \"type\": \"multiple_choice\"\n",
        "                }\n",
        "                transformed_data_MC1.append(transformed_item)\n",
        "        return transformed_data_MC1\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "transformed_MC1_data = transform_MC1_dataset()\n",
        "print(json.dumps(transformed_MC1_data[:3], indent=4))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GntAi1MxNs7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_short_answer1_dataset():\n",
        "    dataset = load_dataset(\"HPAI-BSC/OpenMedQA\")\n",
        "    transformed_data_short_answer1 = []\n",
        "    for item in dataset['train']:  # Assuming 'train' split contains the data\n",
        "        transformed_item = {\n",
        "            \"answer\": item[\"answer\"],\n",
        "            \"question\": item[\"question\"],\n",
        "            \"source\": {\n",
        "                \"isbn\": \"000-0000000000\",  # Placeholder value\n",
        "                \"page\": 0,  # Placeholder value\n",
        "                \"paragraph_id\": \"000-0000000000-p00-para26\"  # Placeholder value\n",
        "            },\n",
        "            \"type\": \"short_answer\"\n",
        "        }\n",
        "        transformed_data_short_answer1.append(transformed_item)\n",
        "    return transformed_data_short_answer1\n",
        "\n",
        "transformed_short_answer1_data = transform_short_answer1_dataset()\n",
        "print(json.dumps(transformed_short_answer1_data[:5], indent=4))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "g_M7w5B2bDDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"qiaojin/PubMedQA\", \"pqa_artificial\")\n",
        "transformed_data_TF2 = [] # Initialize an empty list for True/False questions\n",
        "for entry in dataset[\"train\"]:\n",
        "    question = entry['question'].strip()\n",
        "    answer = entry['final_decision'].strip()\n",
        "    # Convert final_decision to True/False\n",
        "    transformed_answer = \"True\" if answer.lower() == \"yes\" else \"False\"\n",
        "    # Create the formatted True/False entry\n",
        "    formatted_entry = {\n",
        "        \"answer\": transformed_answer,\n",
        "        \"question\": question,\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para31\"\n",
        "        },\n",
        "        \"type\": \"true_false\"\n",
        "    }\n",
        "\n",
        "    transformed_data_TF2.append(formatted_entry)\n",
        "transformed_TF2_data = transformed_data_TF2\n",
        "# Print the first 3 formatted entries\n",
        "print(json.dumps(transformed_TF2_data[:3], indent=4))"
      ],
      "metadata": {
        "id": "YsJ3APzfhprG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama"
      ],
      "metadata": {
        "id": "OSGk_HvEVq_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import Memory  # For caching\n",
        "from tqdm.auto import tqdm  # For progress monitoring\n",
        "\n",
        "# Set pad_token_id to eos_token_id for open-end generation\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"lavita/MedQuAD\", split=\"train\")\n",
        "\n",
        "# Initialize caching\n",
        "memory = Memory(location=\".cache\", verbose=0)\n",
        "\n",
        "# Function to generate prompt (extracted for modularity)\n",
        "def generate_prompt(example):\n",
        "    return f\"\"\"\n",
        "    Question: {example['question']}\n",
        "    Answer: {example['answer']}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "\n",
        "# Function to extract reasoning (extracted for modularity)\n",
        "def extract_reasoning(response):\n",
        "    steps = response[0][\"generated_text\"].split(\"\\n\")\n",
        "    reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(steps) if step]\n",
        "    return reasoning\n",
        "\n",
        "# Function to generate reasoning steps (with error handling and caching)\n",
        "@memory.cache  # Apply caching to this function\n",
        "def generate_reasoning_steps(examples):\n",
        "    prompts = [generate_prompt(example) for example in examples]\n",
        "\n",
        "    reasoning_steps = []\n",
        "    for prompt in prompts:\n",
        "        try:\n",
        "            # Batch generation using the pipeline (reduced batch size to 4)\n",
        "            pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "            response = pipe(prompt, max_new_tokens=256, do_sample=True, batch_size=4)\n",
        "            reasoning_steps.append(extract_reasoning(response))\n",
        "        except Exception as e:\n",
        "            print(f\"Error during llama_pipeline call: {e}\")\n",
        "            reasoning_steps.append([\"Error: Could not generate reasoning.\"])  # Handle error\n",
        "\n",
        "    return {\"reasoning\": reasoning_steps}\n",
        "\n",
        "# Apply the function to the dataset with progress monitoring\n",
        "dataset = dataset.map(generate_reasoning_steps, batched=True, batch_size=4)\n",
        "\n",
        "# Transform the dataset to the desired format\n",
        "transformed_data_R1 = []\n",
        "for item in tqdm(dataset, desc=\"Transforming data\"):  # Add progress bar\n",
        "    formatted_item = {\n",
        "        \"answer\": item[\"answer\"],\n",
        "        \"question\": item[\"question\"],\n",
        "        \"reasoning\": item[\"reasoning\"],\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "    transformed_data_R1.append(formatted_item)\n",
        "\n",
        "# Print the first 3 formatted entries\n",
        "print(json.dumps(transformed_data_R1[:3], indent=4))\n",
        "transformed_R1_data = transformed_data_R1"
      ],
      "metadata": {
        "id": "JLkjqNJmGsX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_MC2_dataset():\n",
        "    try:\n",
        "        # Load both test and validation splits\n",
        "        dataset_test = load_dataset(\"stellalisy/mediQ\",split=\"test\")\n",
        "        dataset_validation = load_dataset(\"stellalisy/mediQ\", split=\"validation\")\n",
        "\n",
        "        transformed_data_MC2 = []\n",
        "\n",
        "        # Process both splits\n",
        "        for dataset in [dataset_test, dataset_validation]:\n",
        "            for item in dataset:\n",
        "                context = item.get(\"context\", \"\")\n",
        "                context = re.sub(r\"[\\[\\]\\{\\}\\(\\)\\'\\\"]\", \"\", str(context)) # Remove other brackets and quotes\n",
        "                transformed_item = {\n",
        "                    \"correct_answer\": item[\"answer_idx\"],\n",
        "                    \"options\": item[\"options\"],\n",
        "                    \"question\": item[\"question\"] + \" \" + context,\n",
        "                    \"source\": {\n",
        "                        \"isbn\": \"000-0000000000\",\n",
        "                        \"page\": 0,\n",
        "                        \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "                    },\n",
        "                    \"type\": \"multiple_choice\"\n",
        "                }\n",
        "                transformed_data_MC2.append(transformed_item)\n",
        "\n",
        "        # Return the combined transformed data\n",
        "        print(json.dumps(transformed_data_MC2[:3], indent=4))\n",
        "        return transformed_data_MC2\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "\n",
        "# Call the function and get the length\n",
        "transformed_MC2_data = transform_MC2_dataset()"
      ],
      "metadata": {
        "id": "opWFWV8jpbhi",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_MC3_dataset():\n",
        "    try:\n",
        "        # Load dataset from Hugging Face\n",
        "        dataset = load_dataset(\"openlifescienceai/medmcqa\")  # Loads the train split directly\n",
        "\n",
        "        transformed_data_MC3 = []\n",
        "\n",
        "        for item in concatenate_datasets([dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]]):  # Iterate directly over dataset\n",
        "            # Map numerical index to letter option\n",
        "            answer_mapping = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}\n",
        "            correct_answer = answer_mapping.get(item[\"cop\"], None)  # Get letter option or None if not found\n",
        "\n",
        "            transformed_item = {\n",
        "                \"correct_answer\": correct_answer, # Use mapped answer\n",
        "                \"options\": {\n",
        "                    \"A\": item[\"opa\"],\n",
        "                    \"B\": item[\"opb\"],\n",
        "                    \"C\": item[\"opc\"],\n",
        "                    \"D\": item[\"opd\"]\n",
        "                },\n",
        "                \"question\": item[\"question\"],\n",
        "                \"source\": {\n",
        "                    \"isbn\": \"000-0000000000\",\n",
        "                    \"page\": 0,\n",
        "                    \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "                },\n",
        "                \"type\": \"multiple_choice\"\n",
        "            }\n",
        "            transformed_data_MC3.append(transformed_item)\n",
        "        return transformed_data_MC3\n",
        "\n",
        "    except RequestException as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "\n",
        "# Now you can use transformed_MC3_data as before\n",
        "transformed_MC3_data = transform_MC3_dataset()\n",
        "print(json.dumps(transformed_MC3_data[:10], indent=4)) # Example: print first 10 entries"
      ],
      "metadata": {
        "id": "X_7Od8Sm90KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of retry attempts\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Attempt to load the dataset with retry logic\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"UCSC-VLAA/MedReason\")  # Replace with your dataset name\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "transformed_data_MC4 = []\n",
        "\n",
        "# Process each entry in the dataset\n",
        "for entry in dataset['train']:\n",
        "    question = entry['question'].strip()  # Assume the question is stored in the 'question' column\n",
        "    #answer = entry['answer'].strip()  # Assume the answer is in the 'answer' column - not needed here\n",
        "    options_raw = entry['options'].strip()  # Assume the options are in the 'options' column\n",
        "\n",
        "    # Extract and format options\n",
        "    options = {}\n",
        "    for line in options_raw.split(\"\\n\"):\n",
        "        if line.strip() and \". \" in line:  # Check if the line is not empty and contains \". \"\n",
        "            choice, text = line.split(\". \", 1)  # Split into choice and text\n",
        "            options[choice.strip()] = text.strip()\n",
        "\n",
        "    # Extract answer text (using string manipulation or regex)\n",
        "    answer_text = entry['answer'].strip().split(\".\")[0]  # Split at the first \".\" and take the first part\n",
        "\n",
        "    # Find the correct answer letter (using word-based matching)\n",
        "    correct_answer_letter = None\n",
        "    for letter, option_text in options.items():\n",
        "        for word in answer_text.split():  # Iterate through words in the answer\n",
        "            if word in option_text:  # Check if the word is present in the option text\n",
        "                correct_answer_letter = letter\n",
        "                break  # Stop searching if a match is found\n",
        "        if correct_answer_letter:  # Stop searching options if a match is found\n",
        "            break\n",
        "\n",
        "    correct_answer = correct_answer_letter\n",
        "\n",
        "    # Define source information (using placeholders)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": \"000-0000000000-p00-para06\"\n",
        "    }\n",
        "\n",
        "    # Construct the formatted entry\n",
        "    formatted_entry = {\n",
        "        \"correct_answer\": correct_answer,  # Use the found letter\n",
        "        \"options\": options,  # Use the formatted options dictionary\n",
        "        \"question\": question,\n",
        "        \"source\": source,\n",
        "        \"type\": \"multiple_choice\"\n",
        "    }\n",
        "    transformed_data_MC4.append(formatted_entry)\n",
        "\n",
        "# Print first 3 formatted entries for verification (optional)\n",
        "print(json.dumps(transformed_data_MC4[:3], indent=4))\n",
        "\n",
        "transformed_MC4_data = transformed_data_MC4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6Tog88KCudq",
        "outputId": "86230c33-c022-4324-fc03-27d0538c0562"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"correct_answer\": \"C\",\n",
            "        \"options\": {\n",
            "            \"A\": \"Deep transverse Perineus\",\n",
            "            \"B\": \"Perinial membrane\",\n",
            "            \"C\": \"Colle's fascia\",\n",
            "            \"D\": \"Sphincter Urethrae\"\n",
            "        },\n",
            "        \"question\": \"Urogenital Diaphragm is made up of the following, except:\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para06\"\n",
            "        },\n",
            "        \"type\": \"multiple_choice\"\n",
            "    },\n",
            "    {\n",
            "        \"correct_answer\": \"A\",\n",
            "        \"options\": {\n",
            "            \"A\": \"After 5 years\",\n",
            "            \"B\": \"After 2 years\",\n",
            "            \"C\": \"After 10 years\",\n",
            "            \"D\": \"At the time of diagnosis\"\n",
            "        },\n",
            "        \"question\": \"Child with Type I Diabetes. What is the advised time for fundus examinations from the time of diagnosis?\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para06\"\n",
            "        },\n",
            "        \"type\": \"multiple_choice\"\n",
            "    },\n",
            "    {\n",
            "        \"correct_answer\": \"A\",\n",
            "        \"options\": {\n",
            "            \"A\": \"Fecal antigen test\",\n",
            "            \"B\": \"Biopsy urease test\",\n",
            "            \"C\": \"Serological test\",\n",
            "            \"D\": \"Urea breath test\"\n",
            "        },\n",
            "        \"question\": \"Most sensitive test for H pylori is-\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para06\"\n",
            "        },\n",
            "        \"type\": \"multiple_choice\"\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama"
      ],
      "metadata": {
        "id": "S675ghjRWI5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Anzahl der Wiederholungsversuche für das Laden des Datensatzes\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Dataset laden mit Wiederholungslogik\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"UCSC-VLAA/MedReason\", split=\"train\")\n",
        "        break\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "# Funktion zur Generierung von Argumentationsschritten\n",
        "def generate_reasoning_steps(question, answer):\n",
        "    prompt = f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "    response = llama_pipeline(prompt, max_length=512, do_sample=True)\n",
        "    steps = response[0][\"generated_text\"].split(\"\\n\")\n",
        "    reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(steps) if step]\n",
        "    return reasoning\n",
        "\n",
        "transformed_data_R2 = []\n",
        "\n",
        "# Process each entry in the dataset\n",
        "for entry in dataset:\n",
        "    question = entry['question'].strip()\n",
        "    answer = entry['answer'].strip()\n",
        "\n",
        "    # Use CodeLlama to generate reasoning\n",
        "    reasoning = generate_reasoning_steps(question, answer)\n",
        "\n",
        "    # Define source information (using placeholders)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": \"000-0000-p00-para01\"\n",
        "    }\n",
        "\n",
        "    # Define the type based on reasoning complexity\n",
        "    response_type = \"multi_hop\"\n",
        "\n",
        "    # Construct the formatted entry\n",
        "    formatted_entry = {\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"answer\": answer,\n",
        "        \"source\": source,\n",
        "        \"type\": response_type\n",
        "    }\n",
        "\n",
        "    transformed_data_R2.append(formatted_entry)\n",
        "\n",
        "# Print first 3 formatted entries\n",
        "#print(json.dumps(transformed_data_R2[:3], indent=4))"
      ],
      "metadata": {
        "id": "zdAWp0_lWKNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy model for sentence segmentation\n",
        "\"\"\"nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Number of retry attempts\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Attempt to load the dataset with retry logic\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"YvanAlvin/medicalQALlama2\", split=\"train\")\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "transformed_data_R3 = []\n",
        "\n",
        "# Process each item in the dataset\n",
        "for item in dataset:\n",
        "    text = item[\"text\"]\n",
        "\n",
        "    # Extract the question from inside [INST]...[/INST]\n",
        "    match = re.search(r'\\[INST\\](.*?)\\[/INST\\]', text, re.DOTALL)\n",
        "    question = match.group(1).strip() if match else \"Unknown Question\"\n",
        "\n",
        "    # Extract the answer as everything after [/INST]\n",
        "    answer = text.split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "    # Use spaCy to segment the answer into sentences\n",
        "    doc = nlp(answer)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents]\n",
        "\n",
        "    # If multiple sentences exist, format them as reasoning steps\n",
        "    if len(sentences) > 1:\n",
        "        reasoning = [f\"Step {i+1}: {sent}\" for i, sent in enumerate(sentences[:-1])]\n",
        "        final_answer = sentences[-1]  # Last sentence is the final answer\n",
        "    else:\n",
        "        reasoning = [\"No explicit reasoning found.\"]\n",
        "        final_answer = answer\n",
        "\n",
        "    formatted_item = {\n",
        "        \"answer\": final_answer,\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "\n",
        "    transformed_data_R3.append(formatted_item)\n",
        "\n",
        "# Print the first 3 formatted entries\n",
        "#print(transformed_data_R3[:3])\n",
        "transformed_R3_data = transformed_data_R3"
      ],
      "metadata": {
        "id": "xkLepzfE3RI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy model for sentence segmentation\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Number of retry attempts\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Attempt to load the dataset with retry logic\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"YvanAlvin/medicalchat200llama2\", split=\"train\")\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "transformed_data_R4 = []\n",
        "\n",
        "# Process each item in the dataset\n",
        "for item in dataset:\n",
        "    text = item[\"text\"]\n",
        "\n",
        "    # Extract the question from inside [INST]...[/INST]\n",
        "    match = re.search(r'\\[INST\\](.*?)\\[/INST\\]', text, re.DOTALL)\n",
        "    question = match.group(1).strip() if match else \"Unknown Question\"\n",
        "\n",
        "    # Extract the answer as everything after [/INST]\n",
        "    answer = text.split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "    # Use spaCy to segment the answer into sentences\n",
        "    doc = nlp(answer)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents]\n",
        "\n",
        "    # If multiple sentences exist, format them as reasoning steps\n",
        "    if len(sentences) > 1:\n",
        "        reasoning = [f\"Step {i+1}: {sent}\" for i, sent in enumerate(sentences[:-1])]\n",
        "        final_answer = sentences[-1]  # Last sentence is the final answer\n",
        "    else:\n",
        "        reasoning = [\"No explicit reasoning found.\"]\n",
        "        final_answer = answer\n",
        "\n",
        "    formatted_item = {\n",
        "        \"answer\": final_answer,\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "\n",
        "    transformed_data_R4.append(formatted_item)\n",
        "\n",
        "# Print the first 3 formatted entries\n",
        "#print(transformed_data_R4[:3])\n",
        "\n",
        "transformed_R4_data = transformed_data_R4"
      ],
      "metadata": {
        "id": "KnhT3qXA3RF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy model for sentence segmentation\n",
        "\"\"\"nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Number of retry attempts\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Attempt to load the dataset with retry logic\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"eashuu/medical_qa\", split=\"train\")\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "transformed_data_R5 = []\n",
        "\n",
        "# Process each item in the dataset\n",
        "for item in dataset:\n",
        "    question = item[\"instruction\"].replace(\"Q. \", \"\", 1).strip()\n",
        "    answer = item[\"output\"].strip()\n",
        "\n",
        "    # Use spaCy to segment answer into sentences\n",
        "    doc = nlp(answer)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents]\n",
        "\n",
        "    # If multiple sentences exist, format them as reasoning steps\n",
        "    if len(sentences) > 1:\n",
        "        reasoning = [f\"Step {i+1}: {sent}\" for i, sent in enumerate(sentences[:-1])]\n",
        "        final_answer = sentences[-1]  # Last sentence is the final answer\n",
        "    else:\n",
        "        reasoning = [\"No explicit reasoning found.\"]\n",
        "        final_answer = answer\n",
        "\n",
        "    formatted_item = {\n",
        "        \"answer\": final_answer,\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "\n",
        "    transformed_data_R5.append(formatted_item)\n",
        "\n",
        "# Print the first 3 formatted entries\n",
        "#print(transformed_data_R5[:3])\n",
        "\n",
        "transformed_R5_data = transformed_data_R5"
      ],
      "metadata": {
        "id": "TL67HAmP3RCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama"
      ],
      "metadata": {
        "id": "-UDM0ZIyVZMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Anzahl der Wiederholungsversuche für das Laden des Datensatzes\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Dataset laden mit Wiederholungslogik\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"eashuu/medical_qa\", split=\"train\")\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "# Funktion zur Generierung von Argumentationsschritten\n",
        "def generate_reasoning_steps(question, answer):\n",
        "    prompt = f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "    response = llama_pipeline(prompt, max_length=512, do_sample=True)\n",
        "    steps = response[0][\"generated_text\"].split(\"\\n\")  # Schritte extrahieren\n",
        "    reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(steps) if step]  # Formatieren\n",
        "    return reasoning\n",
        "\n",
        "# Modify processing logic\n",
        "transformed_data_R5 = []\n",
        "\n",
        "for item in dataset:\n",
        "    question = item[\"instruction\"].replace(\"Q. \", \"\", 1).strip()\n",
        "    answer = item[\"output\"].strip()\n",
        "\n",
        "    # CodeLlama für Argumentation verwenden\n",
        "    reasoning = generate_reasoning_steps(question, answer)\n",
        "\n",
        "    formatted_item = {\n",
        "        \"answer\": answer,\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "\n",
        "    transformed_data_R5.append(formatted_item)"
      ],
      "metadata": {
        "id": "mGRo0VieXQDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of retry attempts\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\")\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "transformed_data_R6 = []\n",
        "\n",
        "# Process each item in the dataset\n",
        "for item in dataset[\"train\"]:\n",
        "    question = item[\"Question\"].strip()\n",
        "    reasoning_text = item[\"Complex_CoT\"].strip()\n",
        "    answer = item[\"Response\"].strip()\n",
        "\n",
        "    # If no reasoning is provided, set a default message\n",
        "    if not reasoning_text:\n",
        "        reasoning = [\"Step 1: No explicit reasoning found.\"]\n",
        "    else:\n",
        "        # Split reasoning into steps (heuristic: split by \". \" or newlines)\n",
        "        raw_steps = [step.strip() for step in reasoning_text.split(\". \") if step]\n",
        "        reasoning = [f\"Step {i+1}: {step}\" for i, step in enumerate(raw_steps)]\n",
        "\n",
        "    formatted_item = {\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"answer\": answer,\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "\n",
        "    transformed_data_R6.append(formatted_item)\n",
        "\n",
        "# Print the first 3 formatted entries\n",
        "#print(transformed_data_R6[:3])\n",
        "\n",
        "transformed_R6_data = transformed_data_R6"
      ],
      "metadata": {
        "id": "RL-Gv1Yb3Q7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy model for sentence segmentation\n",
        "\"\"\"nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Number of retry attempts\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Attempt to load the dataset with retry logic\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"wentechno/medicalQA-50thPlus\", split=\"train\")\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "transformed_data_R7 = []\n",
        "\n",
        "# Process each item in the dataset\n",
        "for item in dataset:\n",
        "    question = item[\"instruction\"].replace(\"Q. \", \"\", 1).strip()\n",
        "    answer = item[\"output\"].strip()\n",
        "\n",
        "    # Use spaCy to segment answer into sentences\n",
        "    doc = nlp(answer)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents]\n",
        "\n",
        "    # If multiple sentences exist, format them as reasoning steps\n",
        "    if len(sentences) > 1:\n",
        "        reasoning = [f\"Step {i+1}: {sent}\" for i, sent in enumerate(sentences[:-1])]\n",
        "        final_answer = sentences[-1]  # Last sentence is the final answer\n",
        "    else:\n",
        "        reasoning = [\"No explicit reasoning found.\"]\n",
        "        final_answer = answer\n",
        "\n",
        "    formatted_item = {\n",
        "        \"answer\": final_answer,\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "\n",
        "    transformed_data_R7.append(formatted_item)\n",
        "\n",
        "# Print the first 3 formatted entries\n",
        "#print(transformed_data_R7[:3])\n",
        "\n",
        "transformed_R7_data = transformed_data_R7"
      ],
      "metadata": {
        "id": "-0G3GJgR3QqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama"
      ],
      "metadata": {
        "id": "PYnz8wurX3tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Anzahl der Wiederholungsversuche für das Laden des Datensatzes\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Dataset laden mit Wiederholungslogik\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"wentechno/medicalQA-50thPlus\", split=\"train\")\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "# Funktion zur Generierung von Argumentationsschritten\n",
        "def generate_reasoning_steps(question, answer):\n",
        "    prompt = f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "    response = llama_pipeline(prompt, max_length=512, do_sample=True)\n",
        "    steps = response[0][\"generated_text\"].split(\"\\n\")  # Schritte extrahieren\n",
        "    reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(steps) if step]  # Formatieren\n",
        "    return reasoning\n",
        "\n",
        "# Modify processing logic\n",
        "transformed_data_R7 = []\n",
        "\n",
        "for item in dataset:\n",
        "    question = item[\"instruction\"].replace(\"Q. \", \"\", 1).strip()\n",
        "    answer = item[\"output\"].strip()\n",
        "\n",
        "    # CodeLlama für Argumentation verwenden\n",
        "    reasoning = generate_reasoning_steps(question, answer)\n",
        "\n",
        "    formatted_item = {\n",
        "        \"answer\": answer,\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "\n",
        "    transformed_data_R7.append(formatted_item)"
      ],
      "metadata": {
        "id": "TGNLQX7wX3Lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset(\"Ajayaadhi/Medical-QA\")\n",
        "\n",
        "# Initialize a list to hold the reformatted entries\n",
        "transformed_data_short_answer2 = []\n",
        "\n",
        "for entry in dataset[\"train\"]:  # Process all entries\n",
        "    text = entry[\"train\"]  # Adjust this if the key is different\n",
        "\n",
        "    # Extract question\n",
        "    question_match = re.search(r\"### Input:\\n(.+?)\\n\\[INST\\]\", text, re.DOTALL)\n",
        "    question = question_match.group(1).strip() if question_match else \"\"\n",
        "\n",
        "    # Extract answer\n",
        "    answer_match = re.search(r\"### Response:\\n(.+?)</s>\", text, re.DOTALL)\n",
        "    answer = answer_match.group(1).strip() if answer_match else \"\"\n",
        "\n",
        "    # Define source information (using placeholders)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": \"000-0000000-p00-para01\"\n",
        "    }\n",
        "\n",
        "    # Determine response type\n",
        "    response_type = \"short_answer\"\n",
        "\n",
        "    # Construct the reformatted entry\n",
        "    reformatted_entry = {\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"source\": source,\n",
        "        \"type\": response_type\n",
        "    }\n",
        "\n",
        "    # Append to the list\n",
        "    transformed_data_short_answer2.append(reformatted_entry)\n",
        "\n",
        "# Print the first 3 formatted entries\n",
        "transformed_short_answer2_data = transformed_data_short_answer2\n",
        "print(json.dumps(transformed_short_answer1_data[:4], indent = 4))"
      ],
      "metadata": {
        "id": "TWOpCxIu3QYq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830,
          "referenced_widgets": [
            "5de7c984e82f46fba53d776bf23ebf2b",
            "beec22736d624e78b9b8281594974ccd",
            "e82bb5cf67104e198fd833ca711bff19",
            "34db7cc336cf414faf365c878e9b1013",
            "e31aaaad98b84df7a644b56e1a88f878",
            "89e48d08bb16461b87e72637c82fc1d9",
            "d419e305d52141bfa1467df5a2212f2a",
            "80c2a4299b184845bf99db5b9184788c",
            "9105b24244f2444eb091e6f1941e00a7",
            "10f511408624461db907b666a60e9aa3",
            "b4c3811ed7af4a74a6ab78b008b25172",
            "592e28249b2a494b92e0c4168659eeec",
            "4bada59db17645db8fc3ad8b62b64ec7",
            "78bb9afcf7c844cf8fda5de188e97de6",
            "c35ed3c940314645bc304e2ecf8ec3fb",
            "d16c633ce5ae479e8c11fd06af647e5d",
            "958690204aef459f82c07a5fbb2e337c",
            "f9ee20c06ec14ed5a94dfe7bb84bc952",
            "0fe107d97bcd42bd87b44270bb35ff75",
            "9808ebe843a347458ce1a1a5ddfd6c08",
            "dfb6ff50b4574129ad64026c1c860559",
            "ab9527d0bafc45b893f33653be99672c"
          ]
        },
        "outputId": "dae5c2a2-ed36-4a55-d8eb-f99aedbc3053"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train_data.csv:   0%|          | 0.00/41.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5de7c984e82f46fba53d776bf23ebf2b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/49897 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "592e28249b2a494b92e0c4168659eeec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"answer\": \"Tell the attending that he cannot fail to disclose this mistake\",\n",
            "        \"question\": \"A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the procedure, the resident inadvertently cuts a flexor tendon, which is subsequently repaired without complication. The attending advises the resident not to report this complication in the operative report, stating that disclosure may unnecessarily worry the patient. What is the appropriate next action for the resident to take in this situation?\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para26\"\n",
            "        },\n",
            "        \"type\": \"short_answer\"\n",
            "    },\n",
            "    {\n",
            "        \"answer\": \"Cross-linking of DNA\",\n",
            "        \"question\": \"A 67-year-old man with transitional cell carcinoma of the bladder comes to the physician because of a 2-day history of ringing sensation in his ear. He received his first course of neoadjuvant chemotherapy 1 week ago. Pure tone audiometry shows a sensorineural hearing loss of 45 dB. What mechanism of action is responsible for the therapeutic benefit of the chemotherapeutic agent most likely causing this patient's symptoms?\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para26\"\n",
            "        },\n",
            "        \"type\": \"short_answer\"\n",
            "    },\n",
            "    {\n",
            "        \"answer\": \"Cholesterol embolization\",\n",
            "        \"question\": \"A 61-year-old man with type 2 diabetes mellitus and osteoarthritis presents with decreased urinary output and malaise two weeks after undergoing emergency cardiac catheterization with stenting for unstable angina. His medications include insulin, naproxen, aspirin, clopidogrel, and metoprolol. Vital signs show a temperature of 38\\u00b0C (100.4\\u00b0F), pulse 93/min, and blood pressure 125/85 mm Hg. Physical examination reveals mottled, reticulated purplish discoloration of the feet. Laboratory studies demonstrate leukocytosis with eosinophilia, elevated serum urea nitrogen and creatinine, and a renal biopsy showing intravascular spindle-shaped vacuoles. What is the most likely diagnosis for this patient's condition?\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para26\"\n",
            "        },\n",
            "        \"type\": \"short_answer\"\n",
            "    },\n",
            "    {\n",
            "        \"answer\": \"Lactose-fermenting, gram-negative rods forming pink colonies on MacConkey agar\",\n",
            "        \"question\": \"A 39-year-old woman is brought to the emergency department with fevers, chills, and left lower quadrant pain. Her vital signs show a temperature of 39.1\\u00b0C (102.3\\u00b0F), pulse 126/min, respirations 28/min, and blood pressure 80/50 mm Hg. Physical examination reveals blood oozing at an IV site, mucopurulent cervical discharge, and left adnexal tenderness. Laboratory results include a platelet count of 14,200/mm\\u00b3, fibrinogen 83 mg/mL, D-dimer 965 ng/mL, and a positive phenol test identifying a phosphorylated N-acetylglucosamine dimer with six fatty acids. What Gram stain characteristics and colony morphology on MacConkey agar would most likely be observed in the organism causing this infection?\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000000-p00-para26\"\n",
            "        },\n",
            "        \"type\": \"short_answer\"\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Number of retry attempts\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"KaungHtetCho/MedicalQA\")\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "transformed_data_R8 = []\n",
        "\n",
        "# Process each entry in the dataset\n",
        "for entry in dataset['train']:\n",
        "    description = entry['Description'].strip()\n",
        "    patient = entry['Patient'].strip()\n",
        "    doctor = entry['Doctor'].strip()\n",
        "\n",
        "    # Form the question from description + patient details\n",
        "    question = f\"{description}\\n\\n{patient}\"\n",
        "\n",
        "    # Split the doctor's response into reasoning steps\n",
        "    if doctor:\n",
        "        raw_steps = [step.strip() for step in doctor.split(\". \") if step]\n",
        "        reasoning = [f\"Step {i+1}: {step}\" for i, step in enumerate(raw_steps)]\n",
        "    else:\n",
        "        reasoning = [\"Step 1: No explicit reasoning found.\"]\n",
        "\n",
        "    # Define source information (using placeholders)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": \"000-0000-p00-para01\"\n",
        "    }\n",
        "\n",
        "    # Define the type based on reasoning complexity\n",
        "    response_type = \"multi_hop\"\n",
        "\n",
        "    # Construct the formatted entry\n",
        "    formatted_entry = {\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"answer\": reasoning[-1] if reasoning else \"No answer provided.\",\n",
        "        \"source\": source,\n",
        "        \"type\": response_type\n",
        "    }\n",
        "\n",
        "    transformed_data_R8.append(formatted_entry)\n",
        "# Print first 3 formatted entries\n",
        "#print(json.dumps(transformed_data_R8[:3], indent=4))\n",
        "transformed_R8_data = transformed_data_R8"
      ],
      "metadata": {
        "id": "WcTqxqw7SwaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama"
      ],
      "metadata": {
        "id": "pKWclgB7YuTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Anzahl der Wiederholungsversuche für das Laden des Datensatzes\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Dataset laden mit Wiederholungslogik\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"KaungHtetCho/MedicalQA\", split=\"train\")\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "\n",
        "# Funktion zur Generierung von Argumentationsschritten\n",
        "def generate_reasoning_steps(question, answer):\n",
        "    prompt = f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "    response = llama_pipeline(prompt, max_length=512, do_sample=True)\n",
        "    steps = response[0][\"generated_text\"].split(\"\\n\")\n",
        "    reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(steps) if step]\n",
        "    return reasoning\n",
        "\n",
        "transformed_data_R8 = []\n",
        "\n",
        "# Process each entry in the dataset\n",
        "for entry in dataset:\n",
        "    description = entry['Description'].strip()\n",
        "    patient = entry['Patient'].strip()\n",
        "    #doctor = entry['Doctor'].strip()  # Not used for answer anymore\n",
        "\n",
        "    # Form the question from description + patient details\n",
        "    question = f\"{description}\\n\\n{patient}\"\n",
        "\n",
        "    # Generate answer using CodeLlama\n",
        "    answer = llama_pipeline(question, max_length=256, do_sample=True)[0][\"generated_text\"].strip()\n",
        "\n",
        "    # Generate reasoning using CodeLlama\n",
        "    reasoning = generate_reasoning_steps(question, answer)\n",
        "\n",
        "\n",
        "    # Define source information (using placeholders)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": \"000-0000-p00-para01\"\n",
        "    }\n",
        "\n",
        "    # Define the type based on reasoning complexity\n",
        "    response_type = \"multi_hop\"\n",
        "\n",
        "    # Construct the formatted entry\n",
        "    formatted_entry = {\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"answer\": answer,  # Using generated answer\n",
        "        \"source\": source,\n",
        "        \"type\": response_type\n",
        "    }\n",
        "\n",
        "    transformed_data_R8.append(formatted_entry)\n",
        "\n",
        "# Print first 3 formatted entries\n",
        "#print(json.dumps(transformed_data_R8[:3], indent=4))"
      ],
      "metadata": {
        "id": "0mlk9Fz6Yfae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Anzahl der Wiederholungsversuche für das Laden des Datensatzes\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Dataset laden mit Wiederholungslogik\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"qiaojin/PubMedQA\", \"pqa_unlabeled\")\n",
        "        break\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "# Funktion zur Generierung von Argumentationsschritten\n",
        "def generate_reasoning_steps(question, answer):\n",
        "    prompt = f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "    response = llama_pipeline(prompt, max_length=512, do_sample=True)\n",
        "    steps = response[0][\"generated_text\"].split(\"\\n\")\n",
        "    reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(steps) if step]\n",
        "    return reasoning\n",
        "\n",
        "transformed_data_R9 = []\n",
        "\n",
        "# Process each entry in the dataset\n",
        "for entry in dataset[\"train\"]:\n",
        "    question = entry['question'].strip()\n",
        "    answer = entry['long_answer'].strip()\n",
        "\n",
        "    # Use CodeLlama to generate reasoning\n",
        "    reasoning = generate_reasoning_steps(question, answer)\n",
        "\n",
        "    # Define source information (using placeholders)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": \"000-0000-p00-para01\"\n",
        "    }\n",
        "\n",
        "    # Define the type based on reasoning complexity\n",
        "    response_type = \"multi_hop\"\n",
        "\n",
        "    # Construct the formatted entry\n",
        "    formatted_entry = {\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"answer\": answer,\n",
        "        \"source\": source,\n",
        "        \"type\": response_type\n",
        "    }\n",
        "\n",
        "    transformed_data_R9.append(formatted_entry)\n",
        "transformed_R9_data = transformed_data_R9\n",
        "# Print first 3 formatted entries\n",
        "print(json.dumps(transformed_data_R9[:3], indent=4))"
      ],
      "metadata": {
        "id": "5itlewIe7gT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA FROM KAGGLE"
      ],
      "metadata": {
        "id": "Wpt4rFU37RaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle pandas\n"
      ],
      "metadata": {
        "id": "Vpzmmvo53jYO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ef269e6-5100-483f-b784-a9b53217c1cd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.1.31)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.3.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d thedevastator/comprehensive-medical-q-a-dataset --unzip\n"
      ],
      "metadata": {
        "id": "gQad59AC3wJ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faeed5b6-ef61-4eda-fe6c-2474d68f25ef"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/thedevastator/comprehensive-medical-q-a-dataset\n",
            "License(s): CC0-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import auth\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "# Authenticate with Kaggle API\n",
        "os.environ['KAGGLE_USERNAME'] = \"apfresh\" # Replace with your username\n",
        "os.environ['KAGGLE_KEY'] = \"50af00b12093dc762e1d2d1c138dd817\"\n",
        "api = KaggleApi()\n",
        "api.authenticate()"
      ],
      "metadata": {
        "id": "7p2ULHnJ67jX"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset (assuming it's already available in Colab)\n",
        "file_path = \"/content/train.csv\"  # Adjust based on actual filename\n",
        "dataset = pd.read_csv(file_path)\n",
        "\n",
        "# Initialize a list to hold the reformatted entries\n",
        "transformed_data_short_answer3 = []\n",
        "\n",
        "# Iterate over the dataset entries\n",
        "for index, entry in dataset.iterrows():\n",
        "    question = entry.get('Question', '').strip()\n",
        "    answer = entry.get('Answer', '').strip()\n",
        "\n",
        "    # Define source information (using placeholders here)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": f\"000-0000000-p00-para{index+1:02d}\"\n",
        "    }\n",
        "\n",
        "    # Determine the response type based on answer length\n",
        "    response_type = \"short_answer\"\n",
        "\n",
        "    # Construct the reformatted entry\n",
        "    reformatted_entry = {\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"source\": source,\n",
        "        \"type\": response_type\n",
        "    }\n",
        "\n",
        "    # Append to the list\n",
        "    transformed_data_short_answer3.append(reformatted_entry)\n",
        "# Convert the list to JSON format and print the first 3 entries\n",
        "#print(json.dumps(transformed_data_short_answer3[:3], indent=4))\n",
        "transformed_short_answer3_data = transformed_data_short_answer3\n",
        "print(json.dumps(transformed_short_answer3_data[:4], indent=4))"
      ],
      "metadata": {
        "id": "IJeGfraN5jJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d pythonafroz/medquad-medical-question-answer-for-ai-research --unzip\n"
      ],
      "metadata": {
        "id": "RBfFVCdr7Ajw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5974d0b0-ed26-4104-eb62-ea1c11447258"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/pythonafroz/medquad-medical-question-answer-for-ai-research\n",
            "License(s): CC-BY-SA-4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset####REASONING#######\n",
        "file_path = \"/content/medquad.csv\"  # Adjust based on the actual filename\n",
        "dataset = pd.read_csv(file_path)\n",
        "\n",
        "# Initialize a list to hold the reformatted entries\n",
        "transformed_data_short_answer4 = []\n",
        "\n",
        "# Iterate over the dataset entries\n",
        "for index, entry in dataset.iterrows():\n",
        "    question = entry.get('Question', '').strip()\n",
        "    answer = entry.get('Answer', '').strip()\n",
        "\n",
        "    # Define source information (using placeholders here)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": f\"000-0000000-p00-para{index+1:02d}\"\n",
        "    }\n",
        "\n",
        "    # Determine the response type based on answer length\n",
        "    response_type = \"short_answer\"\n",
        "\n",
        "    # Construct the reformatted entry\n",
        "    reformatted_entry = {\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"source\": source,\n",
        "        \"type\": response_type\n",
        "    }\n",
        "\n",
        "    # Append to the list\n",
        "    transformed_data_short_answer4.append(reformatted_entry)\n",
        "# Convert the list to JSON format and print the first 3 entries\n",
        "transformed_short_answer4_data = transformed_data_short_answer4\n",
        "print(json.dumps(transformed_short_answer4_data[:3], indent=4))"
      ],
      "metadata": {
        "id": "yJIs-8SU58bi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01a5293b-19ca-4748-c946-c6eff2db1c3a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"question\": \"\",\n",
            "        \"answer\": \"\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000-p00-para01\"\n",
            "        },\n",
            "        \"type\": \"short_answer\"\n",
            "    },\n",
            "    {\n",
            "        \"question\": \"\",\n",
            "        \"answer\": \"\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000-p00-para02\"\n",
            "        },\n",
            "        \"type\": \"short_answer\"\n",
            "    },\n",
            "    {\n",
            "        \"question\": \"\",\n",
            "        \"answer\": \"\",\n",
            "        \"source\": {\n",
            "            \"isbn\": \"000-0000000000\",\n",
            "            \"page\": 0,\n",
            "            \"paragraph_id\": \"000-0000000-p00-para03\"\n",
            "        },\n",
            "        \"type\": \"short_answer\"\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LOCAL DATA"
      ],
      "metadata": {
        "id": "yLVNoX6_Ehrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from io import StringIO\n",
        "\n",
        "#not working\n",
        "csv = \"/content/true_false_questions.csv\"\n",
        "\n",
        "# Fetch the CSV data\n",
        "try:\n",
        "    df = pd.read_csv(csv)  # Read CSV directly from Colab environment\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found: {csv}. Please make sure it is uploaded to Colab.\")\n",
        "    exit()\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse the CSV file: {csv}. Please check its format.\")\n",
        "    exit()\n",
        "\n",
        "df = pd.read_csv(csv)\n",
        "# Transform data\n",
        "transformed_data_TF1 = []\n",
        "for _, row in df.iterrows():\n",
        "    formatted_item = {\n",
        "        \"question\": row[\"text\"],  # Extract question\n",
        "        \"answer\": str(row[\"label\"]),  # Extract answer as string\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",  # Placeholder value\n",
        "            \"page\": 0,  # Placeholder value\n",
        "            \"paragraph_id\": \"000-0000000000-p00-paraXX\"  # Placeholder value\n",
        "        },\n",
        "        \"type\": \"true_false\"\n",
        "    }\n",
        "    transformed_data_TF1.append(formatted_item)\n",
        "\n",
        "# Save formatted data to JSON\n",
        "transformed_TF1_data = transformed_data_TF1"
      ],
      "metadata": {
        "id": "O64XzUf2ifuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ENTIRE DATASET"
      ],
      "metadata": {
        "id": "w_wLy9bMCy9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "lists = [transformed_TF1_data, transformed_TF2_data, transformed_short_answer1_data, transformed_short_answer2_data, transformed_short_answer3_data, transformed_short_answer4_data,\n",
        "        transformed_MC1_data, transformed_MC2_data, transformed_MC3_data, transformed_MC4_data,\n",
        "        transformed_R1_data, transformed_R2_data,transformed_R3_data,transformed_R4_data,transformed_R5_data,transformed_R6_data,transformed_R7_data,transformed_R8_data, transformed_R9_data ]\n",
        "DATA = list(itertools.chain(*lists))\n"
      ],
      "metadata": {
        "id": "7TdXFkBOra0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removes duplicates, tokenization, stopwords, lemmatization, padding\n",
        "\n",
        "# Handle Class Imbalance:\n",
        "\n",
        "#    SMOTE will help generate synthetic samples for underrepresented classes in the dataset.\n",
        "\n",
        "#    Class Weights can be used in the model to give more importance to underrepresented classes during training.\n",
        "\n",
        "# Paraphrasing / Question Modification:\n",
        "\n",
        "#    We will use a GPT-based model to paraphrase or modify questions to generate additional training samples."
      ],
      "metadata": {
        "id": "OUPfGmrhCdpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SPLIT BY TYPE and SAVE .ZIP TO REPOSITORY"
      ],
      "metadata": {
        "id": "h3W5LepvH9ky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "\n",
        "    Ensure the data is preprocessed appropriately for each prompt style before creating the datasets.\n",
        "    Adjust the batch size, training steps, epochs, and other hyperparameters to find the best performance.\n",
        "    Regularly evaluate the performance on your test set for each prompt style.\n",
        "    This example assumes your prompt styles data is readily available.\n",
        "    Remember to make necessary imports and data modifications for smooth execution.\n"
      ],
      "metadata": {
        "id": "Ui9VFHI3-ZHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "Ad-Pq5_GkFvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Lemmatizes, removes stopwords, and lowercases the input text.\"\"\"\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.lower() not in stop_words and token.isalnum()]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def remove_duplicates_and_empty(data):\n",
        "    \"\"\"Removes duplicates and entries with empty questions or answers.\"\"\"\n",
        "    seen_pairs = set()\n",
        "    filtered_data = []\n",
        "    for entry in data:\n",
        "        q, a = entry[\"question\"], entry[\"answer\"]\n",
        "\n",
        "        if not q or not a:  # Remove if empty\n",
        "            continue\n",
        "\n",
        "        pair = (q.strip().lower(), a.strip().lower())  # Normalize case for comparison\n",
        "        if pair not in seen_pairs:\n",
        "            seen_pairs.add(pair)\n",
        "            filtered_data.append(entry)\n",
        "    return filtered_data"
      ],
      "metadata": {
        "id": "cP519IeiB1rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#It's usually better to apply stopword removal and lemmatization before removing duplicates\n",
        "preprocessed_data = [{k: preprocess_text(v) if isinstance(v, str) else v\n",
        "                       for k, v in d.items()} for d in DATA]\n",
        "preprocessed_data = remove_duplicates_and_empty(preprocessed_data)  # Apply to DATA"
      ],
      "metadata": {
        "id": "0DIk_DxVD8Rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split in Test and Train"
      ],
      "metadata": {
        "id": "rGRfqej7jn_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Group by Question Type\n",
        "grouped_data = {}\n",
        "for item in preprocessed_data:\n",
        "    question_type = item['type']\n",
        "    if question_type not in grouped_data:\n",
        "        grouped_data[question_type] = []\n",
        "    grouped_data[question_type].append(item)\n",
        "\n",
        "# 2. Stratified Split within Each Group\n",
        "train_data = []\n",
        "test_data = []\n",
        "for question_type, data in grouped_data.items():\n",
        "    # Create a temporary DataFrame for easier stratification (optional)\n",
        "    df = pd.DataFrame(data)\n",
        "    # Perform stratified split, using 'type' column for stratification\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['type'], random_state=42)  # Adjust test_size as needed\n",
        "    # Append the split data to the overall train and test sets\n",
        "    train_data.extend(train_df.to_dict('records'))\n",
        "    test_data.extend(test_df.to_dict('records'))\n",
        "\n",
        "# 3. Combine Splits\n",
        "# Now you have train_data and test_data with equal representation of question types\n",
        "print(f\"Train data size: {len(train_data)}\")\n",
        "print(f\"Test data size: {len(test_data)}\")"
      ],
      "metadata": {
        "id": "m1ZpZyEHjmxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting before formatting avoids potential data leakage, where information from the test set might influence the model during training."
      ],
      "metadata": {
        "id": "M8ynijmQnGex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format training data\n",
        "train_short_answer_data = [\n",
        "    {\n",
        "        \"input\": f\"Answer the following question:\\n{d['question']}\",\n",
        "        \"output\": d[\"answer\"]\n",
        "    }\n",
        "    for d in train_data if d['type'] == 'short_answer'\n",
        "]\n",
        "\n",
        "train_multi_hop_data = [\n",
        "    {\n",
        "        \"input\": f\"Answer the following multi-hop question:\\n{d['question']}\",\n",
        "        \"output\": f\"{d['reasoning']}\\nAnswer: {d['answer']}\"\n",
        "    }\n",
        "    for d in train_data if d['type'] == 'multi_hop'\n",
        "]\n",
        "\n",
        "train_true_false_data = [\n",
        "    {\n",
        "        \"input\": f\"Is the following statement true or false?\\nStatement: {d['question']}\",\n",
        "        \"output\": d[\"answer\"]\n",
        "    }\n",
        "    for d in train_data if d['type'] == 'true_false'\n",
        "]\n",
        "\n",
        "train_multiple_choice_data = [\n",
        "    {\n",
        "        \"input\": f\"Choose the correct option:\\nQuestion: {d['question']}\\nOptions:\\n\" +\n",
        "                 '\\n'.join([f\"{chr(65+i)}) {opt}\" for i, opt in enumerate(d['options'])]),\n",
        "        \"output\": d[\"correct_answer\"]\n",
        "    }\n",
        "    for d in train_data if d['type'] == 'multiple_choice'\n",
        "]\n",
        "\n",
        "# Format testing data\n",
        "test_short_answer_data = [\n",
        "    {\n",
        "        \"input\": f\"Answer the following question:\\n{d['question']}\",\n",
        "        \"output\": d[\"answer\"]\n",
        "    }\n",
        "    for d in test_data if d['type'] == 'short_answer'\n",
        "]\n",
        "\n",
        "test_multi_hop_data = [\n",
        "    {\n",
        "        \"input\": f\"Answer the following multi-hop question:\\n{d['question']}\",\n",
        "        \"output\": f\"{d['reasoning']}\\nAnswer: {d['answer']}\"\n",
        "    }\n",
        "    for d in test_data if d['type'] == 'multi_hop'\n",
        "]\n",
        "\n",
        "\n",
        "test_true_false_data = [\n",
        "    {\n",
        "        \"input\": f\"Is the following statement true or false?\\nStatement: {d['question']}\",\n",
        "        \"output\": d[\"answer\"]\n",
        "    }\n",
        "    for d in test_data if d['type'] == 'true_false'\n",
        "]\n",
        "\n",
        "test_multiple_choice_data = [\n",
        "    {\n",
        "        \"input\": f\"Choose the correct option:\\nQuestion: {d['question']}\\nOptions:\\n\" +\n",
        "                 '\\n'.join([f\"{chr(65+i)}) {opt}\" for i, opt in enumerate(d['options'])]),\n",
        "        \"output\": d[\"correct_answer\"]\n",
        "    }\n",
        "    for d in test_data if d['type'] == 'multiple_choice'\n",
        "]\n"
      ],
      "metadata": {
        "id": "kuuBQY0i0s1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save data to .zip"
      ],
      "metadata": {
        "id": "sa-eaSLc1Zyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = {\n",
        "    \"short_answer\": [],\n",
        "    \"true_false\": [],\n",
        "    \"multiple_choice\": [],\n",
        "    \"multi_hop\": []\n",
        "}\n",
        "\n",
        "test_data = {\n",
        "    \"short_answer\": [],\n",
        "    \"true_false\": [],\n",
        "    \"multiple_choice\": [],\n",
        "    \"multi_hop\": []\n",
        "}\n",
        "\n",
        "# Add your transformed data to the appropriate lists:\n",
        "train_data[\"short_answer\"].extend(train_short_answer_data)\n",
        "train_data[\"true_false\"].extend(train_true_false_data)\n",
        "train_data[\"multiple_choice\"].extend(train_multi_hop_data)\n",
        "train_data[\"multi_hop\"].extend(train_multiple_choice_data)\n",
        "\n",
        "test_data[\"short_answer\"].extend(test_short_answer_data)\n",
        "test_data[\"true_false\"].extend(test_true_false_data)\n",
        "test_data[\"multiple_choice\"].extend(test_multi_hop_data)\n",
        "test_data[\"multi_hop\"].extend(test_multiple_choice_data)"
      ],
      "metadata": {
        "id": "GJrKVTMU1dps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install github3.py"
      ],
      "metadata": {
        "id": "FAyUrYNW7HLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import zipfile\n",
        "import github3\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# 1. Get GitHub token from Secrets\n",
        "github_token = userdata.get('git')\n",
        "\n",
        "# 2. Authenticate with GitHub\n",
        "gh = github3.login(token=github_token)\n",
        "\n",
        "# 3. Repository Information\n",
        "repo_owner = 'Adria100'  # Replace with your username\n",
        "repo_name = 'clin_IQ'  # Replace with your repository name\n",
        "repo = gh.repository(repo_owner, repo_name)\n",
        "\n",
        "# 4. Function to create zip and upload to GitHub\n",
        "def save_data_to_zip_and_upload(data_dict, zip_file_name):\n",
        "    with zipfile.ZipFile(zip_file_name, \"w\") as zipf:\n",
        "        for data_type, data_list in data_dict.items():\n",
        "            file_name = f\"{data_type}_data.json\"\n",
        "            with zipf.open(file_name, \"w\") as f:\n",
        "                f.write(json.dumps(data_list, indent=4).encode())\n",
        "\n",
        "    # Upload the zip file to GitHub\n",
        "    with open(zip_file_name, \"rb\") as f:\n",
        "        content = f.read()\n",
        "        repo.create_file(\n",
        "            path=f\"data/processed/{zip_file_name}\",  # Path in the repository\n",
        "            message=f\"Adding {zip_file_name}\",  # Commit message\n",
        "            content=content,\n",
        "            branch='main'  # Replace with your branch name if needed\n",
        "        )\n",
        "\n",
        "    print(f\"Uploaded {zip_file_name} to GitHub\")\n",
        "    os.remove(zip_file_name)  # Remove local zip file\n",
        "\n",
        "# 5. Assuming you have train_data and test_data dictionaries populated\n",
        "# ... (your code to populate train_data and test_data) ...\n",
        "\n",
        "# 6. Save and upload the zip files\n",
        "save_data_to_zip_and_upload(train_data, \"train_dataset.zip\")\n",
        "save_data_to_zip_and_upload(test_data, \"test_dataset.zip\")"
      ],
      "metadata": {
        "id": "TVwD0KMA7Bxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "\n",
        "model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\"\"\"inputs = tokenizer(\n",
        "    your_data,\n",
        "    padding=\"max_length\",  # Pad to the maximum length\n",
        "    truncation=True,        # Truncate if exceeding the maximum length\n",
        "    max_length=512,        # Adjust the maximum length as needed\n",
        "    return_tensors=\"pt\"     # Return PyTorch tensors\n",
        ")\"\"\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             load_in_8bit=True,\n",
        "                                             device_map='auto')"
      ],
      "metadata": {
        "id": "eJEtDDNH-ECw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiPromptTrainer(Trainer):\n",
        "    def __init__(self, *args, prompt_styles_data, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.prompt_styles_data = prompt_styles_data  # Store data for each style\n",
        "\n",
        "    def training_step(self, model, inputs):\n",
        "        # Iterate over each prompt style\n",
        "        for style, data in self.prompt_styles_data.items():\n",
        "            # Create a dataloader for the current style\n",
        "            train_dataloader = self.get_train_dataloader(data)\n",
        "\n",
        "            # Perform a training step for the current style\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "              batch = batch.to(self.args.device)\n",
        "              outputs = model(**batch)\n",
        "              loss = outputs.loss\n",
        "              loss.backward()\n",
        "              self.optimizer.step()\n",
        "              self.optimizer.zero_grad()\n",
        "\n",
        "        return {'loss': loss.item()}  # Return the loss"
      ],
      "metadata": {
        "id": "cBeb4K8R-N1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",          # Output directory\n",
        "    per_device_train_batch_size=4,  # Batch size per device\n",
        "    gradient_accumulation_steps=4,  # Gradient accumulation steps\n",
        "    num_train_epochs=3,              # Number of training epochs\n",
        "    fp16=True,                       # Enable mixed precision training\n",
        "    logging_dir='./logs',            # Directory for storing logs\n",
        "    learning_rate=2e-5,             # Learning rate\n",
        "    weight_decay=0.01,              # Weight decay\n",
        "    optim=\"adamw_torch\",\n",
        "    save_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "trainer = MultiPromptTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=None,   # Not used in this example\n",
        "    prompt_styles_data={\n",
        "        \"short_answer\": train_short_answer_data,\n",
        "        \"multi_hop\": train_multi_hop_data,\n",
        "        \"true_false\": train_true_false_data,\n",
        "        \"multiple_choice\": train_multiple_choice_data\n",
        "    }\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "z3VOm5-E-PcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"./fine_tuned_llama\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_llama\")"
      ],
      "metadata": {
        "id": "kK8OKhTd-Sjb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}