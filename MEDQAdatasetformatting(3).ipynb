{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adria100/clin_IQ/blob/main/MEDQAdatasetformatting(3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOME IMPORTS"
      ],
      "metadata": {
        "id": "uJntZ1SVyOF-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RYOj4bW6RoRA"
      },
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "!pip install datasets\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset, concatenate_datasets\n",
        "import json\n",
        "from requests.exceptions import RequestException\n",
        "import time\n",
        "import re\n",
        "import pandas as pd\n",
        "import json\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", load_in_4bit=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "llama_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "_mOrUK6J1s0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#check dataset structure"
      ],
      "metadata": {
        "id": "ChHKpN9QDFsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_dataset_structure(x):\n",
        "    try:\n",
        "        dataset = load_dataset(x)\n",
        "\n",
        "        # Print the names of the splits\n",
        "        print(\"Dataset splits:\", dataset.keys())\n",
        "\n",
        "        # Print number of samples in each split\n",
        "        for split in dataset.keys():\n",
        "            print(f\"{split} size: {len(dataset[split])}\")\n",
        "\n",
        "        # Print column names (structure)\n",
        "        print(\"Columns:\", dataset[\"train\"].column_names)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ElNQ7bmuNc-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA from GITHUB and mostly HUGGINGFACE"
      ],
      "metadata": {
        "id": "47ek1TepdbTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_MC1_dataset():\n",
        "    try:\n",
        "        # Load dataset from Hugging Face\n",
        "        dataset = load_dataset(\"bigbio/med_qa\")\n",
        "        transformed_data_MC1 = []\n",
        "        for item in concatenate_datasets([dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]]):\n",
        "            # Ensure only English questions are kept\n",
        "            #if item[\"language\"] == \"english\":\n",
        "                transformed_item = {\n",
        "                    \"correct_answer\": item[\"answer_idx\"],  # Convert index to A/B/C/D format\n",
        "                    \"options\": {  # Extract only the values from option dictionary\n",
        "                        \"A\": item[\"options\"][0][\"value\"],\n",
        "                        \"B\": item[\"options\"][1][\"value\"],\n",
        "                        \"C\": item[\"options\"][2][\"value\"],\n",
        "                        \"D\": item[\"options\"][3][\"value\"]\n",
        "                    },\n",
        "                    \"question\": item[\"question\"],\n",
        "                    \"source\": {\n",
        "                        \"isbn\": \"000-0000000000\",\n",
        "                        \"page\": 0,\n",
        "                        \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "                    },\n",
        "                    \"type\": \"multiple_choice\"\n",
        "                }\n",
        "                transformed_data_MC1.append(transformed_item)\n",
        "                return transformed_data_MC1\n",
        "        #print(len(json.dumps(transformed_data_MC1)))\n",
        "        #print(json.dumps(transformed_data_MC1[:3], indent=4))  # Print a preview\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "transformed_MC1_data = transform_MC1_dataset()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GntAi1MxNs7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_short_answer1_dataset():\n",
        "    dataset = load_dataset(\"HPAI-BSC/OpenMedQA\")\n",
        "    transformed_data_short_answer1 = []\n",
        "    for item in dataset['train']:  # Assuming 'train' split contains the data\n",
        "        transformed_item = {\n",
        "            \"answer\": item[\"answer\"],\n",
        "            \"question\": item[\"question\"],\n",
        "            \"source\": {\n",
        "                \"isbn\": \"000-0000000000\",  # Placeholder value\n",
        "                \"page\": 0,  # Placeholder value\n",
        "                \"paragraph_id\": \"000-0000000000-p00-para26\"  # Placeholder value\n",
        "            },\n",
        "            \"type\": \"short_answer\"\n",
        "        }\n",
        "        transformed_data_short_answer1.append(transformed_item)\n",
        "    return transformed_data_short_answer1\n",
        "    #print(json.dumps(transformed_data_short_answer1, indent=4))\n",
        "transformed_short_answer1_data = transform_short_answer1_dataset()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "g_M7w5B2bDDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"qiaojin/PubMedQA\", \"pqa_artificial\")\n",
        "transformed_data_TF2 = [] # Initialize an empty list for True/False questions\n",
        "for entry in dataset[\"train\"]:\n",
        "    question = entry['question'].strip()\n",
        "    answer = entry['final_decision'].strip()\n",
        "    # Convert final_decision to True/False\n",
        "    transformed_answer = \"True\" if answer.lower() == \"yes\" else \"False\"\n",
        "    # Create the formatted True/False entry\n",
        "    formatted_entry = {\n",
        "        \"answer\": transformed_answer,\n",
        "        \"question\": question,\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para31\"\n",
        "        },\n",
        "        \"type\": \"true_false\"\n",
        "    }\n",
        "\n",
        "    transformed_data_TF2.append(formatted_entry)\n",
        "transformed_TF2_data = transformed_data_TF2\n",
        "# Print the first 3 formatted entries\n",
        "print(json.dumps(transformed_data_TF2[:3], indent=4))"
      ],
      "metadata": {
        "id": "YsJ3APzfhprG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# spaCy-Modell laden (falls nicht installiert: `!python -m spacy download en_core_web_sm`)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Anzahl der Wiederholungsversuche für das Laden des Datensatzes\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"lavita/MedQuAD\", split=\"train\")\n",
        "        break  # Erfolgreich geladen, aus Schleife ausbrechen\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Warte 5 Sekunden vor erneutem Versuch\n",
        "        else:\n",
        "            raise  # Letzter Versuch fehlgeschlagen → Exception auslösen\n",
        "\n",
        "transformed_data_R1 = []\n",
        "\n",
        "for item in dataset:\n",
        "    question = item[\"question\"]\n",
        "    answer = item[\"answer\"]\n",
        "\n",
        "    # Check if answer is None and skip if it is\n",
        "    if answer is None:\n",
        "      #  print(\"Skipping item due to missing answer.\")  # Optional: Print a message for debugging\n",
        "        continue\n",
        "\n",
        "    # Segmentiere die Antwort in Sätze\n",
        "    doc = nlp(answer)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents]\n",
        "\n",
        "    # Falls mehrere Sätze vorhanden sind, strukturiere sie in Schritte\n",
        "    if len(sentences) > 1:\n",
        "        reasoning = [f\"Step {i+1}: {sent}\" for i, sent in enumerate(sentences[:-1])]\n",
        "        final_answer = sentences[-1]  # Letzter Satz ist die finale Antwort\n",
        "    else:\n",
        "        reasoning = [\"No explicit reasoning found.\"]\n",
        "        final_answer = answer\n",
        "\n",
        "    # Formatiertes Item erstellen\n",
        "    formatted_item = {\n",
        "        \"answer\": final_answer,\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "\n",
        "    transformed_data_R1.append(formatted_item)\n",
        "\n",
        "# Beispielausgabe (erste 3 Einträge)\n",
        "#print(transformed_data_R1[:3])\n",
        "transformed_R1_data = transformed_data_R1"
      ],
      "metadata": {
        "id": "YRc2-jTbRsef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama"
      ],
      "metadata": {
        "id": "OSGk_HvEVq_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import Memory  # For caching\n",
        "from tqdm.auto import tqdm  # For progress monitoring\n",
        "\n",
        "# Set pad_token_id to eos_token_id for open-end generation\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"lavita/MedQuAD\", split=\"train\")\n",
        "\n",
        "# Initialize caching\n",
        "memory = Memory(location=\".cache\", verbose=0)\n",
        "\n",
        "# Function to generate prompt (extracted for modularity)\n",
        "def generate_prompt(example):\n",
        "    return f\"\"\"\n",
        "    Question: {example['question']}\n",
        "    Answer: {example['answer']}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "\n",
        "# Function to extract reasoning (extracted for modularity)\n",
        "def extract_reasoning(response):\n",
        "    steps = response[0][\"generated_text\"].split(\"\\n\")\n",
        "    reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(steps) if step]\n",
        "    return reasoning\n",
        "\n",
        "# Function to generate reasoning steps (with error handling and caching)\n",
        "@memory.cache  # Apply caching to this function\n",
        "def generate_reasoning_steps(examples):\n",
        "    prompts = [generate_prompt(example) for example in examples]\n",
        "\n",
        "    reasoning_steps = []\n",
        "    for prompt in prompts:\n",
        "        try:\n",
        "            # Batch generation using the pipeline (reduced batch size to 4)\n",
        "            pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "            response = pipe(prompt, max_new_tokens=256, do_sample=True, batch_size=4)\n",
        "            reasoning_steps.append(extract_reasoning(response))\n",
        "        except Exception as e:\n",
        "            print(f\"Error during llama_pipeline call: {e}\")\n",
        "            reasoning_steps.append([\"Error: Could not generate reasoning.\"])  # Handle error\n",
        "\n",
        "    return {\"reasoning\": reasoning_steps}\n",
        "\n",
        "# Apply the function to the dataset with progress monitoring\n",
        "dataset = dataset.map(generate_reasoning_steps, batched=True, batch_size=4)\n",
        "\n",
        "# Transform the dataset to the desired format\n",
        "transformed_data_R1 = []\n",
        "for item in tqdm(dataset, desc=\"Transforming data\"):  # Add progress bar\n",
        "    formatted_item = {\n",
        "        \"answer\": item[\"answer\"],\n",
        "        \"question\": item[\"question\"],\n",
        "        \"reasoning\": item[\"reasoning\"],\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "    transformed_data_R1.append(formatted_item)\n",
        "\n",
        "# Print the first 3 formatted entries\n",
        "print(json.dumps(transformed_data_R1[:3], indent=4))"
      ],
      "metadata": {
        "id": "JLkjqNJmGsX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_MC2_dataset():\n",
        "    try:\n",
        "        # Load both test and validation splits\n",
        "        dataset_test = load_dataset(\"stellalisy/mediQ\",split=\"test\")\n",
        "        dataset_validation = load_dataset(\"stellalisy/mediQ\", split=\"validation\")\n",
        "\n",
        "        transformed_data_MC2 = []\n",
        "\n",
        "        # Process both splits\n",
        "        for dataset in [dataset_test, dataset_validation]:\n",
        "            for item in dataset:\n",
        "                context = item.get(\"context\", \"\")\n",
        "                context = re.sub(r\"[\\[\\]\\{\\}\\(\\)\\'\\\"]\", \"\", str(context)) # Remove other brackets and quotes\n",
        "                transformed_item = {\n",
        "                    \"correct_answer\": item[\"answer_idx\"],\n",
        "                    \"options\": item[\"options\"],\n",
        "                    \"question\": item[\"question\"] + \" \" + context,\n",
        "                    \"source\": {\n",
        "                        \"isbn\": \"000-0000000000\",\n",
        "                        \"page\": 0,\n",
        "                        \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "                    },\n",
        "                    \"type\": \"multiple_choice\"\n",
        "                }\n",
        "                transformed_data_MC2.append(transformed_item)\n",
        "\n",
        "        # Return the combined transformed data\n",
        "        print(json.dumps(transformed_data_MC2[:3], indent=4))\n",
        "        return transformed_data_MC2\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "\n",
        "# Call the function and get the length\n",
        "transformed_MC2_data = transform_MC2_dataset()"
      ],
      "metadata": {
        "id": "opWFWV8jpbhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_MC3_dataset():\n",
        "    try:\n",
        "        # Load dataset from Hugging Face\n",
        "        dataset = load_dataset(\"openlifescienceai/medmcqa\")  # Loads the train split directly\n",
        "\n",
        "        transformed_data_MC3 = []\n",
        "\n",
        "        for item in concatenate_datasets([dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]]):  # Iterate directly over dataset\n",
        "            transformed_item = {\n",
        "                \"correct_answer\": item[\"cop\"],\n",
        "                \"options\": {\n",
        "                    \"A\": item[\"opa\"],\n",
        "                    \"B\": item[\"opb\"],\n",
        "                    \"C\": item[\"opc\"],\n",
        "                    \"D\": item[\"opd\"]\n",
        "                },\n",
        "                \"question\": item[\"question\"],\n",
        "                \"source\": {\n",
        "                    \"isbn\": \"000-0000000000\",\n",
        "                    \"page\": 0,\n",
        "                    \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "                },\n",
        "                \"type\": \"multiple_choice\"\n",
        "            }\n",
        "            transformed_data_MC3.append(transformed_item)\n",
        "        return transformed_data_MC3\n",
        "\n",
        "        # Print first 3 entries for debugging\n",
        "        print(transformed_data_MC3[:3])\n",
        "\n",
        "    except RequestException as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "\n",
        "transformed_MC3_data = transform_MC3_dataset()"
      ],
      "metadata": {
        "id": "vLPJKc8J3RLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of retry attempts\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Attempt to load the dataset with retry logic\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"UCSC-VLAA/MedReason\")  # Replace with your dataset name\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "transformed_data_MC4 = []\n",
        "\n",
        "# Process each entry in the dataset\n",
        "for entry in dataset['train']:\n",
        "    question = entry['question'].strip()  # Assume the question is stored in the 'question' column\n",
        "    answer = entry['answer'].strip()  # Assume the answer is in the 'answer' column\n",
        "    options_raw = entry['options'].strip()  # Assume the options are in the 'options' column\n",
        "\n",
        "    # Define the answer choices (we assume options are provided in the format \"A. Option, B. Option, C. Option, D. Option\")\n",
        "    options = {}\n",
        "    options_list = options_raw.split(\"\\n\")  # Assuming options are split by newlines\n",
        "\n",
        "    for option in options_list:\n",
        "        if option:\n",
        "            choice, text = option.split(\". \", 1)  # Split each line into option and its text\n",
        "            options[choice] = text.strip()\n",
        "\n",
        "    # Extract the correct answer\n",
        "    correct_answer = answer.split(\".\")[0].strip()  # First letter is the correct answer (e.g., \"B\" from \"B. Biopsy urease test\")\n",
        "\n",
        "    # Define source information (using placeholders)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": \"000-0000000000-p00-para06\"\n",
        "    }\n",
        "\n",
        "    # Construct the formatted entry\n",
        "    formatted_entry = {\n",
        "        \"correct_answer\": correct_answer,\n",
        "        \"options\": options,\n",
        "        \"question\": question,\n",
        "        \"source\": source,\n",
        "        \"type\": \"multiple_choice\"\n",
        "    }\n",
        "\n",
        "    transformed_data_MC4.append(formatted_entry)\n",
        "\n",
        "# Print first 3 formatted entries\n",
        "#print(json.dumps(transformed_data_MC4[:3], indent=4))\n",
        "transformed_MC4_data =transformed_data_MC4"
      ],
      "metadata": {
        "id": "J67rb9D-FIaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of retry attempts\n",
        "\"\"\"MAX_RETRIES = 3\n",
        "\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"UCSC-VLAA/MedReason\")  # Ersetze mit deinem Dataset-Namen\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "transformed_data_R2 = []\n",
        "\n",
        "# Process each entry in the dataset\n",
        "for entry in dataset['train']:\n",
        "    question = entry['question'].strip()\n",
        "    answer = entry['answer'].strip()\n",
        "    reasoning_raw = entry['reasoning'].strip()\n",
        "\n",
        "    # Split reasoning into structured steps\n",
        "    reasoning_steps = reasoning_raw.split(\"\\n\")  # Annahme: Schritte sind durch Zeilenumbrüche getrennt\n",
        "    reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(reasoning_steps) if step]\n",
        "\n",
        "    # Define source information (using placeholders)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": \"000-0000-p00-para01\"\n",
        "    }\n",
        "\n",
        "    # Define the type based on reasoning complexity\n",
        "    response_type = \"multi_hop\"\n",
        "\n",
        "    # Construct the formatted entry\n",
        "    formatted_entry = {\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"answer\": answer,\n",
        "        \"source\": source,\n",
        "        \"type\": response_type\n",
        "    }\n",
        "\n",
        "    transformed_data_R2.append(formatted_entry)\n",
        "\n",
        "# Print first 3 formatted entries\n",
        "#print(json.dumps(transformed_data_R2[:3], indent=4))\n",
        "transformed_R2_data = transformed_data_R2"
      ],
      "metadata": {
        "id": "RhWVgKRBFGqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama"
      ],
      "metadata": {
        "id": "S675ghjRWI5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Anzahl der Wiederholungsversuche für das Laden des Datensatzes\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Dataset laden mit Wiederholungslogik\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"UCSC-VLAA/MedReason\", split=\"train\")\n",
        "        break\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "# Funktion zur Generierung von Argumentationsschritten\n",
        "def generate_reasoning_steps(question, answer):\n",
        "    prompt = f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "    response = llama_pipeline(prompt, max_length=512, do_sample=True)\n",
        "    steps = response[0][\"generated_text\"].split(\"\\n\")\n",
        "    reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(steps) if step]\n",
        "    return reasoning\n",
        "\n",
        "transformed_data_R2 = []\n",
        "\n",
        "# Process each entry in the dataset\n",
        "for entry in dataset:\n",
        "    question = entry['question'].strip()\n",
        "    answer = entry['answer'].strip()\n",
        "\n",
        "    # Use CodeLlama to generate reasoning\n",
        "    reasoning = generate_reasoning_steps(question, answer)\n",
        "\n",
        "    # Define source information (using placeholders)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": \"000-0000-p00-para01\"\n",
        "    }\n",
        "\n",
        "    # Define the type based on reasoning complexity\n",
        "    response_type = \"multi_hop\"\n",
        "\n",
        "    # Construct the formatted entry\n",
        "    formatted_entry = {\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"answer\": answer,\n",
        "        \"source\": source,\n",
        "        \"type\": response_type\n",
        "    }\n",
        "\n",
        "    transformed_data_R2.append(formatted_entry)\n",
        "\n",
        "# Print first 3 formatted entries\n",
        "#print(json.dumps(transformed_data_R2[:3], indent=4))"
      ],
      "metadata": {
        "id": "zdAWp0_lWKNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy model for sentence segmentation\n",
        "\"\"\"nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Number of retry attempts\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Attempt to load the dataset with retry logic\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"YvanAlvin/medicalQALlama2\", split=\"train\")\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "transformed_data_R3 = []\n",
        "\n",
        "# Process each item in the dataset\n",
        "for item in dataset:\n",
        "    text = item[\"text\"]\n",
        "\n",
        "    # Extract the question from inside [INST]...[/INST]\n",
        "    match = re.search(r'\\[INST\\](.*?)\\[/INST\\]', text, re.DOTALL)\n",
        "    question = match.group(1).strip() if match else \"Unknown Question\"\n",
        "\n",
        "    # Extract the answer as everything after [/INST]\n",
        "    answer = text.split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "    # Use spaCy to segment the answer into sentences\n",
        "    doc = nlp(answer)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents]\n",
        "\n",
        "    # If multiple sentences exist, format them as reasoning steps\n",
        "    if len(sentences) > 1:\n",
        "        reasoning = [f\"Step {i+1}: {sent}\" for i, sent in enumerate(sentences[:-1])]\n",
        "        final_answer = sentences[-1]  # Last sentence is the final answer\n",
        "    else:\n",
        "        reasoning = [\"No explicit reasoning found.\"]\n",
        "        final_answer = answer\n",
        "\n",
        "    formatted_item = {\n",
        "        \"answer\": final_answer,\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "\n",
        "    transformed_data_R3.append(formatted_item)\n",
        "\n",
        "# Print the first 3 formatted entries\n",
        "#print(transformed_data_R3[:3])\n",
        "transformed_R3_data = transformed_data_R3"
      ],
      "metadata": {
        "id": "xkLepzfE3RI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy model for sentence segmentation\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Number of retry attempts\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Attempt to load the dataset with retry logic\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"YvanAlvin/medicalchat200llama2\", split=\"train\")\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "transformed_data_R4 = []\n",
        "\n",
        "# Process each item in the dataset\n",
        "for item in dataset:\n",
        "    text = item[\"text\"]\n",
        "\n",
        "    # Extract the question from inside [INST]...[/INST]\n",
        "    match = re.search(r'\\[INST\\](.*?)\\[/INST\\]', text, re.DOTALL)\n",
        "    question = match.group(1).strip() if match else \"Unknown Question\"\n",
        "\n",
        "    # Extract the answer as everything after [/INST]\n",
        "    answer = text.split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "    # Use spaCy to segment the answer into sentences\n",
        "    doc = nlp(answer)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents]\n",
        "\n",
        "    # If multiple sentences exist, format them as reasoning steps\n",
        "    if len(sentences) > 1:\n",
        "        reasoning = [f\"Step {i+1}: {sent}\" for i, sent in enumerate(sentences[:-1])]\n",
        "        final_answer = sentences[-1]  # Last sentence is the final answer\n",
        "    else:\n",
        "        reasoning = [\"No explicit reasoning found.\"]\n",
        "        final_answer = answer\n",
        "\n",
        "    formatted_item = {\n",
        "        \"answer\": final_answer,\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "\n",
        "    transformed_data_R4.append(formatted_item)\n",
        "\n",
        "# Print the first 3 formatted entries\n",
        "#print(transformed_data_R4[:3])\n",
        "\n",
        "transformed_R4_data = transformed_data_R4"
      ],
      "metadata": {
        "id": "KnhT3qXA3RF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy model for sentence segmentation\n",
        "\"\"\"nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Number of retry attempts\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Attempt to load the dataset with retry logic\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"eashuu/medical_qa\", split=\"train\")\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "transformed_data_R5 = []\n",
        "\n",
        "# Process each item in the dataset\n",
        "for item in dataset:\n",
        "    question = item[\"instruction\"].replace(\"Q. \", \"\", 1).strip()\n",
        "    answer = item[\"output\"].strip()\n",
        "\n",
        "    # Use spaCy to segment answer into sentences\n",
        "    doc = nlp(answer)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents]\n",
        "\n",
        "    # If multiple sentences exist, format them as reasoning steps\n",
        "    if len(sentences) > 1:\n",
        "        reasoning = [f\"Step {i+1}: {sent}\" for i, sent in enumerate(sentences[:-1])]\n",
        "        final_answer = sentences[-1]  # Last sentence is the final answer\n",
        "    else:\n",
        "        reasoning = [\"No explicit reasoning found.\"]\n",
        "        final_answer = answer\n",
        "\n",
        "    formatted_item = {\n",
        "        \"answer\": final_answer,\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "\n",
        "    transformed_data_R5.append(formatted_item)\n",
        "\n",
        "# Print the first 3 formatted entries\n",
        "#print(transformed_data_R5[:3])\n",
        "\n",
        "transformed_R5_data = transformed_data_R5"
      ],
      "metadata": {
        "id": "TL67HAmP3RCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama"
      ],
      "metadata": {
        "id": "-UDM0ZIyVZMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Anzahl der Wiederholungsversuche für das Laden des Datensatzes\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Dataset laden mit Wiederholungslogik\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"eashuu/medical_qa\", split=\"train\")\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "# Funktion zur Generierung von Argumentationsschritten\n",
        "def generate_reasoning_steps(question, answer):\n",
        "    prompt = f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "    response = llama_pipeline(prompt, max_length=512, do_sample=True)\n",
        "    steps = response[0][\"generated_text\"].split(\"\\n\")  # Schritte extrahieren\n",
        "    reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(steps) if step]  # Formatieren\n",
        "    return reasoning\n",
        "\n",
        "# Modify processing logic\n",
        "transformed_data_R5 = []\n",
        "\n",
        "for item in dataset:\n",
        "    question = item[\"instruction\"].replace(\"Q. \", \"\", 1).strip()\n",
        "    answer = item[\"output\"].strip()\n",
        "\n",
        "    # CodeLlama für Argumentation verwenden\n",
        "    reasoning = generate_reasoning_steps(question, answer)\n",
        "\n",
        "    formatted_item = {\n",
        "        \"answer\": answer,\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "\n",
        "    transformed_data_R5.append(formatted_item)"
      ],
      "metadata": {
        "id": "mGRo0VieXQDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of retry attempts\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\")\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "transformed_data_R6 = []\n",
        "\n",
        "# Process each item in the dataset\n",
        "for item in dataset[\"train\"]:\n",
        "    question = item[\"Question\"].strip()\n",
        "    reasoning_text = item[\"Complex_CoT\"].strip()\n",
        "    answer = item[\"Response\"].strip()\n",
        "\n",
        "    # If no reasoning is provided, set a default message\n",
        "    if not reasoning_text:\n",
        "        reasoning = [\"Step 1: No explicit reasoning found.\"]\n",
        "    else:\n",
        "        # Split reasoning into steps (heuristic: split by \". \" or newlines)\n",
        "        raw_steps = [step.strip() for step in reasoning_text.split(\". \") if step]\n",
        "        reasoning = [f\"Step {i+1}: {step}\" for i, step in enumerate(raw_steps)]\n",
        "\n",
        "    formatted_item = {\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"answer\": answer,\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "\n",
        "    transformed_data_R6.append(formatted_item)\n",
        "\n",
        "# Print the first 3 formatted entries\n",
        "#print(transformed_data_R6[:3])\n",
        "\n",
        "transformed_R6_data = transformed_data_R6"
      ],
      "metadata": {
        "id": "RL-Gv1Yb3Q7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy model for sentence segmentation\n",
        "\"\"\"nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Number of retry attempts\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Attempt to load the dataset with retry logic\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"wentechno/medicalQA-50thPlus\", split=\"train\")\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "transformed_data_R7 = []\n",
        "\n",
        "# Process each item in the dataset\n",
        "for item in dataset:\n",
        "    question = item[\"instruction\"].replace(\"Q. \", \"\", 1).strip()\n",
        "    answer = item[\"output\"].strip()\n",
        "\n",
        "    # Use spaCy to segment answer into sentences\n",
        "    doc = nlp(answer)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents]\n",
        "\n",
        "    # If multiple sentences exist, format them as reasoning steps\n",
        "    if len(sentences) > 1:\n",
        "        reasoning = [f\"Step {i+1}: {sent}\" for i, sent in enumerate(sentences[:-1])]\n",
        "        final_answer = sentences[-1]  # Last sentence is the final answer\n",
        "    else:\n",
        "        reasoning = [\"No explicit reasoning found.\"]\n",
        "        final_answer = answer\n",
        "\n",
        "    formatted_item = {\n",
        "        \"answer\": final_answer,\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "\n",
        "    transformed_data_R7.append(formatted_item)\n",
        "\n",
        "# Print the first 3 formatted entries\n",
        "#print(transformed_data_R7[:3])\n",
        "\n",
        "transformed_R7_data = transformed_data_R7"
      ],
      "metadata": {
        "id": "-0G3GJgR3QqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama"
      ],
      "metadata": {
        "id": "PYnz8wurX3tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Anzahl der Wiederholungsversuche für das Laden des Datensatzes\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Dataset laden mit Wiederholungslogik\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"wentechno/medicalQA-50thPlus\", split=\"train\")\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "# Funktion zur Generierung von Argumentationsschritten\n",
        "def generate_reasoning_steps(question, answer):\n",
        "    prompt = f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "    response = llama_pipeline(prompt, max_length=512, do_sample=True)\n",
        "    steps = response[0][\"generated_text\"].split(\"\\n\")  # Schritte extrahieren\n",
        "    reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(steps) if step]  # Formatieren\n",
        "    return reasoning\n",
        "\n",
        "# Modify processing logic\n",
        "transformed_data_R7 = []\n",
        "\n",
        "for item in dataset:\n",
        "    question = item[\"instruction\"].replace(\"Q. \", \"\", 1).strip()\n",
        "    answer = item[\"output\"].strip()\n",
        "\n",
        "    # CodeLlama für Argumentation verwenden\n",
        "    reasoning = generate_reasoning_steps(question, answer)\n",
        "\n",
        "    formatted_item = {\n",
        "        \"answer\": answer,\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "\n",
        "    transformed_data_R7.append(formatted_item)"
      ],
      "metadata": {
        "id": "TGNLQX7wX3Lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset(\"Ajayaadhi/Medical-QA\")\n",
        "\n",
        "# Initialize a list to hold the reformatted entries\n",
        "transformed_data_short_answer2 = []\n",
        "\n",
        "for entry in dataset[\"train\"]:  # Process all entries\n",
        "    text = entry[\"train\"]  # Adjust this if the key is different\n",
        "\n",
        "    # Extract question\n",
        "    question_match = re.search(r\"### Input:\\n(.+?)\\n\\[INST\\]\", text, re.DOTALL)\n",
        "    question = question_match.group(1).strip() if question_match else \"\"\n",
        "\n",
        "    # Extract answer\n",
        "    answer_match = re.search(r\"### Response:\\n(.+?)</s>\", text, re.DOTALL)\n",
        "    answer = answer_match.group(1).strip() if answer_match else \"\"\n",
        "\n",
        "    # Define source information (using placeholders)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": \"000-0000000-p00-para01\"\n",
        "    }\n",
        "\n",
        "    # Determine response type\n",
        "    response_type = \"short_answer\"\n",
        "\n",
        "    # Construct the reformatted entry\n",
        "    reformatted_entry = {\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"source\": source,\n",
        "        \"type\": response_type\n",
        "    }\n",
        "\n",
        "    # Append to the list\n",
        "    transformed_data_short_answer2.append(reformatted_entry)\n",
        "\n",
        "# Print the first 3 formatted entries\n",
        "#print(json.dumps(transformed_data_short_answer2[:3], indent=4))\n",
        "\n",
        "transformed_short_answer2_data = transformed_data_short_answer2"
      ],
      "metadata": {
        "id": "TWOpCxIu3QYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Number of retry attempts\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"KaungHtetCho/MedicalQA\")\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "transformed_data_R8 = []\n",
        "\n",
        "# Process each entry in the dataset\n",
        "for entry in dataset['train']:\n",
        "    description = entry['Description'].strip()\n",
        "    patient = entry['Patient'].strip()\n",
        "    doctor = entry['Doctor'].strip()\n",
        "\n",
        "    # Form the question from description + patient details\n",
        "    question = f\"{description}\\n\\n{patient}\"\n",
        "\n",
        "    # Split the doctor's response into reasoning steps\n",
        "    if doctor:\n",
        "        raw_steps = [step.strip() for step in doctor.split(\". \") if step]\n",
        "        reasoning = [f\"Step {i+1}: {step}\" for i, step in enumerate(raw_steps)]\n",
        "    else:\n",
        "        reasoning = [\"Step 1: No explicit reasoning found.\"]\n",
        "\n",
        "    # Define source information (using placeholders)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": \"000-0000-p00-para01\"\n",
        "    }\n",
        "\n",
        "    # Define the type based on reasoning complexity\n",
        "    response_type = \"multi_hop\"\n",
        "\n",
        "    # Construct the formatted entry\n",
        "    formatted_entry = {\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"answer\": reasoning[-1] if reasoning else \"No answer provided.\",\n",
        "        \"source\": source,\n",
        "        \"type\": response_type\n",
        "    }\n",
        "\n",
        "    transformed_data_R8.append(formatted_entry)\n",
        "# Print first 3 formatted entries\n",
        "#print(json.dumps(transformed_data_R8[:3], indent=4))\n",
        "transformed_R8_data = transformed_data_R8"
      ],
      "metadata": {
        "id": "WcTqxqw7SwaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama"
      ],
      "metadata": {
        "id": "pKWclgB7YuTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Anzahl der Wiederholungsversuche für das Laden des Datensatzes\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Dataset laden mit Wiederholungslogik\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"KaungHtetCho/MedicalQA\", split=\"train\")\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "\n",
        "# Funktion zur Generierung von Argumentationsschritten\n",
        "def generate_reasoning_steps(question, answer):\n",
        "    prompt = f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "    response = llama_pipeline(prompt, max_length=512, do_sample=True)\n",
        "    steps = response[0][\"generated_text\"].split(\"\\n\")\n",
        "    reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(steps) if step]\n",
        "    return reasoning\n",
        "\n",
        "transformed_data_R8 = []\n",
        "\n",
        "# Process each entry in the dataset\n",
        "for entry in dataset:\n",
        "    description = entry['Description'].strip()\n",
        "    patient = entry['Patient'].strip()\n",
        "    #doctor = entry['Doctor'].strip()  # Not used for answer anymore\n",
        "\n",
        "    # Form the question from description + patient details\n",
        "    question = f\"{description}\\n\\n{patient}\"\n",
        "\n",
        "    # Generate answer using CodeLlama\n",
        "    answer = llama_pipeline(question, max_length=256, do_sample=True)[0][\"generated_text\"].strip()\n",
        "\n",
        "    # Generate reasoning using CodeLlama\n",
        "    reasoning = generate_reasoning_steps(question, answer)\n",
        "\n",
        "\n",
        "    # Define source information (using placeholders)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": \"000-0000-p00-para01\"\n",
        "    }\n",
        "\n",
        "    # Define the type based on reasoning complexity\n",
        "    response_type = \"multi_hop\"\n",
        "\n",
        "    # Construct the formatted entry\n",
        "    formatted_entry = {\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"answer\": answer,  # Using generated answer\n",
        "        \"source\": source,\n",
        "        \"type\": response_type\n",
        "    }\n",
        "\n",
        "    transformed_data_R8.append(formatted_entry)\n",
        "\n",
        "# Print first 3 formatted entries\n",
        "#print(json.dumps(transformed_data_R8[:3], indent=4))"
      ],
      "metadata": {
        "id": "0mlk9Fz6Yfae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Anzahl der Wiederholungsversuche für das Laden des Datensatzes\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Dataset laden mit Wiederholungslogik\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"qiaojin/PubMedQA\", \"pqa_unlabeled\")\n",
        "        break\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "# Funktion zur Generierung von Argumentationsschritten\n",
        "def generate_reasoning_steps(question, answer):\n",
        "    prompt = f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "    response = llama_pipeline(prompt, max_length=512, do_sample=True)\n",
        "    steps = response[0][\"generated_text\"].split(\"\\n\")\n",
        "    reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(steps) if step]\n",
        "    return reasoning\n",
        "\n",
        "transformed_data_R9 = []\n",
        "\n",
        "# Process each entry in the dataset\n",
        "for entry in dataset[\"train\"]:\n",
        "    question = entry['question'].strip()\n",
        "    answer = entry['long_answer'].strip()\n",
        "\n",
        "    # Use CodeLlama to generate reasoning\n",
        "    reasoning = generate_reasoning_steps(question, answer)\n",
        "\n",
        "    # Define source information (using placeholders)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": \"000-0000-p00-para01\"\n",
        "    }\n",
        "\n",
        "    # Define the type based on reasoning complexity\n",
        "    response_type = \"multi_hop\"\n",
        "\n",
        "    # Construct the formatted entry\n",
        "    formatted_entry = {\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"answer\": answer,\n",
        "        \"source\": source,\n",
        "        \"type\": response_type\n",
        "    }\n",
        "\n",
        "    transformed_data_R9.append(formatted_entry)\n",
        "transformed_R9_data = transformed_data_R9\n",
        "# Print first 3 formatted entries\n",
        "print(json.dumps(transformed_data_R9[:3], indent=4))"
      ],
      "metadata": {
        "id": "5itlewIe7gT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA FROM KAGGLE"
      ],
      "metadata": {
        "id": "Wpt4rFU37RaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle pandas\n"
      ],
      "metadata": {
        "id": "Vpzmmvo53jYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d thedevastator/comprehensive-medical-q-a-dataset --unzip\n"
      ],
      "metadata": {
        "id": "gQad59AC3wJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import auth\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "# Get the API key\n",
        "kaggle_key = os.environ.get('KAGGLE_KEY')\n",
        "\n",
        "# Authenticate with Kaggle API\n",
        "os.environ['KAGGLE_USERNAME'] = \"apfresh\" # Replace with your username\n",
        "os.environ['KAGGLE_KEY'] = kaggle_key\n",
        "api = KaggleApi()\n",
        "api.authenticate()"
      ],
      "metadata": {
        "id": "7p2ULHnJ67jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset (assuming it's already available in Colab)\n",
        "file_path = \"Comprehensive_Medical_Q&A\"  # Adjust based on actual filename\n",
        "dataset = pd.read_csv(file_path)\n",
        "\n",
        "# Initialize a list to hold the reformatted entries\n",
        "transformed_data_short_answer3 = []\n",
        "\n",
        "# Iterate over the dataset entries\n",
        "for index, entry in dataset.iterrows():\n",
        "    question = entry.get('Question', '').strip()\n",
        "    answer = entry.get('Answer', '').strip()\n",
        "\n",
        "    # Define source information (using placeholders here)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": f\"000-0000000-p00-para{index+1:02d}\"\n",
        "    }\n",
        "\n",
        "    # Determine the response type based on answer length\n",
        "    response_type = \"short_answer\"\n",
        "\n",
        "    # Construct the reformatted entry\n",
        "    reformatted_entry = {\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"source\": source,\n",
        "        \"type\": response_type\n",
        "    }\n",
        "\n",
        "    # Append to the list\n",
        "    transformed_data_short_answer3.append(reformatted_entry)\n",
        "# Convert the list to JSON format and print the first 3 entries\n",
        "#print(json.dumps(transformed_data_short_answer3[:3], indent=4))\n",
        "transformed_short_answer3_data = transformed_data_short_answer3\n"
      ],
      "metadata": {
        "id": "IJeGfraN5jJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d pythonafroz/medquad-medical-question-answer-for-ai-research --unzip\n"
      ],
      "metadata": {
        "id": "RBfFVCdr7Ajw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "file_path = \"MedQuAD: Medical Question-Answer Dataset\"  # Adjust based on the actual filename\n",
        "dataset = pd.read_csv(file_path)\n",
        "\n",
        "# Initialize a list to hold the reformatted entries\n",
        "transformed_data_short_answer4 = []\n",
        "\n",
        "# Iterate over the dataset entries\n",
        "for index, entry in dataset.iterrows():\n",
        "    question = entry.get('Question', '').strip()\n",
        "    answer = entry.get('Answer', '').strip()\n",
        "\n",
        "    # Define source information (using placeholders here)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": f\"000-0000000-p00-para{index+1:02d}\"\n",
        "    }\n",
        "\n",
        "    # Determine the response type based on answer length\n",
        "    response_type = \"short_answer\"\n",
        "\n",
        "    # Construct the reformatted entry\n",
        "    reformatted_entry = {\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"source\": source,\n",
        "        \"type\": response_type\n",
        "    }\n",
        "\n",
        "    # Append to the list\n",
        "    transformed_data_short_answer4.append(reformatted_entry)\n",
        "# Convert the list to JSON format and print the first 3 entries\n",
        "#print(json.dumps(transformed_data_short_answer4[:3], indent=4))\n",
        "transformed_short_answer4_data = transformed_data_short_answer4"
      ],
      "metadata": {
        "id": "yJIs-8SU58bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LOCAL DATA"
      ],
      "metadata": {
        "id": "yLVNoX6_Ehrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from io import StringIO\n",
        "\n",
        "#not working\n",
        "csv = \"/content/true_false_questions.csv\"\n",
        "\n",
        "# Fetch the CSV data\n",
        "try:\n",
        "    df = pd.read_csv(csv)  # Read CSV directly from Colab environment\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found: {csv}. Please make sure it is uploaded to Colab.\")\n",
        "    exit()\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse the CSV file: {csv}. Please check its format.\")\n",
        "    exit()\n",
        "\n",
        "df = pd.read_csv(csv)\n",
        "# Transform data\n",
        "transformed_data_TF1 = []\n",
        "for _, row in df.iterrows():\n",
        "    formatted_item = {\n",
        "        \"question\": row[\"text\"],  # Extract question\n",
        "        \"answer\": str(row[\"label\"]),  # Extract answer as string\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",  # Placeholder value\n",
        "            \"page\": 0,  # Placeholder value\n",
        "            \"paragraph_id\": \"000-0000000000-p00-paraXX\"  # Placeholder value\n",
        "        },\n",
        "        \"type\": \"true_false\"\n",
        "    }\n",
        "    transformed_data_TF1.append(formatted_item)\n",
        "\n",
        "# Save formatted data to JSON\n",
        "\"\"\"with open(\"formatted_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(transformed_data_TF1, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(json.dumps(transformed_data_TF1[:3], indent=4))\"\"\"\n",
        "transformed_TF1_data = transformed_data_TF1"
      ],
      "metadata": {
        "id": "O64XzUf2ifuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ENTIRE DATASET"
      ],
      "metadata": {
        "id": "w_wLy9bMCy9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "lists = [transformed_TF1_data, transformed_TF2_data, transformed_short_answer1_data, transformed_short_answer2_data, transformed_short_answer3_data, transformed_short_answer4_data,\n",
        "        transformed_MC1_data, transformed_MC2_data, transformed_MC3_data, transformed_MC4_data,\n",
        "        transformed_R1_data, transformed_R2_data,transformed_R3_data,transformed_R4_data,transformed_R5_data,transformed_R6_data,transformed_R7_data,transformed_R8_data, transformed_R9_data ]\n",
        "DATA = list(itertools.chain(*lists))\n"
      ],
      "metadata": {
        "id": "7TdXFkBOra0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removes duplicates, tokenization, stopwords, lemmatization, padding\n",
        "\n",
        "# Handle Class Imbalance:\n",
        "\n",
        "#    SMOTE will help generate synthetic samples for underrepresented classes in the dataset.\n",
        "\n",
        "#    Class Weights can be used in the model to give more importance to underrepresented classes during training.\n",
        "\n",
        "# Paraphrasing / Question Modification:\n",
        "\n",
        "#    We will use a GPT-based model to paraphrase or modify questions to generate additional training samples."
      ],
      "metadata": {
        "id": "OUPfGmrhCdpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SPLIT BY TYPE and SAVE .ZIP TO REPOSITORY"
      ],
      "metadata": {
        "id": "h3W5LepvH9ky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "\n",
        "    Ensure the data is preprocessed appropriately for each prompt style before creating the datasets.\n",
        "    Adjust the batch size, training steps, epochs, and other hyperparameters to find the best performance.\n",
        "    Regularly evaluate the performance on your test set for each prompt style.\n",
        "    This example assumes your prompt styles data is readily available.\n",
        "    Remember to make necessary imports and data modifications for smooth execution.\n"
      ],
      "metadata": {
        "id": "Ui9VFHI3-ZHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "Ad-Pq5_GkFvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Lemmatizes, removes stopwords, and lowercases the input text.\"\"\"\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.lower() not in stop_words and token.isalnum()]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def remove_duplicates_and_empty(data):\n",
        "    \"\"\"Removes duplicates and entries with empty questions or answers.\"\"\"\n",
        "    seen_pairs = set()\n",
        "    filtered_data = []\n",
        "    for entry in data:\n",
        "        q, a = entry[\"question\"], entry[\"answer\"]\n",
        "\n",
        "        if not q or not a:  # Remove if empty\n",
        "            continue\n",
        "\n",
        "        pair = (q.strip().lower(), a.strip().lower())  # Normalize case for comparison\n",
        "        if pair not in seen_pairs:\n",
        "            seen_pairs.add(pair)\n",
        "            filtered_data.append(entry)\n",
        "    return filtered_data"
      ],
      "metadata": {
        "id": "cP519IeiB1rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#It's usually better to apply stopword removal and lemmatization before removing duplicates\n",
        "preprocessed_data = [{k: preprocess_text(v) if isinstance(v, str) else v\n",
        "                       for k, v in d.items()} for d in DATA]\n",
        "preprocessed_data = remove_duplicates_and_empty(preprocessed_data)  # Apply to DATA"
      ],
      "metadata": {
        "id": "0DIk_DxVD8Rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split in Test and Train"
      ],
      "metadata": {
        "id": "rGRfqej7jn_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Group by Question Type\n",
        "grouped_data = {}\n",
        "for item in preprocessed_data:\n",
        "    question_type = item['type']\n",
        "    if question_type not in grouped_data:\n",
        "        grouped_data[question_type] = []\n",
        "    grouped_data[question_type].append(item)\n",
        "\n",
        "# 2. Stratified Split within Each Group\n",
        "train_data = []\n",
        "test_data = []\n",
        "for question_type, data in grouped_data.items():\n",
        "    # Create a temporary DataFrame for easier stratification (optional)\n",
        "    df = pd.DataFrame(data)\n",
        "    # Perform stratified split, using 'type' column for stratification\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['type'], random_state=42)  # Adjust test_size as needed\n",
        "    # Append the split data to the overall train and test sets\n",
        "    train_data.extend(train_df.to_dict('records'))\n",
        "    test_data.extend(test_df.to_dict('records'))\n",
        "\n",
        "# 3. Combine Splits\n",
        "# Now you have train_data and test_data with equal representation of question types\n",
        "print(f\"Train data size: {len(train_data)}\")\n",
        "print(f\"Test data size: {len(test_data)}\")"
      ],
      "metadata": {
        "id": "m1ZpZyEHjmxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting before formatting avoids potential data leakage, where information from the test set might influence the model during training."
      ],
      "metadata": {
        "id": "M8ynijmQnGex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format training data\n",
        "train_short_answer_data = [\n",
        "    {\n",
        "        \"input\": f\"Answer the following question:\\n{d['question']}\",\n",
        "        \"output\": d[\"answer\"]\n",
        "    }\n",
        "    for d in train_data if d['type'] == 'short_answer'\n",
        "]\n",
        "\n",
        "train_multi_hop_data = [\n",
        "    {\n",
        "        \"input\": f\"Answer the following multi-hop question:\\n{d['question']}\",\n",
        "        \"output\": f\"{d['reasoning']}\\nAnswer: {d['answer']}\"\n",
        "    }\n",
        "    for d in train_data if d['type'] == 'multi_hop'\n",
        "]\n",
        "\n",
        "train_true_false_data = [\n",
        "    {\n",
        "        \"input\": f\"Is the following statement true or false?\\nStatement: {d['question']}\",\n",
        "        \"output\": d[\"answer\"]\n",
        "    }\n",
        "    for d in train_data if d['type'] == 'true_false'\n",
        "]\n",
        "\n",
        "train_multiple_choice_data = [\n",
        "    {\n",
        "        \"input\": f\"Choose the correct option:\\nQuestion: {d['question']}\\nOptions:\\n\" +\n",
        "                 '\\n'.join([f\"{chr(65+i)}) {opt}\" for i, opt in enumerate(d['options'])]),\n",
        "        \"output\": d[\"correct_answer\"]\n",
        "    }\n",
        "    for d in train_data if d['type'] == 'multiple_choice'\n",
        "]\n",
        "\n",
        "# Format testing data\n",
        "test_short_answer_data = [\n",
        "    {\n",
        "        \"input\": f\"Answer the following question:\\n{d['question']}\",\n",
        "        \"output\": d[\"answer\"]\n",
        "    }\n",
        "    for d in test_data if d['type'] == 'short_answer'\n",
        "]\n",
        "\n",
        "test_multi_hop_data = [\n",
        "    {\n",
        "        \"input\": f\"Answer the following multi-hop question:\\n{d['question']}\",\n",
        "        \"output\": f\"{d['reasoning']}\\nAnswer: {d['answer']}\"\n",
        "    }\n",
        "    for d in test_data if d['type'] == 'multi_hop'\n",
        "]\n",
        "\n",
        "\n",
        "test_true_false_data = [\n",
        "    {\n",
        "        \"input\": f\"Is the following statement true or false?\\nStatement: {d['question']}\",\n",
        "        \"output\": d[\"answer\"]\n",
        "    }\n",
        "    for d in test_data if d['type'] == 'true_false'\n",
        "]\n",
        "\n",
        "test_multiple_choice_data = [\n",
        "    {\n",
        "        \"input\": f\"Choose the correct option:\\nQuestion: {d['question']}\\nOptions:\\n\" +\n",
        "                 '\\n'.join([f\"{chr(65+i)}) {opt}\" for i, opt in enumerate(d['options'])]),\n",
        "        \"output\": d[\"correct_answer\"]\n",
        "    }\n",
        "    for d in test_data if d['type'] == 'multiple_choice'\n",
        "]\n"
      ],
      "metadata": {
        "id": "kuuBQY0i0s1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save data to .zip"
      ],
      "metadata": {
        "id": "sa-eaSLc1Zyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = {\n",
        "    \"short_answer\": [],\n",
        "    \"true_false\": [],\n",
        "    \"multiple_choice\": [],\n",
        "    \"multi_hop\": []\n",
        "}\n",
        "\n",
        "test_data = {\n",
        "    \"short_answer\": [],\n",
        "    \"true_false\": [],\n",
        "    \"multiple_choice\": [],\n",
        "    \"multi_hop\": []\n",
        "}\n",
        "\n",
        "# Add your transformed data to the appropriate lists:\n",
        "train_data[\"short_answer\"].extend(train_short_answer_data)\n",
        "train_data[\"true_false\"].extend(train_true_false_data)\n",
        "train_data[\"multiple_choice\"].extend(train_multi_hop_data)\n",
        "train_data[\"multi_hop\"].extend(train_multiple_choice_data)\n",
        "\n",
        "test_data[\"short_answer\"].extend(test_short_answer_data)\n",
        "test_data[\"true_false\"].extend(test_true_false_data)\n",
        "test_data[\"multiple_choice\"].extend(test_multi_hop_data)\n",
        "test_data[\"multi_hop\"].extend(test_multiple_choice_data)"
      ],
      "metadata": {
        "id": "GJrKVTMU1dps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install github3.py"
      ],
      "metadata": {
        "id": "FAyUrYNW7HLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import zipfile\n",
        "import github3\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# 1. Get GitHub token from Secrets\n",
        "github_token = userdata.get('git')\n",
        "\n",
        "# 2. Authenticate with GitHub\n",
        "gh = github3.login(token=github_token)\n",
        "\n",
        "# 3. Repository Information\n",
        "repo_owner = 'Adria100'  # Replace with your username\n",
        "repo_name = 'clin_IQ'  # Replace with your repository name\n",
        "repo = gh.repository(repo_owner, repo_name)\n",
        "\n",
        "# 4. Function to create zip and upload to GitHub\n",
        "def save_data_to_zip_and_upload(data_dict, zip_file_name):\n",
        "    with zipfile.ZipFile(zip_file_name, \"w\") as zipf:\n",
        "        for data_type, data_list in data_dict.items():\n",
        "            file_name = f\"{data_type}_data.json\"\n",
        "            with zipf.open(file_name, \"w\") as f:\n",
        "                f.write(json.dumps(data_list, indent=4).encode())\n",
        "\n",
        "    # Upload the zip file to GitHub\n",
        "    with open(zip_file_name, \"rb\") as f:\n",
        "        content = f.read()\n",
        "        repo.create_file(\n",
        "            path=f\"data/processed/{zip_file_name}\",  # Path in the repository\n",
        "            message=f\"Adding {zip_file_name}\",  # Commit message\n",
        "            content=content,\n",
        "            branch='main'  # Replace with your branch name if needed\n",
        "        )\n",
        "\n",
        "    print(f\"Uploaded {zip_file_name} to GitHub\")\n",
        "    os.remove(zip_file_name)  # Remove local zip file\n",
        "\n",
        "# 5. Assuming you have train_data and test_data dictionaries populated\n",
        "# ... (your code to populate train_data and test_data) ...\n",
        "\n",
        "# 6. Save and upload the zip files\n",
        "save_data_to_zip_and_upload(train_data, \"train_dataset.zip\")\n",
        "save_data_to_zip_and_upload(test_data, \"test_dataset.zip\")"
      ],
      "metadata": {
        "id": "TVwD0KMA7Bxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "\n",
        "model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\"\"\"inputs = tokenizer(\n",
        "    your_data,\n",
        "    padding=\"max_length\",  # Pad to the maximum length\n",
        "    truncation=True,        # Truncate if exceeding the maximum length\n",
        "    max_length=512,        # Adjust the maximum length as needed\n",
        "    return_tensors=\"pt\"     # Return PyTorch tensors\n",
        ")\"\"\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             load_in_8bit=True,\n",
        "                                             device_map='auto')"
      ],
      "metadata": {
        "id": "eJEtDDNH-ECw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiPromptTrainer(Trainer):\n",
        "    def __init__(self, *args, prompt_styles_data, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.prompt_styles_data = prompt_styles_data  # Store data for each style\n",
        "\n",
        "    def training_step(self, model, inputs):\n",
        "        # Iterate over each prompt style\n",
        "        for style, data in self.prompt_styles_data.items():\n",
        "            # Create a dataloader for the current style\n",
        "            train_dataloader = self.get_train_dataloader(data)\n",
        "\n",
        "            # Perform a training step for the current style\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "              batch = batch.to(self.args.device)\n",
        "              outputs = model(**batch)\n",
        "              loss = outputs.loss\n",
        "              loss.backward()\n",
        "              self.optimizer.step()\n",
        "              self.optimizer.zero_grad()\n",
        "\n",
        "        return {'loss': loss.item()}  # Return the loss"
      ],
      "metadata": {
        "id": "cBeb4K8R-N1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",          # Output directory\n",
        "    per_device_train_batch_size=4,  # Batch size per device\n",
        "    gradient_accumulation_steps=4,  # Gradient accumulation steps\n",
        "    num_train_epochs=3,              # Number of training epochs\n",
        "    fp16=True,                       # Enable mixed precision training\n",
        "    logging_dir='./logs',            # Directory for storing logs\n",
        "    learning_rate=2e-5,             # Learning rate\n",
        "    weight_decay=0.01,              # Weight decay\n",
        "    optim=\"adamw_torch\",\n",
        "    save_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "trainer = MultiPromptTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=None,   # Not used in this example\n",
        "    prompt_styles_data={\n",
        "        \"short_answer\": train_short_answer_data,\n",
        "        \"multi_hop\": train_multi_hop_data,\n",
        "        \"true_false\": train_true_false_data,\n",
        "        \"multiple_choice\": train_multiple_choice_data\n",
        "    }\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "z3VOm5-E-PcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"./fine_tuned_llama\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_llama\")"
      ],
      "metadata": {
        "id": "kK8OKhTd-Sjb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}