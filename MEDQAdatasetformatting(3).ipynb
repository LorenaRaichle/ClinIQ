{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adria100/clin_IQ/blob/main/MEDQAdatasetformatting(3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOME IMPORTS"
      ],
      "metadata": {
        "id": "uJntZ1SVyOF-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RYOj4bW6RoRA"
      },
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "!pip install datasets\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset, concatenate_datasets\n",
        "import json\n",
        "from requests.exceptions import RequestException\n",
        "import time\n",
        "import re\n",
        "import pandas as pd\n",
        "import json\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", load_in_4bit=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "llama_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "_mOrUK6J1s0j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "6a277a5b-92fb-482d-fe94-85cb4bfa93f8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'model_id = \"codellama/CodeLlama-7b-Instruct-hf\"\\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", load_in_4bit=True)\\ntokenizer = AutoTokenizer.from_pretrained(model_id)\\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\\nllama_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#check dataset structure"
      ],
      "metadata": {
        "id": "ChHKpN9QDFsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_dataset_structure(x):\n",
        "    try:\n",
        "        dataset = load_dataset(x)\n",
        "\n",
        "        # Print the names of the splits\n",
        "        print(\"Dataset splits:\", dataset.keys())\n",
        "\n",
        "        # Print number of samples in each split\n",
        "        for split in dataset.keys():\n",
        "            print(f\"{split} size: {len(dataset[split])}\")\n",
        "\n",
        "        # Print column names (structure)\n",
        "        print(\"Columns:\", dataset[\"train\"].column_names)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ElNQ7bmuNc-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA from GITHUB and mostly HUGGINGFACE"
      ],
      "metadata": {
        "id": "47ek1TepdbTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_MC1_dataset():\n",
        "    try:\n",
        "        # Load dataset from Hugging Face\n",
        "        dataset = load_dataset(\"bigbio/med_qa\")\n",
        "        transformed_data_MC1 = []\n",
        "        for item in concatenate_datasets([dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]]):\n",
        "            # Ensure only English questions are kept\n",
        "            #if item[\"language\"] == \"english\":\n",
        "                transformed_item = {\n",
        "                    \"correct_answer\": item[\"answer_idx\"],  # Convert index to A/B/C/D format\n",
        "                    \"options\": {  # Extract only the values from option dictionary\n",
        "                        \"A\": item[\"options\"][0][\"value\"],\n",
        "                        \"B\": item[\"options\"][1][\"value\"],\n",
        "                        \"C\": item[\"options\"][2][\"value\"],\n",
        "                        \"D\": item[\"options\"][3][\"value\"]\n",
        "                    },\n",
        "                    \"question\": item[\"question\"],\n",
        "                    \"source\": {\n",
        "                        \"isbn\": \"000-0000000000\",\n",
        "                        \"page\": 0,\n",
        "                        \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "                    },\n",
        "                    \"type\": \"multiple_choice\"\n",
        "                }\n",
        "                transformed_data_MC1.append(transformed_item)\n",
        "        return transformed_data_MC1\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "transformed_MC1_data = transform_MC1_dataset()\n",
        "print(json.dumps(transformed_MC1_data[:3], indent=4))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GntAi1MxNs7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_short_answer1_dataset():\n",
        "    dataset = load_dataset(\"HPAI-BSC/OpenMedQA\")\n",
        "    transformed_data_short_answer1 = []\n",
        "    for item in dataset['train']:  # Assuming 'train' split contains the data\n",
        "        transformed_item = {\n",
        "            \"answer\": item[\"answer\"],\n",
        "            \"question\": item[\"question\"],\n",
        "            \"source\": {\n",
        "                \"isbn\": \"000-0000000000\",  # Placeholder value\n",
        "                \"page\": 0,  # Placeholder value\n",
        "                \"paragraph_id\": \"000-0000000000-p00-para26\"  # Placeholder value\n",
        "            },\n",
        "            \"type\": \"short_answer\"\n",
        "        }\n",
        "        transformed_data_short_answer1.append(transformed_item)\n",
        "    return transformed_data_short_answer1\n",
        "\n",
        "transformed_short_answer1_data = transform_short_answer1_dataset()\n",
        "print(json.dumps(transformed_short_answer1_data[:5], indent=4))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "g_M7w5B2bDDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"qiaojin/PubMedQA\", \"pqa_artificial\")\n",
        "transformed_data_TF2 = [] # Initialize an empty list for True/False questions\n",
        "for entry in dataset[\"train\"]:\n",
        "    question = entry['question'].strip()\n",
        "    answer = entry['final_decision'].strip()\n",
        "    # Convert final_decision to True/False\n",
        "    transformed_answer = \"True\" if answer.lower() == \"yes\" else \"False\"\n",
        "    # Create the formatted True/False entry\n",
        "    formatted_entry = {\n",
        "        \"answer\": transformed_answer,\n",
        "        \"question\": question,\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para31\"\n",
        "        },\n",
        "        \"type\": \"true_false\"\n",
        "    }\n",
        "\n",
        "    transformed_data_TF2.append(formatted_entry)\n",
        "transformed_TF2_data = transformed_data_TF2\n",
        "# Print the first 3 formatted entries\n",
        "print(json.dumps(transformed_TF2_data[:3], indent=4))"
      ],
      "metadata": {
        "id": "YsJ3APzfhprG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama"
      ],
      "metadata": {
        "id": "OSGk_HvEVq_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import Memory\n",
        "from tqdm.auto import tqdm\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data if not already downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Set pad_token_id to eos_token_id for open-end generation\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Initialize caching\n",
        "memory = Memory(location=\".cache\", verbose=0)\n",
        "\n",
        "# Function to generate prompt\n",
        "def generate_prompt(example):\n",
        "    return f\"\"\"\n",
        "    Question: {example['question']}\n",
        "    Answer: {example['answer']}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "\n",
        "# Function to extract reasoning and answer\n",
        "def extract_reasoning(response):\n",
        "    generated_text = response[0][\"generated_text\"]\n",
        "    sentences = nltk.sent_tokenize(generated_text)\n",
        "    answer = sentences[-1]\n",
        "    reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(sentences[:-1]) if step]\n",
        "    return reasoning, answer\n",
        "\n",
        "# Function to generate reasoning steps (with caching)\n",
        "@memory.cache\n",
        "def generate_reasoning_steps(examples):\n",
        "    prompts = [generate_prompt(example) for example in examples]\n",
        "    reasoning_steps = []\n",
        "    answers = []\n",
        "    for prompt in prompts:\n",
        "        try:\n",
        "            pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "            response = pipe(prompt, max_new_tokens=256, do_sample=True, batch_size=4)\n",
        "            reasoning, answer = extract_reasoning(response)\n",
        "            reasoning_steps.append(reasoning)\n",
        "            answers.append(answer)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during llama_pipeline call: {e}\")\n",
        "            reasoning_steps.append([\"Error: Could not generate reasoning.\"])\n",
        "            answers.append(\"Error: Could not generate answer.\")\n",
        "    return {\"reasoning\": reasoning_steps, \"answer\": answers}\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"lavita/MedQuAD\", split=\"train\")\n",
        "\n",
        "# Apply the function to the dataset\n",
        "dataset = dataset.map(generate_reasoning_steps, batched=True, batch_size=4)\n",
        "\n",
        "# Transform the dataset to the desired format\n",
        "transformed_data_R1 = []\n",
        "for item in tqdm(dataset, desc=\"Transforming data\"):\n",
        "    formatted_item = {\n",
        "        \"answer\": item[\"answer\"],\n",
        "        \"question\": item[\"question\"],\n",
        "        \"reasoning\": item[\"reasoning\"],\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "    transformed_data_R1.append(formatted_item)\n",
        "\n",
        "# Print the first 3 formatted entries\n",
        "print(json.dumps(transformed_data_R1[:3], indent=4))\n",
        "transformed_R1_data = transformed_data_R1"
      ],
      "metadata": {
        "id": "n9ivzROnOVrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_MC2_dataset():\n",
        "    try:\n",
        "        # Load both test and validation splits\n",
        "        dataset_test = load_dataset(\"stellalisy/mediQ\",split=\"test\")\n",
        "        dataset_validation = load_dataset(\"stellalisy/mediQ\", split=\"validation\")\n",
        "\n",
        "        transformed_data_MC2 = []\n",
        "\n",
        "        # Process both splits\n",
        "        for dataset in [dataset_test, dataset_validation]:\n",
        "            for item in dataset:\n",
        "                context = item.get(\"context\", \"\")\n",
        "                context = re.sub(r\"[\\[\\]\\{\\}\\(\\)\\'\\\"]\", \"\", str(context)) # Remove other brackets and quotes\n",
        "                transformed_item = {\n",
        "                    \"correct_answer\": item[\"answer_idx\"],\n",
        "                    \"options\": item[\"options\"],\n",
        "                    \"question\": item[\"question\"] + \" \" + context,\n",
        "                    \"source\": {\n",
        "                        \"isbn\": \"000-0000000000\",\n",
        "                        \"page\": 0,\n",
        "                        \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "                    },\n",
        "                    \"type\": \"multiple_choice\"\n",
        "                }\n",
        "                transformed_data_MC2.append(transformed_item)\n",
        "\n",
        "        # Return the combined transformed data\n",
        "        print(json.dumps(transformed_data_MC2[:3], indent=4))\n",
        "        return transformed_data_MC2\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "\n",
        "# Call the function and get the length\n",
        "transformed_MC2_data = transform_MC2_dataset()"
      ],
      "metadata": {
        "id": "opWFWV8jpbhi",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_MC3_dataset():\n",
        "    try:\n",
        "        # Load dataset from Hugging Face\n",
        "        dataset = load_dataset(\"openlifescienceai/medmcqa\")  # Loads the train split directly\n",
        "\n",
        "        transformed_data_MC3 = []\n",
        "\n",
        "        for item in concatenate_datasets([dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]]):  # Iterate directly over dataset\n",
        "            # Map numerical index to letter option\n",
        "            answer_mapping = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}\n",
        "            correct_answer = answer_mapping.get(item[\"cop\"], None)  # Get letter option or None if not found\n",
        "\n",
        "            transformed_item = {\n",
        "                \"correct_answer\": correct_answer, # Use mapped answer\n",
        "                \"options\": {\n",
        "                    \"A\": item[\"opa\"],\n",
        "                    \"B\": item[\"opb\"],\n",
        "                    \"C\": item[\"opc\"],\n",
        "                    \"D\": item[\"opd\"]\n",
        "                },\n",
        "                \"question\": item[\"question\"],\n",
        "                \"source\": {\n",
        "                    \"isbn\": \"000-0000000000\",\n",
        "                    \"page\": 0,\n",
        "                    \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "                },\n",
        "                \"type\": \"multiple_choice\"\n",
        "            }\n",
        "            transformed_data_MC3.append(transformed_item)\n",
        "        return transformed_data_MC3\n",
        "\n",
        "    except RequestException as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "\n",
        "# Now you can use transformed_MC3_data as before\n",
        "transformed_MC3_data = transform_MC3_dataset()\n",
        "print(json.dumps(transformed_MC3_data[:10], indent=4)) # Example: print first 10 entries"
      ],
      "metadata": {
        "id": "X_7Od8Sm90KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of retry attempts\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Attempt to load the dataset with retry logic\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"UCSC-VLAA/MedReason\")  # Replace with your dataset name\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "transformed_data_MC4 = []\n",
        "\n",
        "# Process each entry in the dataset\n",
        "for entry in dataset['train']:\n",
        "    question = entry['question'].strip()  # Assume the question is stored in the 'question' column\n",
        "    #answer = entry['answer'].strip()  # Assume the answer is in the 'answer' column - not needed here\n",
        "    options_raw = entry['options'].strip()  # Assume the options are in the 'options' column\n",
        "\n",
        "    # Extract and format options\n",
        "    options = {}\n",
        "    for line in options_raw.split(\"\\n\"):\n",
        "        if line.strip() and \". \" in line:  # Check if the line is not empty and contains \". \"\n",
        "            choice, text = line.split(\". \", 1)  # Split into choice and text\n",
        "            options[choice.strip()] = text.strip()\n",
        "\n",
        "    # Extract answer text (using string manipulation or regex)\n",
        "    answer_text = entry['answer'].strip().split(\".\")[0]  # Split at the first \".\" and take the first part\n",
        "\n",
        "    # Find the correct answer letter (using word-based matching)\n",
        "    correct_answer_letter = None\n",
        "    for letter, option_text in options.items():\n",
        "        for word in answer_text.split():  # Iterate through words in the answer\n",
        "            if word in option_text:  # Check if the word is present in the option text\n",
        "                correct_answer_letter = letter\n",
        "                break  # Stop searching if a match is found\n",
        "        if correct_answer_letter:  # Stop searching options if a match is found\n",
        "            break\n",
        "\n",
        "    correct_answer = correct_answer_letter\n",
        "\n",
        "    # Define source information (using placeholders)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": \"000-0000000000-p00-para06\"\n",
        "    }\n",
        "\n",
        "    # Construct the formatted entry\n",
        "    formatted_entry = {\n",
        "        \"correct_answer\": correct_answer,  # Use the found letter\n",
        "        \"options\": options,  # Use the formatted options dictionary\n",
        "        \"question\": question,\n",
        "        \"source\": source,\n",
        "        \"type\": \"multiple_choice\"\n",
        "    }\n",
        "    transformed_data_MC4.append(formatted_entry)\n",
        "\n",
        "# Print first 3 formatted entries for verification (optional)\n",
        "print(json.dumps(transformed_data_MC4[:3], indent=4))\n",
        "\n",
        "transformed_MC4_data = transformed_data_MC4"
      ],
      "metadata": {
        "id": "G6Tog88KCudq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "no llama used, reasoning in the dataset"
      ],
      "metadata": {
        "id": "S675ghjRWI5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "import re\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"UCSC-VLAA/MedReason\", split=\"train\")\n",
        "\n",
        "# Function to extract reasoning and answer from the \"reasoning\" column\n",
        "def extract_reasoning_and_answer(example):\n",
        "    reasoning_text = example[\"reasoning\"]\n",
        "\n",
        "    # Split into sections using regex\n",
        "    sections = re.split(r\"(Finding reasoning paths:|Reasoning Process:|Conclusion:)\", reasoning_text)\n",
        "\n",
        "    # Extract relevant parts\n",
        "    reasoning_process = sections[4].strip() if len(sections) > 4 else \"\"\n",
        "    conclusion = sections[6].strip() if len(sections) > 6 else \"\"\n",
        "\n",
        "    # Combine reasoning paths and process into steps, starting from 1\n",
        "    reasoning_steps = []\n",
        "    step_counter = 1  # Initialize step counter\n",
        "\n",
        "    if reasoning_process:\n",
        "        for line in reasoning_process.split('\\n'):\n",
        "              if line.strip():  # Check if line is not empty\n",
        "                    reasoning_steps.append(f\"Step {step_counter}: {line.strip()}\")\n",
        "                    step_counter += 1  # Increment step counter\n",
        "\n",
        "        # Extract the answer from the conclusion\n",
        "        answer = conclusion.split('.')[-2].strip() if conclusion else \"\"  # Last sentence before trailing period\n",
        "\n",
        "    return {\"reasoning\": reasoning_steps, \"answer\": answer}\n",
        "\n",
        "# Apply the function to the dataset\n",
        "dataset = dataset.map(extract_reasoning_and_answer)\n",
        "\n",
        "# Transform the dataset to the desired format\n",
        "transformed_data_R2 = []\n",
        "for item in tqdm(dataset, desc=\"Transforming data\"):\n",
        "    formatted_item = {\n",
        "        \"answer\": item[\"answer\"],\n",
        "        \"question\": item[\"question\"],\n",
        "        \"reasoning\": item[\"reasoning\"],\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000-p00-para01\"  # You can adjust the paragraph_id as needed\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "    transformed_data_R2.append(formatted_item)\n",
        "\n",
        "# Print the first 3 formatted entries\n",
        "print(json.dumps(transformed_data_R2[:3], indent=4))\n",
        "transformed_R2_data = transformed_data_R2"
      ],
      "metadata": {
        "id": "1apaXR1pQPtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama"
      ],
      "metadata": {
        "id": "hCbEPdayTf6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data if not already downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Set pad_token_id to eos_token_id for open-end generation\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Function to generate prompt\n",
        "def generate_prompt(example):\n",
        "    return f\"\"\"\n",
        "    Question: {example['question']}\n",
        "    Answer: {example['answer']}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "\n",
        "# Function to extract reasoning and answer\n",
        "def extract_reasoning(response):\n",
        "    generated_text = response[0][\"generated_text\"]\n",
        "    sentences = nltk.sent_tokenize(generated_text)\n",
        "    answer = sentences[-1]  # Last sentence is the answer\n",
        "    reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(sentences[:-1]) if step]\n",
        "    return reasoning, answer\n",
        "\n",
        "# Function to generate reasoning steps\n",
        "def generate_reasoning_steps(examples):\n",
        "    prompts = [generate_prompt(example) for example in examples]\n",
        "    reasoning_steps = []\n",
        "    answers = []\n",
        "    for prompt in prompts:\n",
        "        try:\n",
        "            pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "            response = pipe(prompt, max_new_tokens=256, do_sample=True, batch_size=4)\n",
        "            reasoning, answer = extract_reasoning(response)\n",
        "            reasoning_steps.append(reasoning)\n",
        "            answers.append(answer)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during llama_pipeline call: {e}\")\n",
        "            reasoning_steps.append([\"Error: Could not generate reasoning.\"])\n",
        "            answers.append(\"Error: Could not generate answer.\")\n",
        "    return {\"reasoning\": reasoning_steps, \"answer\": answers}\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"YvanAlvin/medicalQALlama2\", split=\"train\")\n",
        "\n",
        "# Extract question and answer from the \"text\" column\n",
        "def extract_question_answer(example):\n",
        "    text = example[\"text\"]\n",
        "    match = re.search(r'\\[INST\\](.*?)\\[/INST\\]', text, re.DOTALL)\n",
        "    question = match.group(1).strip() if match else \"Unknown Question\"\n",
        "    answer = text.split(\"[/INST]\")[-1].strip()\n",
        "    return {\"question\": question, \"answer\": answer}\n",
        "\n",
        "dataset = dataset.map(extract_question_answer)\n",
        "\n",
        "# Apply the reasoning generation function\n",
        "dataset = dataset.map(generate_reasoning_steps, batched=True, batch_size=4)\n",
        "\n",
        "# Transform the dataset to the desired format\n",
        "transformed_data_R3 = []\n",
        "for item in tqdm(dataset, desc=\"Transforming data\"):\n",
        "    formatted_item = {\n",
        "        \"answer\": item[\"answer\"],\n",
        "        \"question\": item[\"question\"],\n",
        "        \"reasoning\": item[\"reasoning\"],\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "    transformed_data_R3.append(formatted_item)\n",
        "\n",
        "# Print the first 3 formatted entries\n",
        "print(json.dumps(transformed_data_R3[:3], indent=4))\n",
        "transformed_R3_data = transformed_data_R3"
      ],
      "metadata": {
        "id": "f1WXUIacSdJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama"
      ],
      "metadata": {
        "id": "boXW3nEHTleq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy model for sentence segmentation\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Number of retry attempts\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Attempt to load the dataset with retry logic\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"YvanAlvin/medicalchat200llama2\", split=\"train\")\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "transformed_data_R4 = []\n",
        "\n",
        "# Process each item in the dataset\n",
        "for item in dataset:\n",
        "    text = item[\"text\"]\n",
        "\n",
        "    # Extract the question from inside [INST]...[/INST]\n",
        "    match = re.search(r'\\[INST\\](.*?)\\[/INST\\]', text, re.DOTALL)\n",
        "    question = match.group(1).strip() if match else \"Unknown Question\"\n",
        "\n",
        "    # Extract the answer as everything after [/INST]\n",
        "    answer = text.split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "    # Generate reasoning using CodeLlama\n",
        "    prompt = f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "        response = pipe(prompt, max_new_tokens=256, do_sample=True, batch_size=4)\n",
        "\n",
        "        generated_text = response[0][\"generated_text\"]\n",
        "        sentences = nltk.sent_tokenize(generated_text)\n",
        "        final_answer = sentences[-1]\n",
        "        reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(sentences[:-1]) if step]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during llama_pipeline call: {e}\")\n",
        "        reasoning = [\"Error: Could not generate reasoning.\"]\n",
        "        final_answer = \"Error: Could not generate answer.\"\n",
        "\n",
        "    formatted_item = {\n",
        "        \"answer\": final_answer,\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "\n",
        "    transformed_data_R4.append(formatted_item)\n",
        "\n",
        "transformed_R4_data = transformed_data_R4"
      ],
      "metadata": {
        "id": "xjvAefDuTH-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama"
      ],
      "metadata": {
        "id": "-UDM0ZIyVZMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Anzahl der Wiederholungsversuche für das Laden des Datensatzes\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Dataset laden mit Wiederholungslogik\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"eashuu/medical_qa\", split=\"train\")\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "# Function to generate prompt\n",
        "def generate_prompt(question, answer):\n",
        "    return f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "\n",
        "# Function to extract reasoning and answer\n",
        "def extract_reasoning(response):\n",
        "    generated_text = response[0][\"generated_text\"]\n",
        "    sentences = nltk.sent_tokenize(generated_text)\n",
        "    answer = sentences[-1]  # Last sentence is the answer\n",
        "    reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(sentences[:-1]) if step]\n",
        "    return reasoning, answer\n",
        "\n",
        "# Function to generate reasoning steps\n",
        "def generate_reasoning_steps(question, answer):\n",
        "    prompt = generate_prompt(question, answer)\n",
        "    try:\n",
        "        pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "        response = pipe(prompt, max_new_tokens=256, do_sample=True, batch_size=4)\n",
        "        reasoning, final_answer = extract_reasoning(response)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during llama_pipeline call: {e}\")\n",
        "        reasoning = [\"Error: Could not generate reasoning.\"]\n",
        "        final_answer = \"Error: Could not generate answer.\"\n",
        "    return reasoning, final_answer\n",
        "\n",
        "# Modify processing logic\n",
        "transformed_data_R5 = []\n",
        "\n",
        "for item in dataset:\n",
        "    question = item[\"instruction\"].replace(\"Q. \", \"\", 1).strip()\n",
        "    answer = item[\"output\"].strip()\n",
        "\n",
        "    # CodeLlama for argumentation verwenden\n",
        "    reasoning, final_answer = generate_reasoning_steps(question, answer)\n",
        "\n",
        "    formatted_item = {\n",
        "        \"answer\": final_answer,\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "\n",
        "    transformed_data_R5.append(formatted_item)\n",
        "transformed_R5_data = transformed_data_R5"
      ],
      "metadata": {
        "id": "Io8hX-sHU5OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama"
      ],
      "metadata": {
        "id": "s3jWWSDmWFRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of retry attempts\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\")\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "transformed_data_R6 = []\n",
        "\n",
        "# Process each item in the dataset\n",
        "for item in dataset[\"train\"]:\n",
        "    question = item[\"Question\"].strip()\n",
        "    answer = item[\"Response\"].strip()\n",
        "\n",
        "    # Generate reasoning using CodeLlama\n",
        "    prompt = f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "        response = pipe(prompt, max_new_tokens=256, do_sample=True, batch_size=4)\n",
        "\n",
        "        generated_text = response[0][\"generated_text\"]\n",
        "        sentences = nltk.sent_tokenize(generated_text)\n",
        "        final_answer = sentences[-1]\n",
        "        reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(sentences[:-1]) if step]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during llama_pipeline call: {e}\")\n",
        "        reasoning = [\"Error: Could not generate reasoning.\"]\n",
        "        final_answer = \"Error: Could not generate answer.\"\n",
        "\n",
        "    formatted_item = {\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"answer\": final_answer,\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "\n",
        "    transformed_data_R6.append(formatted_item)\n",
        "\n",
        "# Print the first 3 formatted entries\n",
        "#print(transformed_data_R6[:3])\n",
        "\n",
        "transformed_R6_data = transformed_data_R6"
      ],
      "metadata": {
        "id": "7-CqzouRWCJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama"
      ],
      "metadata": {
        "id": "PYnz8wurX3tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Anzahl der Wiederholungsversuche für das Laden des Datensatzes\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Dataset laden mit Wiederholungslogik\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"wentechno/medicalQA-50thPlus\", split=\"train\")\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "# Function to generate prompt\n",
        "def generate_prompt(question, answer):\n",
        "    return f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "\n",
        "# Function to extract reasoning and answer\n",
        "def extract_reasoning(response):\n",
        "    generated_text = response[0][\"generated_text\"]\n",
        "    sentences = nltk.sent_tokenize(generated_text)\n",
        "    answer = sentences[-1]  # Last sentence is the answer\n",
        "    reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(sentences[:-1]) if step]\n",
        "    return reasoning, answer\n",
        "\n",
        "# Function to generate reasoning steps\n",
        "def generate_reasoning_steps(question, answer):\n",
        "    prompt = generate_prompt(question, answer)\n",
        "    try:\n",
        "        pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "        response = pipe(prompt, max_new_tokens=256, do_sample=True, batch_size=4)\n",
        "        reasoning, final_answer = extract_reasoning(response)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during llama_pipeline call: {e}\")\n",
        "        reasoning = [\"Error: Could not generate reasoning.\"]\n",
        "        final_answer = \"Error: Could not generate answer.\"\n",
        "    return reasoning, final_answer\n",
        "\n",
        "# Modify processing logic\n",
        "transformed_data_R7 = []\n",
        "\n",
        "for item in dataset:\n",
        "    question = item[\"instruction\"].replace(\"Q. \", \"\", 1).strip()\n",
        "    answer = item[\"output\"].strip()\n",
        "\n",
        "    # CodeLlama für Argumentation verwenden\n",
        "    reasoning, final_answer = generate_reasoning_steps(question, answer)\n",
        "\n",
        "    formatted_item = {\n",
        "        \"answer\": final_answer,  # Use the extracted final answer\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,  # Use the extracted reasoning steps\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",\n",
        "            \"page\": 0,\n",
        "            \"paragraph_id\": \"000-0000000000-p00-para00\"\n",
        "        },\n",
        "        \"type\": \"multi_hop\"\n",
        "    }\n",
        "\n",
        "    transformed_data_R7.append(formatted_item)\n",
        "transformed_R7_data = transformed_data_R7"
      ],
      "metadata": {
        "id": "Tp7i4dqtWoAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset(\"Ajayaadhi/Medical-QA\")\n",
        "\n",
        "# Initialize a list to hold the reformatted entries\n",
        "transformed_data_short_answer2 = []\n",
        "\n",
        "for entry in dataset[\"train\"]:  # Process all entries\n",
        "    text = entry[\"train\"]  # Adjust this if the key is different\n",
        "\n",
        "    # Extract question\n",
        "    question_match = re.search(r\"### Input:\\n(.+?)\\n\\[INST\\]\", text, re.DOTALL)\n",
        "    question = question_match.group(1).strip() if question_match else \"\"\n",
        "\n",
        "    # Extract answer\n",
        "    answer_match = re.search(r\"### Response:\\n(.+?)</s>\", text, re.DOTALL)\n",
        "    answer = answer_match.group(1).strip() if answer_match else \"\"\n",
        "\n",
        "    # Define source information (using placeholders)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": \"000-0000000-p00-para01\"\n",
        "    }\n",
        "\n",
        "    # Determine response type\n",
        "    response_type = \"short_answer\"\n",
        "\n",
        "    # Construct the reformatted entry\n",
        "    reformatted_entry = {\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"source\": source,\n",
        "        \"type\": response_type\n",
        "    }\n",
        "\n",
        "    # Append to the list\n",
        "    transformed_data_short_answer2.append(reformatted_entry)\n",
        "\n",
        "# Print the first 3 formatted entries\n",
        "transformed_short_answer2_data = transformed_data_short_answer2\n",
        "print(json.dumps(transformed_short_answer1_data[:4], indent = 4))"
      ],
      "metadata": {
        "id": "TWOpCxIu3QYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama"
      ],
      "metadata": {
        "id": "pKWclgB7YuTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Anzahl der Wiederholungsversuche für das Laden des Datensatzes\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Dataset laden mit Wiederholungslogik\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"KaungHtetCho/MedicalQA\", split=\"train\")\n",
        "        break  # Exit the loop if successful\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "        else:\n",
        "            raise  # Re-raise the exception if all retries fail\n",
        "\n",
        "\n",
        "# Funktion zur Generierung von Argumentationsschritten\n",
        "def generate_reasoning_steps(question, answer):\n",
        "    prompt = f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "        response = pipe(prompt, max_new_tokens=256, do_sample=True, batch_size=4) # Using the pipeline\n",
        "        # Extract reasoning and final answer\n",
        "        generated_text = response[0][\"generated_text\"]\n",
        "        sentences = nltk.sent_tokenize(generated_text)\n",
        "        final_answer = sentences[-1]  # Last sentence is the final answer\n",
        "        reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(sentences[:-1]) if step]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during llama_pipeline call: {e}\")\n",
        "        reasoning = [\"Error: Could not generate reasoning.\"]\n",
        "        final_answer = \"Error: Could not generate answer.\"\n",
        "\n",
        "    return reasoning, final_answer\n",
        "\n",
        "transformed_data_R8 = []\n",
        "\n",
        "# Process each entry in the dataset\n",
        "for entry in dataset:\n",
        "    patient = entry['Patient'].strip()\n",
        "    doctor = entry['Doctor'].strip()\n",
        "\n",
        "    # Form the question from description + patient details\n",
        "    question = patient\n",
        "\n",
        "    # Generate answer using CodeLlama\n",
        "    # answer = llama_pipeline(question, max_length=256, do_sample=True)[0][\"generated_text\"].strip()\n",
        "    answer = doctor\n",
        "    # Generate reasoning using CodeLlama\n",
        "    reasoning, final_answer = generate_reasoning_steps(question, answer)\n",
        "\n",
        "\n",
        "    # Define source information (using placeholders)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": \"000-0000-p00-para01\"\n",
        "    }\n",
        "\n",
        "    # Define the type based on reasoning complexity\n",
        "    response_type = \"multi_hop\"\n",
        "\n",
        "    # Construct the formatted entry\n",
        "    formatted_entry = {\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"answer\": final_answer,  # Using generated answer\n",
        "        \"source\": source,\n",
        "        \"type\": response_type\n",
        "    }\n",
        "\n",
        "    transformed_data_R8.append(formatted_entry)\n",
        "transformed_R8_data = transformed_data_R8\n",
        "# Print first 3 formatted entries\n",
        "print(json.dumps(transformed_data_R8[:3], indent=4))"
      ],
      "metadata": {
        "id": "7hBl8wfoYDVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Anzahl der Wiederholungsversuche für das Laden des Datensatzes\n",
        "MAX_RETRIES = 3\n",
        "\n",
        "# Dataset laden mit Wiederholungslogik\n",
        "for attempt in range(MAX_RETRIES):\n",
        "    try:\n",
        "        dataset = load_dataset(\"qiaojin/PubMedQA\", \"pqa_unlabeled\")\n",
        "        break\n",
        "    except RequestException:\n",
        "        if attempt < MAX_RETRIES - 1:\n",
        "            print(f\"Download attempt {attempt + 1} failed. Retrying...\")\n",
        "            time.sleep(5)\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "# Funktion zur Generierung von Argumentationsschritten\n",
        "def generate_reasoning_steps(question, answer):\n",
        "    prompt = f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "        response = pipe(prompt, max_new_tokens=256, do_sample=True, batch_size=4) # Using the pipeline\n",
        "        # Extract reasoning and final answer\n",
        "        generated_text = response[0][\"generated_text\"]\n",
        "        sentences = nltk.sent_tokenize(generated_text)\n",
        "        final_answer = sentences[-1]  # Last sentence is the final answer\n",
        "        reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(sentences[:-1]) if step]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during llama_pipeline call: {e}\")\n",
        "        reasoning = [\"Error: Could not generate reasoning.\"]\n",
        "        final_answer = \"Error: Could not generate answer.\"\n",
        "\n",
        "    return reasoning, final_answer\n",
        "\n",
        "transformed_data_R9 = []\n",
        "\n",
        "# Process each entry in the dataset\n",
        "for entry in dataset[\"train\"]:\n",
        "    question = entry['question'].strip()\n",
        "    answer = entry['long_answer'].strip()\n",
        "\n",
        "    # Use CodeLlama to generate reasoning\n",
        "    reasoning, final_answer = generate_reasoning_steps(question, answer)  # Extract final_answer\n",
        "\n",
        "    # Define source information (using placeholders)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": \"000-0000-p00-para01\"\n",
        "    }\n",
        "\n",
        "    # Define the type based on reasoning complexity\n",
        "    response_type = \"multi_hop\"\n",
        "\n",
        "    # Construct the formatted entry\n",
        "    formatted_entry = {\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,\n",
        "        \"answer\": final_answer,  # Use extracted final_answer\n",
        "        \"source\": source,\n",
        "        \"type\": response_type\n",
        "    }\n",
        "\n",
        "    transformed_data_R9.append(formatted_entry)\n",
        "\n",
        "transformed_R9_data = transformed_data_R9\n",
        "# Print first 3 formatted entries\n",
        "#print(json.dumps(transformed_data_R9[:3], indent=4))"
      ],
      "metadata": {
        "id": "oXxUdiBbZKLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA FROM KAGGLE"
      ],
      "metadata": {
        "id": "Wpt4rFU37RaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle pandas\n"
      ],
      "metadata": {
        "id": "Vpzmmvo53jYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d thedevastator/comprehensive-medical-q-a-dataset --unzip\n"
      ],
      "metadata": {
        "id": "gQad59AC3wJ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faeed5b6-ef61-4eda-fe6c-2474d68f25ef"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/thedevastator/comprehensive-medical-q-a-dataset\n",
            "License(s): CC0-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import auth\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "# Authenticate with Kaggle API\n",
        "os.environ['KAGGLE_USERNAME'] = \"apfresh\" # Replace with your username\n",
        "os.environ['KAGGLE_KEY'] = \"50af00b12093dc762e1d2d1c138dd817\"\n",
        "api = KaggleApi()\n",
        "api.authenticate()"
      ],
      "metadata": {
        "id": "7p2ULHnJ67jX"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset (assuming it's already available in Colab)\n",
        "file_path = \"/content/train.csv\"  # Adjust based on actual filename\n",
        "dataset = pd.read_csv(file_path)\n",
        "\n",
        "# Initialize a list to hold the reformatted entries\n",
        "transformed_data_short_answer3 = []\n",
        "\n",
        "# Iterate over the dataset entries\n",
        "for index, entry in dataset.iterrows():\n",
        "    question = entry.get('Question', '').strip()\n",
        "    answer = entry.get('Answer', '').strip()\n",
        "\n",
        "    # Define source information (using placeholders here)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": f\"000-0000000-p00-para{index+1:02d}\"\n",
        "    }\n",
        "\n",
        "    # Determine the response type based on answer length\n",
        "    response_type = \"short_answer\"\n",
        "\n",
        "    # Construct the reformatted entry\n",
        "    reformatted_entry = {\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"source\": source,\n",
        "        \"type\": response_type\n",
        "    }\n",
        "\n",
        "    # Append to the list\n",
        "    transformed_data_short_answer3.append(reformatted_entry)\n",
        "# Convert the list to JSON format and print the first 3 entries\n",
        "#print(json.dumps(transformed_data_short_answer3[:3], indent=4))\n",
        "transformed_short_answer3_data = transformed_data_short_answer3\n",
        "print(json.dumps(transformed_short_answer3_data[:4], indent=4))"
      ],
      "metadata": {
        "id": "IJeGfraN5jJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from transformers import pipeline\n",
        "\n",
        "# Assuming you have already loaded the model and tokenizer as 'model' and 'tokenizer'\n",
        "# and that nltk.download('punkt') has been executed\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/medquad.csv\"  # Adjust based on the actual filename\n",
        "dataset = pd.read_csv(file_path)\n",
        "\n",
        "# Function to generate prompt\n",
        "def generate_prompt(question, answer):\n",
        "    return f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    Provide a step-by-step reasoning breakdown explaining how the answer was derived.\n",
        "    Each step should be clearly numbered and logically connected.\n",
        "    \"\"\"\n",
        "\n",
        "# Function to extract reasoning and answer\n",
        "def extract_reasoning(response):\n",
        "    generated_text = response[0][\"generated_text\"]\n",
        "    sentences = nltk.sent_tokenize(generated_text)\n",
        "    answer = sentences[-1]  # Last sentence is the answer\n",
        "    reasoning = [f\"Step {i+1}: {step.strip()}\" for i, step in enumerate(sentences[:-1]) if step]\n",
        "    return reasoning, answer\n",
        "\n",
        "# Function to generate reasoning steps\n",
        "def generate_reasoning_steps(question, answer):\n",
        "    prompt = generate_prompt(question, answer)\n",
        "    try:\n",
        "        pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "        response = pipe(prompt, max_new_tokens=256, do_sample=True, batch_size=4)\n",
        "        reasoning, final_answer = extract_reasoning(response)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during llama_pipeline call: {e}\")\n",
        "        reasoning = [\"Error: Could not generate reasoning.\"]\n",
        "        final_answer = \"Error: Could not generate answer.\"\n",
        "    return reasoning, final_answer\n",
        "\n",
        "# Initialize a list to hold the reformatted entries\n",
        "transformed_data_R10 = []\n",
        "\n",
        "# Iterate over the dataset entries\n",
        "for index, entry in dataset.iterrows():\n",
        "    question = entry.get('Question', '').strip()\n",
        "    answer = entry.get('Answer', '').strip()\n",
        "\n",
        "    # Generate reasoning using CodeLlama\n",
        "    reasoning, final_answer = generate_reasoning_steps(question, answer)\n",
        "\n",
        "    # Define source information (using placeholders here)\n",
        "    source = {\n",
        "        \"isbn\": \"000-0000000000\",\n",
        "        \"page\": 0,\n",
        "        \"paragraph_id\": f\"000-0000000-p00-para{index+1:02d}\"\n",
        "    }\n",
        "\n",
        "    # Construct the reformatted entry\n",
        "    reformatted_entry = {\n",
        "        \"question\": question,\n",
        "        \"reasoning\": reasoning,  # Include the reasoning steps\n",
        "        \"answer\": final_answer,  # Use the final answer (single sentence)\n",
        "        \"source\": source,\n",
        "        \"type\": \"multi_hop\"  # You might need to adjust the type\n",
        "    }\n",
        "\n",
        "    # Append to the list\n",
        "    transformed_data_R10.append(reformatted_entry)\n",
        "\n",
        "  transformed_R10_data = transformed_data_R10"
      ],
      "metadata": {
        "id": "lCfTdDtYZ1BG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d pythonafroz/medquad-medical-question-answer-for-ai-research --unzip\n"
      ],
      "metadata": {
        "id": "RBfFVCdr7Ajw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5974d0b0-ed26-4104-eb62-ea1c11447258"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/pythonafroz/medquad-medical-question-answer-for-ai-research\n",
            "License(s): CC-BY-SA-4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LOCAL DATA"
      ],
      "metadata": {
        "id": "yLVNoX6_Ehrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from io import StringIO\n",
        "\n",
        "#not working\n",
        "csv = \"/content/true_false_questions.csv\"\n",
        "\n",
        "# Fetch the CSV data\n",
        "try:\n",
        "    df = pd.read_csv(csv)  # Read CSV directly from Colab environment\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found: {csv}. Please make sure it is uploaded to Colab.\")\n",
        "    exit()\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse the CSV file: {csv}. Please check its format.\")\n",
        "    exit()\n",
        "\n",
        "df = pd.read_csv(csv)\n",
        "# Transform data\n",
        "transformed_data_TF1 = []\n",
        "for _, row in df.iterrows():\n",
        "    formatted_item = {\n",
        "        \"question\": row[\"text\"],  # Extract question\n",
        "        \"answer\": str(row[\"label\"]),  # Extract answer as string\n",
        "        \"source\": {\n",
        "            \"isbn\": \"000-0000000000\",  # Placeholder value\n",
        "            \"page\": 0,  # Placeholder value\n",
        "            \"paragraph_id\": \"000-0000000000-p00-paraXX\"  # Placeholder value\n",
        "        },\n",
        "        \"type\": \"true_false\"\n",
        "    }\n",
        "    transformed_data_TF1.append(formatted_item)\n",
        "\n",
        "# Save formatted data to JSON\n",
        "transformed_TF1_data = transformed_data_TF1"
      ],
      "metadata": {
        "id": "O64XzUf2ifuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ENTIRE DATASET"
      ],
      "metadata": {
        "id": "w_wLy9bMCy9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "lists = [transformed_TF1_data, transformed_TF2_data, transformed_short_answer1_data, transformed_short_answer2_data, transformed_short_answer3_data,\n",
        "        transformed_MC1_data, transformed_MC2_data, transformed_MC3_data, transformed_MC4_data,\n",
        "        transformed_R1_data, transformed_R2_data,transformed_R3_data,transformed_R4_data,transformed_R5_data,transformed_R6_data,transformed_R7_data,transformed_R8_data, transformed_R9_data, transformed_R10_data ]\n",
        "DATA = list(itertools.chain(*lists))\n"
      ],
      "metadata": {
        "id": "7TdXFkBOra0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removes duplicates, tokenization, stopwords, lemmatization, padding\n",
        "\n",
        "# Handle Class Imbalance:\n",
        "\n",
        "#    SMOTE will help generate synthetic samples for underrepresented classes in the dataset.\n",
        "\n",
        "#    Class Weights can be used in the model to give more importance to underrepresented classes during training.\n",
        "\n",
        "# Paraphrasing / Question Modification:\n",
        "\n",
        "#    We will use a GPT-based model to paraphrase or modify questions to generate additional training samples."
      ],
      "metadata": {
        "id": "OUPfGmrhCdpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SPLIT BY TYPE and SAVE .ZIP TO REPOSITORY"
      ],
      "metadata": {
        "id": "h3W5LepvH9ky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "\n",
        "    Ensure the data is preprocessed appropriately for each prompt style before creating the datasets.\n",
        "    Adjust the batch size, training steps, epochs, and other hyperparameters to find the best performance.\n",
        "    Regularly evaluate the performance on your test set for each prompt style.\n",
        "    This example assumes your prompt styles data is readily available.\n",
        "    Remember to make necessary imports and data modifications for smooth execution.\n"
      ],
      "metadata": {
        "id": "Ui9VFHI3-ZHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "Ad-Pq5_GkFvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Lemmatizes, removes stopwords, and lowercases the input text.\"\"\"\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.lower() not in stop_words and token.isalnum()]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def remove_duplicates_and_empty(data):\n",
        "    \"\"\"Removes duplicates and entries with empty questions or answers.\"\"\"\n",
        "    seen_pairs = set()\n",
        "    filtered_data = []\n",
        "    for entry in data:\n",
        "        q, a = entry[\"question\"], entry[\"answer\"]\n",
        "\n",
        "        if not q or not a:  # Remove if empty\n",
        "            continue\n",
        "\n",
        "        pair = (q.strip().lower(), a.strip().lower())  # Normalize case for comparison\n",
        "        if pair not in seen_pairs:\n",
        "            seen_pairs.add(pair)\n",
        "            filtered_data.append(entry)\n",
        "    return filtered_data"
      ],
      "metadata": {
        "id": "cP519IeiB1rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#It's usually better to apply stopword removal and lemmatization before removing duplicates\n",
        "preprocessed_data = [{k: preprocess_text(v) if isinstance(v, str) else v\n",
        "                       for k, v in d.items()} for d in DATA]\n",
        "preprocessed_data = remove_duplicates_and_empty(preprocessed_data)  # Apply to DATA"
      ],
      "metadata": {
        "id": "0DIk_DxVD8Rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split in Test and Train"
      ],
      "metadata": {
        "id": "rGRfqej7jn_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Group by Question Type\n",
        "grouped_data = {}\n",
        "for item in preprocessed_data:\n",
        "    question_type = item['type']\n",
        "    if question_type not in grouped_data:\n",
        "        grouped_data[question_type] = []\n",
        "    grouped_data[question_type].append(item)\n",
        "\n",
        "# 2. Stratified Split within Each Group\n",
        "train_data = []\n",
        "test_data = []\n",
        "for question_type, data in grouped_data.items():\n",
        "    # Create a temporary DataFrame for easier stratification (optional)\n",
        "    df = pd.DataFrame(data)\n",
        "    # Perform stratified split, using 'type' column for stratification\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['type'], random_state=42)  # Adjust test_size as needed\n",
        "    # Append the split data to the overall train and test sets\n",
        "    train_data.extend(train_df.to_dict('records'))\n",
        "    test_data.extend(test_df.to_dict('records'))\n",
        "\n",
        "# 3. Combine Splits\n",
        "# Now you have train_data and test_data with equal representation of question types\n",
        "print(f\"Train data size: {len(train_data)}\")\n",
        "print(f\"Test data size: {len(test_data)}\")"
      ],
      "metadata": {
        "id": "m1ZpZyEHjmxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting before formatting avoids potential data leakage, where information from the test set might influence the model during training."
      ],
      "metadata": {
        "id": "M8ynijmQnGex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format training data\n",
        "train_short_answer_data = [\n",
        "    {\n",
        "        \"input\": f\"Answer the following question:\\n{d['question']}\",\n",
        "        \"output\": d[\"answer\"]\n",
        "    }\n",
        "    for d in train_data if d['type'] == 'short_answer'\n",
        "]\n",
        "\n",
        "train_multi_hop_data = [\n",
        "    {\n",
        "        \"input\": f\"Answer the following multi-hop question:\\n{d['question']}\",\n",
        "        \"output\": f\"{d['reasoning']}\\nAnswer: {d['answer']}\"\n",
        "    }\n",
        "    for d in train_data if d['type'] == 'multi_hop'\n",
        "]\n",
        "\n",
        "train_true_false_data = [\n",
        "    {\n",
        "        \"input\": f\"Is the following statement true or false?\\nStatement: {d['question']}\",\n",
        "        \"output\": d[\"answer\"]\n",
        "    }\n",
        "    for d in train_data if d['type'] == 'true_false'\n",
        "]\n",
        "\n",
        "train_multiple_choice_data = [\n",
        "    {\n",
        "        \"input\": f\"Choose the correct option:\\nQuestion: {d['question']}\\nOptions:\\n\" +\n",
        "                 '\\n'.join([f\"{chr(65+i)}) {opt}\" for i, opt in enumerate(d['options'])]),\n",
        "        \"output\": d[\"correct_answer\"]\n",
        "    }\n",
        "    for d in train_data if d['type'] == 'multiple_choice'\n",
        "]\n",
        "\n",
        "# Format testing data\n",
        "test_short_answer_data = [\n",
        "    {\n",
        "        \"input\": f\"Answer the following question:\\n{d['question']}\",\n",
        "        \"output\": d[\"answer\"]\n",
        "    }\n",
        "    for d in test_data if d['type'] == 'short_answer'\n",
        "]\n",
        "\n",
        "test_multi_hop_data = [\n",
        "    {\n",
        "        \"input\": f\"Answer the following multi-hop question:\\n{d['question']}\",\n",
        "        \"output\": f\"{d['reasoning']}\\nAnswer: {d['answer']}\"\n",
        "    }\n",
        "    for d in test_data if d['type'] == 'multi_hop'\n",
        "]\n",
        "\n",
        "\n",
        "test_true_false_data = [\n",
        "    {\n",
        "        \"input\": f\"Is the following statement true or false?\\nStatement: {d['question']}\",\n",
        "        \"output\": d[\"answer\"]\n",
        "    }\n",
        "    for d in test_data if d['type'] == 'true_false'\n",
        "]\n",
        "\n",
        "test_multiple_choice_data = [\n",
        "    {\n",
        "        \"input\": f\"Choose the correct option:\\nQuestion: {d['question']}\\nOptions:\\n\" +\n",
        "                 '\\n'.join([f\"{chr(65+i)}) {opt}\" for i, opt in enumerate(d['options'])]),\n",
        "        \"output\": d[\"correct_answer\"]\n",
        "    }\n",
        "    for d in test_data if d['type'] == 'multiple_choice'\n",
        "]\n"
      ],
      "metadata": {
        "id": "kuuBQY0i0s1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save data to .zip"
      ],
      "metadata": {
        "id": "sa-eaSLc1Zyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = {\n",
        "    \"short_answer\": [],\n",
        "    \"true_false\": [],\n",
        "    \"multiple_choice\": [],\n",
        "    \"multi_hop\": []\n",
        "}\n",
        "\n",
        "test_data = {\n",
        "    \"short_answer\": [],\n",
        "    \"true_false\": [],\n",
        "    \"multiple_choice\": [],\n",
        "    \"multi_hop\": []\n",
        "}\n",
        "\n",
        "# Add your transformed data to the appropriate lists:\n",
        "train_data[\"short_answer\"].extend(train_short_answer_data)\n",
        "train_data[\"true_false\"].extend(train_true_false_data)\n",
        "train_data[\"multiple_choice\"].extend(train_multi_hop_data)\n",
        "train_data[\"multi_hop\"].extend(train_multiple_choice_data)\n",
        "\n",
        "test_data[\"short_answer\"].extend(test_short_answer_data)\n",
        "test_data[\"true_false\"].extend(test_true_false_data)\n",
        "test_data[\"multiple_choice\"].extend(test_multi_hop_data)\n",
        "test_data[\"multi_hop\"].extend(test_multiple_choice_data)"
      ],
      "metadata": {
        "id": "GJrKVTMU1dps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install github3.py"
      ],
      "metadata": {
        "id": "FAyUrYNW7HLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import zipfile\n",
        "import github3\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# 1. Get GitHub token from Secrets\n",
        "github_token = userdata.get('git')\n",
        "\n",
        "# 2. Authenticate with GitHub\n",
        "gh = github3.login(token=github_token)\n",
        "\n",
        "# 3. Repository Information\n",
        "repo_owner = 'Adria100'  # Replace with your username\n",
        "repo_name = 'clin_IQ'  # Replace with your repository name\n",
        "repo = gh.repository(repo_owner, repo_name)\n",
        "\n",
        "# 4. Function to create zip and upload to GitHub\n",
        "def save_data_to_zip_and_upload(data_dict, zip_file_name):\n",
        "    with zipfile.ZipFile(zip_file_name, \"w\") as zipf:\n",
        "        for data_type, data_list in data_dict.items():\n",
        "            file_name = f\"{data_type}_data.json\"\n",
        "            with zipf.open(file_name, \"w\") as f:\n",
        "                f.write(json.dumps(data_list, indent=4).encode())\n",
        "\n",
        "    # Upload the zip file to GitHub\n",
        "    with open(zip_file_name, \"rb\") as f:\n",
        "        content = f.read()\n",
        "        repo.create_file(\n",
        "            path=f\"data/processed/{zip_file_name}\",  # Path in the repository\n",
        "            message=f\"Adding {zip_file_name}\",  # Commit message\n",
        "            content=content,\n",
        "            branch='main'  # Replace with your branch name if needed\n",
        "        )\n",
        "\n",
        "    print(f\"Uploaded {zip_file_name} to GitHub\")\n",
        "    os.remove(zip_file_name)  # Remove local zip file\n",
        "\n",
        "# 5. Assuming you have train_data and test_data dictionaries populated\n",
        "# ... (your code to populate train_data and test_data) ...\n",
        "\n",
        "# 6. Save and upload the zip files\n",
        "save_data_to_zip_and_upload(train_data, \"train_dataset.zip\")\n",
        "save_data_to_zip_and_upload(test_data, \"test_dataset.zip\")"
      ],
      "metadata": {
        "id": "TVwD0KMA7Bxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "\n",
        "model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\"\"\"inputs = tokenizer(\n",
        "    your_data,\n",
        "    padding=\"max_length\",  # Pad to the maximum length\n",
        "    truncation=True,        # Truncate if exceeding the maximum length\n",
        "    max_length=512,        # Adjust the maximum length as needed\n",
        "    return_tensors=\"pt\"     # Return PyTorch tensors\n",
        ")\"\"\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             load_in_8bit=True,\n",
        "                                             device_map='auto')"
      ],
      "metadata": {
        "id": "eJEtDDNH-ECw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiPromptTrainer(Trainer):\n",
        "    def __init__(self, *args, prompt_styles_data, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.prompt_styles_data = prompt_styles_data  # Store data for each style\n",
        "\n",
        "    def training_step(self, model, inputs):\n",
        "        # Iterate over each prompt style\n",
        "        for style, data in self.prompt_styles_data.items():\n",
        "            # Create a dataloader for the current style\n",
        "            train_dataloader = self.get_train_dataloader(data)\n",
        "\n",
        "            # Perform a training step for the current style\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "              batch = batch.to(self.args.device)\n",
        "              outputs = model(**batch)\n",
        "              loss = outputs.loss\n",
        "              loss.backward()\n",
        "              self.optimizer.step()\n",
        "              self.optimizer.zero_grad()\n",
        "\n",
        "        return {'loss': loss.item()}  # Return the loss"
      ],
      "metadata": {
        "id": "cBeb4K8R-N1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",          # Output directory\n",
        "    per_device_train_batch_size=4,  # Batch size per device\n",
        "    gradient_accumulation_steps=4,  # Gradient accumulation steps\n",
        "    num_train_epochs=3,              # Number of training epochs\n",
        "    fp16=True,                       # Enable mixed precision training\n",
        "    logging_dir='./logs',            # Directory for storing logs\n",
        "    learning_rate=2e-5,             # Learning rate\n",
        "    weight_decay=0.01,              # Weight decay\n",
        "    optim=\"adamw_torch\",\n",
        "    save_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "trainer = MultiPromptTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=None,   # Not used in this example\n",
        "    prompt_styles_data={\n",
        "        \"short_answer\": train_short_answer_data,\n",
        "        \"multi_hop\": train_multi_hop_data,\n",
        "        \"true_false\": train_true_false_data,\n",
        "        \"multiple_choice\": train_multiple_choice_data\n",
        "    }\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "z3VOm5-E-PcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"./fine_tuned_llama\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_llama\")"
      ],
      "metadata": {
        "id": "kK8OKhTd-Sjb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}